{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "twitter_train_and_test.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "muh07KQiJNda",
        "OkvLFe71JNdb"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTm9feG6YYS9",
        "outputId": "c6d86dbd-7bb4-4eaf-b7d5-8a35503127f6"
      },
      "source": [
        "pip install emojis"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emojis\n",
            "  Downloading https://files.pythonhosted.org/packages/2e/94/61025e53488acd95b49862ec854e05b036f92fe9d0e512ca551a5a8b03d6/emojis-0.6.0-py3-none-any.whl\n",
            "Installing collected packages: emojis\n",
            "Successfully installed emojis-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tj1APzmYcdr",
        "outputId": "3b440265-414c-41f8-94de-8345688e2211"
      },
      "source": [
        "pip install trax"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting trax\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/1d/c0a3aeed127c26a0c3f0925fc9cc7278c272e52310eedfc322477e854972/trax-1.3.6-py2.py3-none-any.whl (468kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 14.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from trax) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from trax) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from trax) (1.18.5)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from trax) (0.3.0)\n",
            "Collecting t5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/39/a607d2450190af7675e4f77c5eff0cc9a83f82fe63fb396872ef2004106b/t5-0.7.1-py3-none-any.whl (172kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 48.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from trax) (4.0.1)\n",
            "Collecting tensorflow-text\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/b2/2dbd90b93913afd07e6101b8b84327c401c394e60141c1e98590038060b3/tensorflow_text-2.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 54.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from trax) (0.17.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from trax) (0.10.0)\n",
            "Collecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.6/dist-packages (from trax) (0.2.6)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.6/dist-packages (from trax) (0.1.57+cuda101)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from t5->trax) (0.22.2.post1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 56.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from t5->trax) (1.7.0+cu101)\n",
            "Collecting mesh-tensorflow[transformer]>=0.1.13\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/8b/553deb763ce8d00afb17debab7cb14a87b209cd4c6f0e8ecfc8d884cb12a/mesh_tensorflow-0.1.17-py3-none-any.whl (342kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 58.1MB/s \n",
            "\u001b[?25hCollecting tfds-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/c4/e010ef8ed0c0b9ab34e7ee297a79f3af969782d9ba5f10915860d4a5cc39/tfds_nightly-4.1.0.dev202011280107-py3-none-any.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 44.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from t5->trax) (3.2.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from t5->trax) (1.1.4)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/c4/8e948f601a4f9609e8b2b58f31966cb13cf17b940b82aa3e767f01c42c52/sacrebleu-1.4.14-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.1MB/s \n",
            "\u001b[?25hCollecting transformers>=2.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 57.8MB/s \n",
            "\u001b[?25hCollecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.6/dist-packages (from t5->trax) (2.9.0)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (3.3.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (2.23.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (1.1.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.1.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (3.12.4)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (20.3.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.3.3)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.8)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.25.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (4.41.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (2.3)\n",
            "Requirement already satisfied: tensorflow<2.4,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text->trax) (2.3.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->trax) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->trax) (1.5.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax->trax) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.6/dist-packages (from jaxlib->trax) (1.12)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->t5->trax) (0.17.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->t5->trax) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->t5->trax) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->t5->trax) (2018.9)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 54.5MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 54.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (20.4)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->trax) (3.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-datasets->trax) (50.3.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.52.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.3.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.33.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.2.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.3.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.35.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.3.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.1.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.7.0->t5->trax) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=2.7.0->t5->trax) (2.4.7)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.4.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (3.3.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (4.6)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.4.8)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=3a91d28b94689e7d755efac2d23444e2b86d3580ac7143a2b02a17225bf6efa7\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "\u001b[31mERROR: transformers 3.5.1 has requirement sentencepiece==0.1.91, but you'll have sentencepiece 0.1.94 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sentencepiece, mesh-tensorflow, tfds-nightly, tensorflow-text, portalocker, sacrebleu, sacremoses, tokenizers, transformers, rouge-score, t5, funcsigs, trax\n",
            "Successfully installed funcsigs-1.0.2 mesh-tensorflow-0.1.17 portalocker-2.0.0 rouge-score-0.0.4 sacrebleu-1.4.14 sacremoses-0.0.43 sentencepiece-0.1.94 t5-0.7.1 tensorflow-text-2.3.0 tfds-nightly-4.1.0.dev202011280107 tokenizers-0.9.3 transformers-3.5.1 trax-1.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LHOCmSOJNdW"
      },
      "source": [
        "import tweepy\n",
        "import json\n",
        "#import geocoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import os \n",
        "import time\n",
        "import emojis\n",
        "\n",
        "\n",
        "import nltk                                # Python library for NLP\n",
        "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
        "import random as rnd                       # pseudo-random number generator\n",
        "\n",
        "import re                                  # library for regular expression operations\n",
        "import string                              # for string operations\n",
        "\n",
        "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
        "from nltk.stem import PorterStemmer        # module for stemming\n",
        "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
        "\n",
        "\n",
        "# import relevant libraries\n",
        "import trax\n",
        "\n",
        "# import trax.fastmath.numpy\n",
        "\n",
        "import trax.fastmath.numpy as np\n",
        "\n",
        "\n",
        "# import trax.layers\n",
        "from trax import layers as tl\n",
        "\n",
        "\n",
        "from trax.supervised import training\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFzozR_0JNdW"
      },
      "source": [
        "\n",
        "# Load Twitter API secrets from an external JSON file\n",
        "secrets = json.load(open(r'XXXXXXXXXXXXXXXXX/secrets.json'))\n",
        "\n",
        "access_token = secrets['access_token']\n",
        "access_token_secret = secrets['access_token_secret']\n",
        "api_key = secrets['api_key']\n",
        "api_secret = secrets['api_secret']\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b__xpjAJJNdW"
      },
      "source": [
        "# authorize api handshake\n",
        "\n",
        "auth = tweepy.OAuthHandler(api_key, api_secret)\n",
        "\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adHCGdQbJNdW"
      },
      "source": [
        "# Download Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSsNkOy5JNdX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWMeHRTfJNdX",
        "outputId": "80f94e23-5361-48fd-8bd6-a7acb4fbcf4a"
      },
      "source": [
        "# downloads sample twitter dataset. uncomment the line below if running on a local machine.\n",
        "nltk.download('twitter_samples')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaFxh2xTJNdX"
      },
      "source": [
        "# select the set of positive and negative tweets\n",
        "nltk_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "nltk_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "V_y0pQjuJNdX",
        "outputId": "809836fc-b063-453f-f52c-467521602351"
      },
      "source": [
        "sentiment140_tweets = pd.read_csv('sentiment140.csv' , encoding = \"ISO-8859-1\", names = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])\n",
        "\n",
        "sentiment140_tweets.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>ids</th>\n",
              "      <th>date</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1599995</th>\n",
              "      <td>4</td>\n",
              "      <td>2193601966</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>AmandaMarie1028</td>\n",
              "      <td>Just woke up. Having no school is the best fee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599996</th>\n",
              "      <td>4</td>\n",
              "      <td>2193601969</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>TheWDBoards</td>\n",
              "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599997</th>\n",
              "      <td>4</td>\n",
              "      <td>2193601991</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>bpbabe</td>\n",
              "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599998</th>\n",
              "      <td>4</td>\n",
              "      <td>2193602064</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>tinydiamondz</td>\n",
              "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599999</th>\n",
              "      <td>4</td>\n",
              "      <td>2193602129</td>\n",
              "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>RyanTrevMorris</td>\n",
              "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         target  ...                                               text\n",
              "1599995       4  ...  Just woke up. Having no school is the best fee...\n",
              "1599996       4  ...  TheWDB.com - Very cool to hear old Walt interv...\n",
              "1599997       4  ...  Are you ready for your MoJo Makeover? Ask me f...\n",
              "1599998       4  ...  Happy 38th Birthday to my boo of alll time!!! ...\n",
              "1599999       4  ...  happy #charitytuesday @theNSPCC @SparksCharity...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_6ueZUBJNdX"
      },
      "source": [
        "\n",
        "fmap = {4:1, 0:0}\n",
        "\n",
        "sentiment140_tweets['target'] = sentiment140_tweets['target'] .map(fmap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzoFvwVoJNdX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53dc7006-bc41-46d0-effe-8fd3fe464e6f"
      },
      "source": [
        "sentiment140_tweets['target'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    800000\n",
              "0    800000\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YUz_OgCJNdX"
      },
      "source": [
        "\n",
        "sentiment140_pos_tweets = list(sentiment140_tweets[sentiment140_tweets['target'] == 1]['text'])\n",
        "\n",
        "sentiment140_neg_tweets = list(sentiment140_tweets[sentiment140_tweets['target'] == 0]['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDyCs3SPJNdX"
      },
      "source": [
        "\n",
        "all_positive_tweets = nltk_positive_tweets + sentiment140_pos_tweets\n",
        "\n",
        "all_negative_tweets = nltk_negative_tweets + sentiment140_neg_tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mtPgfI7JNdY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18349539-47b3-4d59-9c7c-766471f78caa"
      },
      "source": [
        "print('Number of positive tweets: ', len(all_positive_tweets))\n",
        "print('Number of negative tweets: ', len(all_negative_tweets))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of positive tweets:  805000\n",
            "Number of negative tweets:  805000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHMSqIDuJNdY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35cb8b2e-d111-4c3e-8e3e-cc296315af43"
      },
      "source": [
        "# print positive in greeen\n",
        "print('\\033[92m' + all_positive_tweets[rnd.randint(0,805000)])\n",
        "\n",
        "# print negative in red\n",
        "print('\\033[91m' + all_negative_tweets[rnd.randint(0,805000)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92m@DanielMoroney yo, found myself relating there with his dating joke. So even if I had gne 2nd round I wouldnt be &quot;creepy&quot; interesting \n",
            "\u001b[91m@sakixry I totally understand u! \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6pNbDjhJNdY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5934c5fa-ae66-4485-c21d-703e069720c4"
      },
      "source": [
        "# Split positive set into validation and training\n",
        "val_pos   = all_positive_tweets[724500:] # generating validation set for positive tweets\n",
        "train_pos  = all_positive_tweets[:724500]# generating training set for positive tweets\n",
        "\n",
        "# Split negative set into validation and training\n",
        "val_neg   = all_negative_tweets[724500:] # generating validation set for negative tweets\n",
        "train_neg  = all_negative_tweets[:724500] # generating training set for nagative tweets\n",
        "\n",
        "# Combine training data into one set\n",
        "train_x = train_pos + train_neg \n",
        "\n",
        "# Combine validation data into one set\n",
        "val_x  = val_pos + val_neg\n",
        "\n",
        "# Set the labels for the training set (1 for positive, 0 for negative)\n",
        "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "\n",
        "# Set the labels for the validation set (1 for positive, 0 for negative)\n",
        "val_y  = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))\n",
        "\n",
        "print(f\"Length of train_x:  {len(train_x)}\")\n",
        "print(f\"Length of val_x:  {len(val_x)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train_x:  1449000\n",
            "Length of val_x:  161000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZp0NcXbJNdY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ylz49FoJNdY"
      },
      "source": [
        "# Pre Process Tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUmAV_sJJNdY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsQ8Yqy1JNdY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac2be67b-8662-4685-d622-13d67b7a9a82"
      },
      "source": [
        "\n",
        "# Our selected sample. Complex enough to exemplify each step\n",
        "tweet = all_positive_tweets[300]\n",
        "print(tweet)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stats for the day have arrived. 2 new followers and NO unfollowers :) via http://t.co/xxlXs6xYwe.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nr-Ae920JNdY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75fd4bf4-e1c1-4794-c509-900e5e0c61af"
      },
      "source": [
        "\n",
        "# download the stopwords from NLTK\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#Import the english stop words list from NLTK\n",
        "stopwords_english = stopwords.words('english') \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7ivIdf-JNdY"
      },
      "source": [
        "\n",
        "def process_tweet(tweet):  \n",
        "\n",
        "    # remove old style retweet text \"RT\"\n",
        "    new_tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    \n",
        "    # decode emojis to text descriptions\n",
        "    new_tweet = emojis.decode(new_tweet)\n",
        "\n",
        "    # remove hyperlinks\n",
        "    new_tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', new_tweet)\n",
        "    new_tweet = re.sub(r'http\\S+', '', new_tweet)\n",
        "\n",
        "    # remove hashtags\n",
        "    new_tweet = re.sub(r'#', '', new_tweet)\n",
        "    \n",
        "    # remove underscores\n",
        "    new_tweet = re.sub(r'_', '', new_tweet)\n",
        "\n",
        "    # remove all numbers\n",
        "    new_tweet = re.sub(r'[0-9]', '', new_tweet)\n",
        "\n",
        "    # remove usernames\n",
        "    new_tweet = re.sub('@[^\\s]+', '', new_tweet)\n",
        "    \n",
        "    # remove punctuation even in the middle of a string \"in.the.middle\"\n",
        "    new_tweet = re.sub(r'[^\\w\\s]',' ', new_tweet)\n",
        "    \n",
        "    # convert text to lower-case\n",
        "    #new_tweet = new_tweet.lower()\n",
        "\n",
        "    # instantiate tokenizer class\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "\n",
        "    # tokenize tweets\n",
        "    tweet_tokens = tokenizer.tokenize(new_tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "\n",
        "    for word in tweet_tokens: # Go through every word in your tokens list\n",
        "#        if (word not in stopwords_english and  # remove stopwords\n",
        "#            word not in string.punctuation):  # remove punctuation\n",
        "#            tweets_clean.append(word)\n",
        "        if (word not in string.punctuation):  # remove punctuation\n",
        "            tweets_clean.append(word)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Instantiate stemming class\n",
        "    stemmer = PorterStemmer() \n",
        "\n",
        "    # Create an empty list to store the stems\n",
        "    tweets_stem = [] \n",
        "\n",
        "    for word in tweets_clean:\n",
        "        stem_word = stemmer.stem(word)  # stemming word\n",
        "        tweets_stem.append(stem_word)  # append to the list\n",
        "    \n",
        "    return tweets_stem\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPZAvDCPJNdY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bd3d1b9-013a-4a88-aec6-10d5823d4a1b"
      },
      "source": [
        "tweet = all_positive_tweets[300]\n",
        "print(f'Original tweet:     {tweet}')\n",
        "\n",
        "processed_tweet = process_tweet(tweet)\n",
        "print(f'Processed tweet:     {processed_tweet}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original tweet:     Stats for the day have arrived. 2 new followers and NO unfollowers :) via http://t.co/xxlXs6xYwe.\n",
            "Processed tweet:     ['stat', 'for', 'the', 'day', 'have', 'arriv', 'new', 'follow', 'and', 'no', 'unfollow', 'via']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkXaOjktJNdY"
      },
      "source": [
        "# Build the Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VKNHmCCJNdY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "60518891-f821-40cc-ab3f-9589f73ae83c"
      },
      "source": [
        "# Build the vocabulary\n",
        "# Unit Test Note - There is no test set here only train/val\n",
        "\n",
        "# Include special tokens \n",
        "# started with pad, end of line and unk tokens\n",
        "Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n",
        "\n",
        "# Note that we build vocab using training data\n",
        "for tweet in train_x: \n",
        "    processed_tweet = process_tweet(tweet)\n",
        "    for word in processed_tweet:\n",
        "        if word not in Vocab: \n",
        "            Vocab[word] = len(Vocab)\n",
        "    \n",
        "print(\"Total words in vocab are\", len(Vocab), '\\n')\n",
        "\n",
        "first_10 = {k: Vocab[k] for k in sorted(Vocab.keys())[:10]}\n",
        "display(first_10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words in vocab are 200415 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'__</e>__': 1,\n",
              " '__PAD__': 0,\n",
              " '__UNK__': 2,\n",
              " 'a': 34,\n",
              " 'aa': 3852,\n",
              " 'aaa': 12810,\n",
              " 'aaaa': 9957,\n",
              " 'aaaaaaaaah': 136831,\n",
              " 'aaaaaahhh': 156494,\n",
              " 'aaaaaariel': 155213}"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbicRpl0JNdZ"
      },
      "source": [
        "json.dump(Vocab, open( \"Vocab.json\", 'w' ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRs20d29JNdZ"
      },
      "source": [
        "# Build a Tensor from a Tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYGm_8RaJNdZ"
      },
      "source": [
        "\n",
        "def tweet_to_tensor(tweet, vocab_dict, unk_token='__UNK__', verbose=False):\n",
        "    '''\n",
        "    Input: \n",
        "        tweet - A string containing a tweet\n",
        "        vocab_dict - The words dictionary\n",
        "        unk_token - The special string for unknown tokens\n",
        "        verbose - Print info durign runtime\n",
        "    Output:\n",
        "        tensor_l - A python list with\n",
        "        \n",
        "    '''  \n",
        "\n",
        "    # Process the tweet into a list of words\n",
        "    # where only important words are kept (stop words removed)\n",
        "    word_l = process_tweet(tweet)\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"List of words from the processed tweet:\")\n",
        "        print(word_l)\n",
        "        \n",
        "    # Initialize the list that will contain the unique integer IDs of each word\n",
        "    tensor_l = []\n",
        "    \n",
        "    # Get the unique integer ID of the __UNK__ token\n",
        "    unk_ID = vocab_dict[unk_token]\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"The unique integer ID for the unk_token is {unk_ID}\")\n",
        "        \n",
        "    # for each word in the list:\n",
        "    for word in word_l:\n",
        "        \n",
        "        # Get the unique integer ID.\n",
        "        # If the word doesn't exist in the vocab dictionary,\n",
        "        # use the unique ID for __UNK__ instead.\n",
        "        word_ID = vocab_dict.get(word, unk_ID)\n",
        "\n",
        "        \n",
        "        # Append the unique integer ID to the tensor list.\n",
        "        tensor_l.append(word_ID) \n",
        "    \n",
        "    return tensor_l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdlrkrLWJNdZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c8bd2ab-4a47-4b37-8d48-d64a92bdd39e"
      },
      "source": [
        "print(\"Actual tweet is\\n\", val_pos[1])\n",
        "print(\"\\nTensor of tweet:\\n\", tweet_to_tensor(val_pos[1], vocab_dict=Vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual tweet is\n",
            " 4 hours and counting \n",
            "\n",
            "Tensor of tweet:\n",
            " [647, 24, 2137]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjvmii9CJNdZ"
      },
      "source": [
        "# Build a Batch Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1r7dur1JNdZ"
      },
      "source": [
        "def data_generator(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False):\n",
        "    '''\n",
        "    Input: \n",
        "        data_pos - Set of posstive examples\n",
        "        data_neg - Set of negative examples\n",
        "        batch_size - number of samples per batch. Must be even\n",
        "        loop - True or False\n",
        "        vocab_dict - The words dictionary\n",
        "        shuffle - Shuffle the data order\n",
        "    Yield:\n",
        "        inputs - Subset of positive and negative examples\n",
        "        targets - The corresponding labels for the subset\n",
        "        example_weights - An array specifying the importance of each example\n",
        "        \n",
        "    '''     \n",
        "\n",
        "    # make sure the batch size is an even number\n",
        "    # to allow an equal number of positive and negative samples\n",
        "    assert batch_size % 2 == 0\n",
        "    \n",
        "    # Number of positive examples in each batch is half of the batch size\n",
        "    # same with number of negative examples in each batch\n",
        "    n_to_take = batch_size // 2\n",
        "    \n",
        "    # Use pos_index to walk through the data_pos array\n",
        "    # same with neg_index and data_neg\n",
        "    pos_index = 0\n",
        "    neg_index = 0\n",
        "    \n",
        "    len_data_pos = len(data_pos)\n",
        "    len_data_neg = len(data_neg)\n",
        "    \n",
        "    # Get and array with the data indexes\n",
        "    pos_index_lines = list(range(len_data_pos))\n",
        "    neg_index_lines = list(range(len_data_neg))\n",
        "    \n",
        "    # shuffle lines if shuffle is set to True\n",
        "    if shuffle:\n",
        "        rnd.shuffle(pos_index_lines)\n",
        "        rnd.shuffle(neg_index_lines)\n",
        "        \n",
        "    stop = False\n",
        "    \n",
        "    # Loop indefinitely\n",
        "    while not stop:  \n",
        "        \n",
        "        # create a batch with positive and negative examples\n",
        "        batch = []\n",
        "        \n",
        "        # First part: Pack n_to_take positive examples\n",
        "        \n",
        "        # Start from pos_index and increment i up to n_to_take\n",
        "        for i in range(n_to_take):\n",
        "                    \n",
        "            # If the positive index goes past the positive dataset lenght,\n",
        "            if pos_index >= len_data_pos: \n",
        "                \n",
        "                # If loop is set to False, break once we reach the end of the dataset\n",
        "                if not loop:\n",
        "                    stop = True;\n",
        "                    break;\n",
        "                \n",
        "                # If user wants to keep re-using the data, reset the index\n",
        "                pos_index = 0\n",
        "                \n",
        "                if shuffle:\n",
        "                    # Shuffle the index of the positive sample\n",
        "                    rnd.shuffle(pos_index_lines)\n",
        "                    \n",
        "            # get the tweet as pos_index\n",
        "            tweet = data_pos[pos_index_lines[pos_index]]\n",
        "            \n",
        "            # convert the tweet into tensors of integers representing the processed words\n",
        "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
        "            \n",
        "            # append the tensor to the batch list\n",
        "            batch.append(tensor)\n",
        "            \n",
        "            # Increment pos_index by one\n",
        "            pos_index = pos_index + 1\n",
        "\n",
        "\n",
        "        # Second part: Pack n_to_take negative examples\n",
        "    \n",
        "        # Using the same batch list, start from neg_index and increment i up to n_to_take\n",
        "        for i in range(n_to_take):\n",
        "            \n",
        "            # If the negative index goes past the negative dataset length,\n",
        "            if neg_index >= len_data_neg:\n",
        "                \n",
        "                # If loop is set to False, break once we reach the end of the dataset\n",
        "                if not loop:\n",
        "                    stop = True;\n",
        "                    break;\n",
        "                    \n",
        "                # If user wants to keep re-using the data, reset the index\n",
        "                neg_index = 0\n",
        "                \n",
        "                if shuffle:\n",
        "                    # Shuffle the index of the negative sample\n",
        "                    rnd.shuffle(neg_index_lines)\n",
        "            # get the tweet as neg_index\n",
        "            tweet = data_neg[neg_index_lines[neg_index]]\n",
        "            \n",
        "            # convert the tweet into tensors of integers representing the processed words\n",
        "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
        "            \n",
        "            # append the tensor to the batch list\n",
        "            batch.append(tensor)\n",
        "            \n",
        "            # Increment neg_index by one\n",
        "            neg_index = neg_index + 1\n",
        "\n",
        "        if stop:\n",
        "            break;\n",
        "\n",
        "        # Update the start index for positive data \n",
        "        # so that it's n_to_take positions after the current pos_index\n",
        "        pos_index += n_to_take\n",
        "        \n",
        "        # Update the start index for negative data \n",
        "        # so that it's n_to_take positions after the current neg_index\n",
        "        neg_index += n_to_take\n",
        "        \n",
        "        # Get the max tweet length (the length of the longest tweet) \n",
        "        # (you will pad all shorter tweets to have this length)\n",
        "        max_len = max([len(t) for t in batch]) \n",
        "        \n",
        "        \n",
        "        # Initialize the input_l, which will \n",
        "        # store the padded versions of the tensors\n",
        "        tensor_pad_l = []\n",
        "        # Pad shorter tweets with zeros\n",
        "        for tensor in batch:\n",
        "\n",
        "            # Get the number of positions to pad for this tensor so that it will be max_len long\n",
        "            n_pad = max_len - len(tensor)\n",
        "            \n",
        "            # Generate a list of zeros, with length n_pad\n",
        "            pad_l = [0] * n_pad\n",
        "            \n",
        "            # concatenate the tensor and the list of padded zeros\n",
        "            tensor_pad = tensor +  pad_l\n",
        "\n",
        "            # append the padded tensor to the list of padded tensors\n",
        "            tensor_pad_l.append(tensor_pad)\n",
        "\n",
        "        # convert the list of padded tensors to a numpy array\n",
        "        # and store this as the model inputs\n",
        "        inputs = np.array(tensor_pad_l)\n",
        "  \n",
        "        # Generate the list of targets for the positive examples (a list of ones)\n",
        "        # The length is the number of positive examples in the batch\n",
        "        target_pos = [1] * n_to_take\n",
        "        \n",
        "        # Generate the list of targets for the negative examples (a list of zeros)\n",
        "        # The length is the number of negative examples in the batch\n",
        "        target_neg = [0] * n_to_take\n",
        "        \n",
        "        # Concatenate the positve and negative targets\n",
        "        target_l = target_pos + target_neg\n",
        "        \n",
        "        # Convert the target list into a numpy array\n",
        "        targets = np.array(target_l)\n",
        "\n",
        "        # Example weights: Treat all examples equally importantly.It should return an np.array. Hint: Use np.ones_like()\n",
        "        example_weights = np.ones_like(targets)\n",
        "        \n",
        "        # note we use yield and not return\n",
        "        yield inputs, targets, example_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1iS6yvLJNdZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "249ea023-3fdc-42c8-9697-c36c9cad7dd4"
      },
      "source": [
        "# Set the random number generator for the shuffle procedure\n",
        "rnd.seed(42) \n",
        "\n",
        "# Create the training data generator\n",
        "def train_generator(batch_size, shuffle = False):\n",
        "    return data_generator(train_pos, train_neg, batch_size, True, Vocab, shuffle)\n",
        "\n",
        "# Create the validation data generator\n",
        "def val_generator(batch_size, shuffle = False):\n",
        "    return data_generator(val_pos, val_neg, batch_size, True, Vocab, shuffle)\n",
        "\n",
        "# Create the validation data generator\n",
        "def test_generator(batch_size, shuffle = False):\n",
        "    return data_generator(val_pos, val_neg, batch_size, False, Vocab, shuffle)\n",
        "\n",
        "# Get a batch from the train_generator and inspect.\n",
        "inputs, targets, example_weights = next(train_generator(4, shuffle=True))\n",
        "\n",
        "# this will print a list of 4 tensors padded with zeros\n",
        "print(f'Inputs: {inputs}')\n",
        "print(f'Targets: {targets}')\n",
        "print(f'Example Weights: {example_weights}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inputs: [[  105    81  1531   128   719   187   315  1385    82   187     5   300\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [  713    97  5028     9   105  1053  5704   245  9294     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [ 1437   154   684   558  1922    37   137 45879  1495   144     9 16853\n",
            "   4639  3345 22509 15478  6016     9   105  3090  1655   279    85  6217]\n",
            " [ 8729   412   105   989    28   105  3510   167   418   433    28   514\n",
            "     24  2252   245   105 16888   280    59     0     0     0     0     0]]\n",
            "Targets: [1 1 0 0]\n",
            "Example Weights: [1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xcjb20nTJNdZ"
      },
      "source": [
        "# Build a Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIPRfiy7JNdZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td8pfNegJNdZ"
      },
      "source": [
        "def classifier(vocab_size=len(Vocab), embedding_dim=256, output_dim=2, mode='train'):\n",
        "        \n",
        "\n",
        "    # create embedding layer\n",
        "    embed_layer = tl.Embedding(\n",
        "        vocab_size=vocab_size, # Size of the vocabulary\n",
        "        d_feature=embedding_dim)  # Embedding dimension\n",
        "    \n",
        "    # Create a mean layer, to create an \"average\" word embedding\n",
        "    mean_layer = tl.Mean(axis=1)\n",
        "    \n",
        "    # Create a dense layer, one unit for each output\n",
        "    dense_output_layer = tl.Dense(n_units = output_dim)\n",
        "\n",
        "    # Create the log softmax layer (no parameters needed)\n",
        "    log_softmax_layer = tl.LogSoftmax()\n",
        "    \n",
        "    # Use tl.Serial to combine all layers\n",
        "    # and create the classifier\n",
        "    # of type trax.layers.combinators.Serial\n",
        "    model = tl.Serial(\n",
        "      embed_layer, # embedding layer\n",
        "      mean_layer, # mean layer\n",
        "      dense_output_layer, # dense output layer \n",
        "      log_softmax_layer # log softmax layer\n",
        "    )\n",
        "\n",
        "    # return the model of type\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZNbERvPJNdZ"
      },
      "source": [
        "tmp_model = classifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8hJPiCuJNdZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "1f3e9545-3870-4fa4-ba83-4f11e0cd7e28"
      },
      "source": [
        "print(type(tmp_model))\n",
        "display(tmp_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'trax.layers.combinators.Serial'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Serial[\n",
              "  Embedding_200415_256\n",
              "  Mean\n",
              "  Dense_2\n",
              "  LogSoftmax\n",
              "]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOK-I00LJNdZ"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFUr4-aeJNdZ"
      },
      "source": [
        "\n",
        "batch_size = 16\n",
        "rnd.seed(42)\n",
        "\n",
        "train_task = training.TrainTask(\n",
        "    labeled_data=train_generator(batch_size=batch_size, shuffle=True),\n",
        "    loss_layer=tl.CrossEntropyLoss(),\n",
        "    optimizer=trax.optimizers.Adam(0.0001),\n",
        "    n_steps_per_checkpoint=100,\n",
        ")\n",
        "\n",
        "eval_task = training.EvalTask(\n",
        "    labeled_data=val_generator(batch_size=batch_size, shuffle=True),\n",
        "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
        ")\n",
        "\n",
        "model = classifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Hr9K5mMdnrF",
        "outputId": "5fe0caa8-b318-40d1-c8d7-2d113fa89e65"
      },
      "source": [
        "! pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jXU5dN_JNdZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b6e06db-eeeb-4f9b-9929-6a81e294a3c3"
      },
      "source": [
        "output_dir = '~/content/model_adam0001_90562_9010_/'\n",
        "output_dir_expand = os.path.expanduser(output_dir)\n",
        "print(output_dir_expand)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/content/model_adam0001_90562_9010_/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjK_r7aYJNda"
      },
      "source": [
        "def train_model(classifier, train_task, eval_task, n_steps, output_dir):\n",
        "    '''\n",
        "    Input: \n",
        "        classifier - the model you are building\n",
        "        train_task - Training task\n",
        "        eval_task - Evaluation task\n",
        "        n_steps - the evaluation steps\n",
        "        output_dir - folder to save your files\n",
        "    Output:\n",
        "        trainer -  trax trainer\n",
        "    '''\n",
        "\n",
        "    training_loop = training.Loop(\n",
        "                                classifier, # The learning model\n",
        "                                train_task, # The training task\n",
        "                                eval_tasks = eval_task, # The evaluation task\n",
        "                                output_dir = output_dir) # The output directory\n",
        "\n",
        "    training_loop.run(n_steps = n_steps)\n",
        "\n",
        "    # Return the training_loop, since it has the model.\n",
        "    return training_loop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ERQKUzcJNda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed63b5c9-3128-4cc4-adf5-88c30fca7457"
      },
      "source": [
        "training_loop = train_model(model, train_task, eval_task, 90562, output_dir_expand)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 51306754\n",
            "Step      1: Ran 1 train steps in 15.08 secs\n",
            "Step      1: train CrossEntropyLoss |  0.69381428\n",
            "Step      1: eval  CrossEntropyLoss |  0.69212425\n",
            "Step      1: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step    100: Ran 99 train steps in 20.93 secs\n",
            "Step    100: train CrossEntropyLoss |  0.69148350\n",
            "Step    100: eval  CrossEntropyLoss |  0.69511348\n",
            "Step    100: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step    200: Ran 100 train steps in 16.41 secs\n",
            "Step    200: train CrossEntropyLoss |  0.68915695\n",
            "Step    200: eval  CrossEntropyLoss |  0.69455999\n",
            "Step    200: eval          Accuracy |  0.43750000\n",
            "\n",
            "Step    300: Ran 100 train steps in 13.61 secs\n",
            "Step    300: train CrossEntropyLoss |  0.68721557\n",
            "Step    300: eval  CrossEntropyLoss |  0.68443823\n",
            "Step    300: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step    400: Ran 100 train steps in 13.20 secs\n",
            "Step    400: train CrossEntropyLoss |  0.68618089\n",
            "Step    400: eval  CrossEntropyLoss |  0.70708752\n",
            "Step    400: eval          Accuracy |  0.31250000\n",
            "\n",
            "Step    500: Ran 100 train steps in 13.16 secs\n",
            "Step    500: train CrossEntropyLoss |  0.68460029\n",
            "Step    500: eval  CrossEntropyLoss |  0.68589658\n",
            "Step    500: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step    600: Ran 100 train steps in 13.41 secs\n",
            "Step    600: train CrossEntropyLoss |  0.68214929\n",
            "Step    600: eval  CrossEntropyLoss |  0.68098187\n",
            "Step    600: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step    700: Ran 100 train steps in 13.88 secs\n",
            "Step    700: train CrossEntropyLoss |  0.67752343\n",
            "Step    700: eval  CrossEntropyLoss |  0.67765218\n",
            "Step    700: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step    800: Ran 100 train steps in 13.34 secs\n",
            "Step    800: train CrossEntropyLoss |  0.67721695\n",
            "Step    800: eval  CrossEntropyLoss |  0.69092780\n",
            "Step    800: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step    900: Ran 100 train steps in 13.18 secs\n",
            "Step    900: train CrossEntropyLoss |  0.67755127\n",
            "Step    900: eval  CrossEntropyLoss |  0.67599005\n",
            "Step    900: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step   1000: Ran 100 train steps in 14.03 secs\n",
            "Step   1000: train CrossEntropyLoss |  0.67192388\n",
            "Step   1000: eval  CrossEntropyLoss |  0.66536599\n",
            "Step   1000: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step   1100: Ran 100 train steps in 13.19 secs\n",
            "Step   1100: train CrossEntropyLoss |  0.66922218\n",
            "Step   1100: eval  CrossEntropyLoss |  0.69741297\n",
            "Step   1100: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step   1200: Ran 100 train steps in 14.05 secs\n",
            "Step   1200: train CrossEntropyLoss |  0.66464639\n",
            "Step   1200: eval  CrossEntropyLoss |  0.67348349\n",
            "Step   1200: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step   1300: Ran 100 train steps in 13.55 secs\n",
            "Step   1300: train CrossEntropyLoss |  0.66204441\n",
            "Step   1300: eval  CrossEntropyLoss |  0.66405851\n",
            "Step   1300: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step   1400: Ran 100 train steps in 14.60 secs\n",
            "Step   1400: train CrossEntropyLoss |  0.66107297\n",
            "Step   1400: eval  CrossEntropyLoss |  0.63457155\n",
            "Step   1400: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step   1500: Ran 100 train steps in 14.19 secs\n",
            "Step   1500: train CrossEntropyLoss |  0.66045380\n",
            "Step   1500: eval  CrossEntropyLoss |  0.62628329\n",
            "Step   1500: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step   1600: Ran 100 train steps in 13.44 secs\n",
            "Step   1600: train CrossEntropyLoss |  0.65530187\n",
            "Step   1600: eval  CrossEntropyLoss |  0.68098181\n",
            "Step   1600: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step   1700: Ran 100 train steps in 14.19 secs\n",
            "Step   1700: train CrossEntropyLoss |  0.64840680\n",
            "Step   1700: eval  CrossEntropyLoss |  0.73091394\n",
            "Step   1700: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step   1800: Ran 100 train steps in 14.28 secs\n",
            "Step   1800: train CrossEntropyLoss |  0.64708000\n",
            "Step   1800: eval  CrossEntropyLoss |  0.66960824\n",
            "Step   1800: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step   1900: Ran 100 train steps in 13.45 secs\n",
            "Step   1900: train CrossEntropyLoss |  0.64065021\n",
            "Step   1900: eval  CrossEntropyLoss |  0.66742772\n",
            "Step   1900: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step   2000: Ran 100 train steps in 13.48 secs\n",
            "Step   2000: train CrossEntropyLoss |  0.64349145\n",
            "Step   2000: eval  CrossEntropyLoss |  0.65625602\n",
            "Step   2000: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step   2100: Ran 100 train steps in 14.47 secs\n",
            "Step   2100: train CrossEntropyLoss |  0.63907975\n",
            "Step   2100: eval  CrossEntropyLoss |  0.76383585\n",
            "Step   2100: eval          Accuracy |  0.31250000\n",
            "\n",
            "Step   2200: Ran 100 train steps in 14.56 secs\n",
            "Step   2200: train CrossEntropyLoss |  0.64054841\n",
            "Step   2200: eval  CrossEntropyLoss |  0.64766794\n",
            "Step   2200: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step   2300: Ran 100 train steps in 14.37 secs\n",
            "Step   2300: train CrossEntropyLoss |  0.62754548\n",
            "Step   2300: eval  CrossEntropyLoss |  0.59422135\n",
            "Step   2300: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step   2400: Ran 100 train steps in 14.36 secs\n",
            "Step   2400: train CrossEntropyLoss |  0.62386823\n",
            "Step   2400: eval  CrossEntropyLoss |  0.61997730\n",
            "Step   2400: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step   2500: Ran 100 train steps in 13.57 secs\n",
            "Step   2500: train CrossEntropyLoss |  0.61907774\n",
            "Step   2500: eval  CrossEntropyLoss |  0.53420264\n",
            "Step   2500: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step   2600: Ran 100 train steps in 14.50 secs\n",
            "Step   2600: train CrossEntropyLoss |  0.62213647\n",
            "Step   2600: eval  CrossEntropyLoss |  0.64335090\n",
            "Step   2600: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step   2700: Ran 100 train steps in 15.19 secs\n",
            "Step   2700: train CrossEntropyLoss |  0.61064053\n",
            "Step   2700: eval  CrossEntropyLoss |  0.59023178\n",
            "Step   2700: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step   2800: Ran 100 train steps in 13.78 secs\n",
            "Step   2800: train CrossEntropyLoss |  0.60839862\n",
            "Step   2800: eval  CrossEntropyLoss |  0.65795332\n",
            "Step   2800: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step   2900: Ran 100 train steps in 15.11 secs\n",
            "Step   2900: train CrossEntropyLoss |  0.61100936\n",
            "Step   2900: eval  CrossEntropyLoss |  0.58756542\n",
            "Step   2900: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step   3000: Ran 100 train steps in 14.58 secs\n",
            "Step   3000: train CrossEntropyLoss |  0.60184348\n",
            "Step   3000: eval  CrossEntropyLoss |  0.53193110\n",
            "Step   3000: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step   3100: Ran 100 train steps in 13.82 secs\n",
            "Step   3100: train CrossEntropyLoss |  0.60656571\n",
            "Step   3100: eval  CrossEntropyLoss |  0.53067601\n",
            "Step   3100: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step   3200: Ran 100 train steps in 14.62 secs\n",
            "Step   3200: train CrossEntropyLoss |  0.59771377\n",
            "Step   3200: eval  CrossEntropyLoss |  0.61926448\n",
            "Step   3200: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step   3300: Ran 100 train steps in 13.86 secs\n",
            "Step   3300: train CrossEntropyLoss |  0.59825063\n",
            "Step   3300: eval  CrossEntropyLoss |  0.61361724\n",
            "Step   3300: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step   3400: Ran 100 train steps in 13.83 secs\n",
            "Step   3400: train CrossEntropyLoss |  0.58524621\n",
            "Step   3400: eval  CrossEntropyLoss |  0.50178254\n",
            "Step   3400: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step   3500: Ran 100 train steps in 13.81 secs\n",
            "Step   3500: train CrossEntropyLoss |  0.58088332\n",
            "Step   3500: eval  CrossEntropyLoss |  0.64942968\n",
            "Step   3500: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step   3600: Ran 100 train steps in 14.66 secs\n",
            "Step   3600: train CrossEntropyLoss |  0.59899157\n",
            "Step   3600: eval  CrossEntropyLoss |  0.54540032\n",
            "Step   3600: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step   3700: Ran 100 train steps in 14.70 secs\n",
            "Step   3700: train CrossEntropyLoss |  0.58446258\n",
            "Step   3700: eval  CrossEntropyLoss |  0.67190081\n",
            "Step   3700: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step   3800: Ran 100 train steps in 14.08 secs\n",
            "Step   3800: train CrossEntropyLoss |  0.57208490\n",
            "Step   3800: eval  CrossEntropyLoss |  0.53054339\n",
            "Step   3800: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step   3900: Ran 100 train steps in 14.74 secs\n",
            "Step   3900: train CrossEntropyLoss |  0.58330870\n",
            "Step   3900: eval  CrossEntropyLoss |  0.57718533\n",
            "Step   3900: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step   4000: Ran 100 train steps in 15.30 secs\n",
            "Step   4000: train CrossEntropyLoss |  0.57599229\n",
            "Step   4000: eval  CrossEntropyLoss |  0.67085534\n",
            "Step   4000: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step   4100: Ran 100 train steps in 13.99 secs\n",
            "Step   4100: train CrossEntropyLoss |  0.57743585\n",
            "Step   4100: eval  CrossEntropyLoss |  0.55709535\n",
            "Step   4100: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step   4200: Ran 100 train steps in 13.95 secs\n",
            "Step   4200: train CrossEntropyLoss |  0.56474531\n",
            "Step   4200: eval  CrossEntropyLoss |  0.55097646\n",
            "Step   4200: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step   4300: Ran 100 train steps in 14.91 secs\n",
            "Step   4300: train CrossEntropyLoss |  0.58080804\n",
            "Step   4300: eval  CrossEntropyLoss |  0.60117775\n",
            "Step   4300: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step   4400: Ran 100 train steps in 14.20 secs\n",
            "Step   4400: train CrossEntropyLoss |  0.57030857\n",
            "Step   4400: eval  CrossEntropyLoss |  0.61644709\n",
            "Step   4400: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step   4500: Ran 100 train steps in 14.03 secs\n",
            "Step   4500: train CrossEntropyLoss |  0.56828207\n",
            "Step   4500: eval  CrossEntropyLoss |  0.61750215\n",
            "Step   4500: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step   4600: Ran 100 train steps in 14.04 secs\n",
            "Step   4600: train CrossEntropyLoss |  0.57231259\n",
            "Step   4600: eval  CrossEntropyLoss |  0.56840920\n",
            "Step   4600: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step   4700: Ran 100 train steps in 14.07 secs\n",
            "Step   4700: train CrossEntropyLoss |  0.56452489\n",
            "Step   4700: eval  CrossEntropyLoss |  0.58079892\n",
            "Step   4700: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step   4800: Ran 100 train steps in 14.09 secs\n",
            "Step   4800: train CrossEntropyLoss |  0.55649513\n",
            "Step   4800: eval  CrossEntropyLoss |  0.49017867\n",
            "Step   4800: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step   4900: Ran 100 train steps in 14.26 secs\n",
            "Step   4900: train CrossEntropyLoss |  0.56721127\n",
            "Step   4900: eval  CrossEntropyLoss |  0.68654680\n",
            "Step   4900: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step   5000: Ran 100 train steps in 14.14 secs\n",
            "Step   5000: train CrossEntropyLoss |  0.54626274\n",
            "Step   5000: eval  CrossEntropyLoss |  0.48176700\n",
            "Step   5000: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step   5100: Ran 100 train steps in 14.15 secs\n",
            "Step   5100: train CrossEntropyLoss |  0.54975849\n",
            "Step   5100: eval  CrossEntropyLoss |  0.50372052\n",
            "Step   5100: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step   5200: Ran 100 train steps in 15.01 secs\n",
            "Step   5200: train CrossEntropyLoss |  0.55652750\n",
            "Step   5200: eval  CrossEntropyLoss |  0.68943191\n",
            "Step   5200: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step   5300: Ran 100 train steps in 14.22 secs\n",
            "Step   5300: train CrossEntropyLoss |  0.54175019\n",
            "Step   5300: eval  CrossEntropyLoss |  0.56682652\n",
            "Step   5300: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step   5400: Ran 100 train steps in 14.25 secs\n",
            "Step   5400: train CrossEntropyLoss |  0.53590322\n",
            "Step   5400: eval  CrossEntropyLoss |  0.73385370\n",
            "Step   5400: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step   5500: Ran 100 train steps in 14.32 secs\n",
            "Step   5500: train CrossEntropyLoss |  0.53733522\n",
            "Step   5500: eval  CrossEntropyLoss |  0.45950660\n",
            "Step   5500: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step   5600: Ran 100 train steps in 14.24 secs\n",
            "Step   5600: train CrossEntropyLoss |  0.54849416\n",
            "Step   5600: eval  CrossEntropyLoss |  0.46309736\n",
            "Step   5600: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step   5700: Ran 100 train steps in 14.22 secs\n",
            "Step   5700: train CrossEntropyLoss |  0.55984586\n",
            "Step   5700: eval  CrossEntropyLoss |  0.54082370\n",
            "Step   5700: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step   5800: Ran 100 train steps in 14.29 secs\n",
            "Step   5800: train CrossEntropyLoss |  0.55398297\n",
            "Step   5800: eval  CrossEntropyLoss |  0.56470209\n",
            "Step   5800: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step   5900: Ran 100 train steps in 14.50 secs\n",
            "Step   5900: train CrossEntropyLoss |  0.54422182\n",
            "Step   5900: eval  CrossEntropyLoss |  0.58462918\n",
            "Step   5900: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step   6000: Ran 100 train steps in 14.45 secs\n",
            "Step   6000: train CrossEntropyLoss |  0.55245811\n",
            "Step   6000: eval  CrossEntropyLoss |  0.63253081\n",
            "Step   6000: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step   6100: Ran 100 train steps in 14.31 secs\n",
            "Step   6100: train CrossEntropyLoss |  0.55013531\n",
            "Step   6100: eval  CrossEntropyLoss |  0.55199540\n",
            "Step   6100: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step   6200: Ran 100 train steps in 14.32 secs\n",
            "Step   6200: train CrossEntropyLoss |  0.55020952\n",
            "Step   6200: eval  CrossEntropyLoss |  0.68970823\n",
            "Step   6200: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step   6300: Ran 100 train steps in 14.32 secs\n",
            "Step   6300: train CrossEntropyLoss |  0.53907311\n",
            "Step   6300: eval  CrossEntropyLoss |  0.68074715\n",
            "Step   6300: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step   6400: Ran 100 train steps in 14.36 secs\n",
            "Step   6400: train CrossEntropyLoss |  0.53949779\n",
            "Step   6400: eval  CrossEntropyLoss |  0.47356901\n",
            "Step   6400: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step   6500: Ran 100 train steps in 14.53 secs\n",
            "Step   6500: train CrossEntropyLoss |  0.52689904\n",
            "Step   6500: eval  CrossEntropyLoss |  0.57143581\n",
            "Step   6500: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step   6600: Ran 100 train steps in 14.60 secs\n",
            "Step   6600: train CrossEntropyLoss |  0.55439734\n",
            "Step   6600: eval  CrossEntropyLoss |  0.43231380\n",
            "Step   6600: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step   6700: Ran 100 train steps in 14.63 secs\n",
            "Step   6700: train CrossEntropyLoss |  0.54803616\n",
            "Step   6700: eval  CrossEntropyLoss |  0.66621280\n",
            "Step   6700: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step   6800: Ran 100 train steps in 14.38 secs\n",
            "Step   6800: train CrossEntropyLoss |  0.50754452\n",
            "Step   6800: eval  CrossEntropyLoss |  0.42548436\n",
            "Step   6800: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step   6900: Ran 100 train steps in 14.43 secs\n",
            "Step   6900: train CrossEntropyLoss |  0.52627879\n",
            "Step   6900: eval  CrossEntropyLoss |  0.53449506\n",
            "Step   6900: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step   7000: Ran 100 train steps in 14.50 secs\n",
            "Step   7000: train CrossEntropyLoss |  0.52739185\n",
            "Step   7000: eval  CrossEntropyLoss |  0.40264314\n",
            "Step   7000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step   7100: Ran 100 train steps in 15.36 secs\n",
            "Step   7100: train CrossEntropyLoss |  0.54160744\n",
            "Step   7100: eval  CrossEntropyLoss |  0.46798477\n",
            "Step   7100: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step   7200: Ran 100 train steps in 14.51 secs\n",
            "Step   7200: train CrossEntropyLoss |  0.53786409\n",
            "Step   7200: eval  CrossEntropyLoss |  0.36874998\n",
            "Step   7200: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step   7300: Ran 100 train steps in 14.46 secs\n",
            "Step   7300: train CrossEntropyLoss |  0.52406472\n",
            "Step   7300: eval  CrossEntropyLoss |  0.55660230\n",
            "Step   7300: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step   7400: Ran 100 train steps in 14.51 secs\n",
            "Step   7400: train CrossEntropyLoss |  0.53292668\n",
            "Step   7400: eval  CrossEntropyLoss |  0.38737780\n",
            "Step   7400: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step   7500: Ran 100 train steps in 14.53 secs\n",
            "Step   7500: train CrossEntropyLoss |  0.53524858\n",
            "Step   7500: eval  CrossEntropyLoss |  0.46841949\n",
            "Step   7500: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step   7600: Ran 100 train steps in 14.55 secs\n",
            "Step   7600: train CrossEntropyLoss |  0.53025478\n",
            "Step   7600: eval  CrossEntropyLoss |  0.70069480\n",
            "Step   7600: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step   7700: Ran 100 train steps in 14.68 secs\n",
            "Step   7700: train CrossEntropyLoss |  0.52684522\n",
            "Step   7700: eval  CrossEntropyLoss |  0.44021609\n",
            "Step   7700: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step   7800: Ran 100 train steps in 14.56 secs\n",
            "Step   7800: train CrossEntropyLoss |  0.54649043\n",
            "Step   7800: eval  CrossEntropyLoss |  0.56520689\n",
            "Step   7800: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step   7900: Ran 100 train steps in 14.64 secs\n",
            "Step   7900: train CrossEntropyLoss |  0.52686185\n",
            "Step   7900: eval  CrossEntropyLoss |  0.60412294\n",
            "Step   7900: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step   8000: Ran 100 train steps in 15.39 secs\n",
            "Step   8000: train CrossEntropyLoss |  0.51008117\n",
            "Step   8000: eval  CrossEntropyLoss |  0.54601228\n",
            "Step   8000: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step   8100: Ran 100 train steps in 14.63 secs\n",
            "Step   8100: train CrossEntropyLoss |  0.52521032\n",
            "Step   8100: eval  CrossEntropyLoss |  0.52404648\n",
            "Step   8100: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step   8200: Ran 100 train steps in 14.71 secs\n",
            "Step   8200: train CrossEntropyLoss |  0.52306306\n",
            "Step   8200: eval  CrossEntropyLoss |  0.48911273\n",
            "Step   8200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step   8300: Ran 100 train steps in 14.70 secs\n",
            "Step   8300: train CrossEntropyLoss |  0.52894342\n",
            "Step   8300: eval  CrossEntropyLoss |  0.55231494\n",
            "Step   8300: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step   8400: Ran 100 train steps in 14.64 secs\n",
            "Step   8400: train CrossEntropyLoss |  0.50591564\n",
            "Step   8400: eval  CrossEntropyLoss |  0.53542024\n",
            "Step   8400: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step   8500: Ran 100 train steps in 14.63 secs\n",
            "Step   8500: train CrossEntropyLoss |  0.53376997\n",
            "Step   8500: eval  CrossEntropyLoss |  0.52511472\n",
            "Step   8500: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step   8600: Ran 100 train steps in 15.65 secs\n",
            "Step   8600: train CrossEntropyLoss |  0.53688842\n",
            "Step   8600: eval  CrossEntropyLoss |  0.54217732\n",
            "Step   8600: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step   8700: Ran 100 train steps in 14.87 secs\n",
            "Step   8700: train CrossEntropyLoss |  0.52481282\n",
            "Step   8700: eval  CrossEntropyLoss |  0.52054179\n",
            "Step   8700: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step   8800: Ran 100 train steps in 14.80 secs\n",
            "Step   8800: train CrossEntropyLoss |  0.51474971\n",
            "Step   8800: eval  CrossEntropyLoss |  0.51995713\n",
            "Step   8800: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step   8900: Ran 100 train steps in 14.69 secs\n",
            "Step   8900: train CrossEntropyLoss |  0.51533294\n",
            "Step   8900: eval  CrossEntropyLoss |  0.69135118\n",
            "Step   8900: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step   9000: Ran 100 train steps in 14.74 secs\n",
            "Step   9000: train CrossEntropyLoss |  0.50828916\n",
            "Step   9000: eval  CrossEntropyLoss |  0.36217907\n",
            "Step   9000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step   9100: Ran 100 train steps in 14.84 secs\n",
            "Step   9100: train CrossEntropyLoss |  0.53191578\n",
            "Step   9100: eval  CrossEntropyLoss |  0.39530438\n",
            "Step   9100: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step   9200: Ran 100 train steps in 14.76 secs\n",
            "Step   9200: train CrossEntropyLoss |  0.50454372\n",
            "Step   9200: eval  CrossEntropyLoss |  0.42866242\n",
            "Step   9200: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step   9300: Ran 100 train steps in 14.87 secs\n",
            "Step   9300: train CrossEntropyLoss |  0.50084859\n",
            "Step   9300: eval  CrossEntropyLoss |  0.72190976\n",
            "Step   9300: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step   9400: Ran 100 train steps in 14.76 secs\n",
            "Step   9400: train CrossEntropyLoss |  0.53050381\n",
            "Step   9400: eval  CrossEntropyLoss |  0.40436104\n",
            "Step   9400: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step   9500: Ran 100 train steps in 14.77 secs\n",
            "Step   9500: train CrossEntropyLoss |  0.49644393\n",
            "Step   9500: eval  CrossEntropyLoss |  0.42935619\n",
            "Step   9500: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step   9600: Ran 100 train steps in 15.56 secs\n",
            "Step   9600: train CrossEntropyLoss |  0.52763951\n",
            "Step   9600: eval  CrossEntropyLoss |  0.50275213\n",
            "Step   9600: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step   9700: Ran 100 train steps in 15.09 secs\n",
            "Step   9700: train CrossEntropyLoss |  0.51384884\n",
            "Step   9700: eval  CrossEntropyLoss |  0.59244174\n",
            "Step   9700: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step   9800: Ran 100 train steps in 14.85 secs\n",
            "Step   9800: train CrossEntropyLoss |  0.49907973\n",
            "Step   9800: eval  CrossEntropyLoss |  0.61618757\n",
            "Step   9800: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step   9900: Ran 100 train steps in 14.89 secs\n",
            "Step   9900: train CrossEntropyLoss |  0.49751875\n",
            "Step   9900: eval  CrossEntropyLoss |  0.46259230\n",
            "Step   9900: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  10000: Ran 100 train steps in 14.85 secs\n",
            "Step  10000: train CrossEntropyLoss |  0.50409287\n",
            "Step  10000: eval  CrossEntropyLoss |  0.67392766\n",
            "Step  10000: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  10100: Ran 100 train steps in 14.87 secs\n",
            "Step  10100: train CrossEntropyLoss |  0.50330108\n",
            "Step  10100: eval  CrossEntropyLoss |  0.49583471\n",
            "Step  10100: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  10200: Ran 100 train steps in 14.79 secs\n",
            "Step  10200: train CrossEntropyLoss |  0.50648522\n",
            "Step  10200: eval  CrossEntropyLoss |  0.45099849\n",
            "Step  10200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  10300: Ran 100 train steps in 14.86 secs\n",
            "Step  10300: train CrossEntropyLoss |  0.50522053\n",
            "Step  10300: eval  CrossEntropyLoss |  0.67706829\n",
            "Step  10300: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  10400: Ran 100 train steps in 15.01 secs\n",
            "Step  10400: train CrossEntropyLoss |  0.50830090\n",
            "Step  10400: eval  CrossEntropyLoss |  0.53855711\n",
            "Step  10400: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  10500: Ran 100 train steps in 14.87 secs\n",
            "Step  10500: train CrossEntropyLoss |  0.50691557\n",
            "Step  10500: eval  CrossEntropyLoss |  0.50991106\n",
            "Step  10500: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  10600: Ran 100 train steps in 14.89 secs\n",
            "Step  10600: train CrossEntropyLoss |  0.51195306\n",
            "Step  10600: eval  CrossEntropyLoss |  0.46192315\n",
            "Step  10600: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  10700: Ran 100 train steps in 16.86 secs\n",
            "Step  10700: train CrossEntropyLoss |  0.48923430\n",
            "Step  10700: eval  CrossEntropyLoss |  0.46630025\n",
            "Step  10700: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  10800: Ran 100 train steps in 14.92 secs\n",
            "Step  10800: train CrossEntropyLoss |  0.51775247\n",
            "Step  10800: eval  CrossEntropyLoss |  0.72490591\n",
            "Step  10800: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  10900: Ran 100 train steps in 15.05 secs\n",
            "Step  10900: train CrossEntropyLoss |  0.50473607\n",
            "Step  10900: eval  CrossEntropyLoss |  0.55940837\n",
            "Step  10900: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  11000: Ran 100 train steps in 14.98 secs\n",
            "Step  11000: train CrossEntropyLoss |  0.50751913\n",
            "Step  11000: eval  CrossEntropyLoss |  0.56392115\n",
            "Step  11000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  11100: Ran 100 train steps in 15.06 secs\n",
            "Step  11100: train CrossEntropyLoss |  0.51735413\n",
            "Step  11100: eval  CrossEntropyLoss |  0.44108814\n",
            "Step  11100: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  11200: Ran 100 train steps in 15.03 secs\n",
            "Step  11200: train CrossEntropyLoss |  0.50607198\n",
            "Step  11200: eval  CrossEntropyLoss |  0.54439980\n",
            "Step  11200: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  11300: Ran 100 train steps in 14.96 secs\n",
            "Step  11300: train CrossEntropyLoss |  0.49609640\n",
            "Step  11300: eval  CrossEntropyLoss |  0.39869046\n",
            "Step  11300: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  11400: Ran 100 train steps in 15.90 secs\n",
            "Step  11400: train CrossEntropyLoss |  0.51055819\n",
            "Step  11400: eval  CrossEntropyLoss |  0.53157240\n",
            "Step  11400: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  11500: Ran 100 train steps in 15.09 secs\n",
            "Step  11500: train CrossEntropyLoss |  0.51405507\n",
            "Step  11500: eval  CrossEntropyLoss |  0.63449883\n",
            "Step  11500: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  11600: Ran 100 train steps in 15.06 secs\n",
            "Step  11600: train CrossEntropyLoss |  0.50984377\n",
            "Step  11600: eval  CrossEntropyLoss |  0.34118721\n",
            "Step  11600: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  11700: Ran 100 train steps in 15.09 secs\n",
            "Step  11700: train CrossEntropyLoss |  0.48870811\n",
            "Step  11700: eval  CrossEntropyLoss |  0.44128019\n",
            "Step  11700: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  11800: Ran 100 train steps in 15.01 secs\n",
            "Step  11800: train CrossEntropyLoss |  0.49281436\n",
            "Step  11800: eval  CrossEntropyLoss |  0.44209406\n",
            "Step  11800: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  11900: Ran 100 train steps in 15.07 secs\n",
            "Step  11900: train CrossEntropyLoss |  0.49781761\n",
            "Step  11900: eval  CrossEntropyLoss |  0.58081377\n",
            "Step  11900: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  12000: Ran 100 train steps in 15.14 secs\n",
            "Step  12000: train CrossEntropyLoss |  0.51289886\n",
            "Step  12000: eval  CrossEntropyLoss |  0.74482143\n",
            "Step  12000: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  12100: Ran 100 train steps in 15.04 secs\n",
            "Step  12100: train CrossEntropyLoss |  0.50089550\n",
            "Step  12100: eval  CrossEntropyLoss |  0.57103920\n",
            "Step  12100: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  12200: Ran 100 train steps in 15.09 secs\n",
            "Step  12200: train CrossEntropyLoss |  0.51159286\n",
            "Step  12200: eval  CrossEntropyLoss |  0.36019397\n",
            "Step  12200: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  12300: Ran 100 train steps in 15.91 secs\n",
            "Step  12300: train CrossEntropyLoss |  0.48503366\n",
            "Step  12300: eval  CrossEntropyLoss |  0.74042392\n",
            "Step  12300: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  12400: Ran 100 train steps in 15.06 secs\n",
            "Step  12400: train CrossEntropyLoss |  0.50446886\n",
            "Step  12400: eval  CrossEntropyLoss |  0.43776283\n",
            "Step  12400: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  12500: Ran 100 train steps in 15.25 secs\n",
            "Step  12500: train CrossEntropyLoss |  0.48516390\n",
            "Step  12500: eval  CrossEntropyLoss |  0.38014716\n",
            "Step  12500: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  12600: Ran 100 train steps in 15.11 secs\n",
            "Step  12600: train CrossEntropyLoss |  0.51310265\n",
            "Step  12600: eval  CrossEntropyLoss |  0.34136987\n",
            "Step  12600: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  12700: Ran 100 train steps in 16.08 secs\n",
            "Step  12700: train CrossEntropyLoss |  0.50205016\n",
            "Step  12700: eval  CrossEntropyLoss |  0.51975727\n",
            "Step  12700: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  12800: Ran 100 train steps in 15.26 secs\n",
            "Step  12800: train CrossEntropyLoss |  0.48461908\n",
            "Step  12800: eval  CrossEntropyLoss |  0.51782382\n",
            "Step  12800: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  12900: Ran 100 train steps in 15.16 secs\n",
            "Step  12900: train CrossEntropyLoss |  0.50381750\n",
            "Step  12900: eval  CrossEntropyLoss |  0.56426519\n",
            "Step  12900: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  13000: Ran 100 train steps in 15.26 secs\n",
            "Step  13000: train CrossEntropyLoss |  0.49364057\n",
            "Step  13000: eval  CrossEntropyLoss |  0.54122794\n",
            "Step  13000: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  13100: Ran 100 train steps in 15.34 secs\n",
            "Step  13100: train CrossEntropyLoss |  0.48493674\n",
            "Step  13100: eval  CrossEntropyLoss |  0.40690553\n",
            "Step  13100: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  13200: Ran 100 train steps in 15.21 secs\n",
            "Step  13200: train CrossEntropyLoss |  0.49644363\n",
            "Step  13200: eval  CrossEntropyLoss |  0.34247261\n",
            "Step  13200: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  13300: Ran 100 train steps in 15.26 secs\n",
            "Step  13300: train CrossEntropyLoss |  0.50302231\n",
            "Step  13300: eval  CrossEntropyLoss |  0.54524070\n",
            "Step  13300: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  13400: Ran 100 train steps in 15.21 secs\n",
            "Step  13400: train CrossEntropyLoss |  0.51060277\n",
            "Step  13400: eval  CrossEntropyLoss |  0.46143705\n",
            "Step  13400: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  13500: Ran 100 train steps in 15.30 secs\n",
            "Step  13500: train CrossEntropyLoss |  0.48361963\n",
            "Step  13500: eval  CrossEntropyLoss |  0.46540347\n",
            "Step  13500: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  13600: Ran 100 train steps in 15.31 secs\n",
            "Step  13600: train CrossEntropyLoss |  0.49354061\n",
            "Step  13600: eval  CrossEntropyLoss |  0.48475096\n",
            "Step  13600: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  13700: Ran 100 train steps in 15.18 secs\n",
            "Step  13700: train CrossEntropyLoss |  0.50272208\n",
            "Step  13700: eval  CrossEntropyLoss |  0.65712106\n",
            "Step  13700: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  13800: Ran 100 train steps in 15.47 secs\n",
            "Step  13800: train CrossEntropyLoss |  0.50909042\n",
            "Step  13800: eval  CrossEntropyLoss |  0.60780323\n",
            "Step  13800: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  13900: Ran 100 train steps in 15.34 secs\n",
            "Step  13900: train CrossEntropyLoss |  0.49012384\n",
            "Step  13900: eval  CrossEntropyLoss |  0.44995123\n",
            "Step  13900: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  14000: Ran 100 train steps in 15.32 secs\n",
            "Step  14000: train CrossEntropyLoss |  0.48572546\n",
            "Step  14000: eval  CrossEntropyLoss |  0.52411902\n",
            "Step  14000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  14100: Ran 100 train steps in 15.52 secs\n",
            "Step  14100: train CrossEntropyLoss |  0.49719313\n",
            "Step  14100: eval  CrossEntropyLoss |  0.52429670\n",
            "Step  14100: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  14200: Ran 100 train steps in 16.23 secs\n",
            "Step  14200: train CrossEntropyLoss |  0.49041477\n",
            "Step  14200: eval  CrossEntropyLoss |  0.69483447\n",
            "Step  14200: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  14300: Ran 100 train steps in 15.32 secs\n",
            "Step  14300: train CrossEntropyLoss |  0.48756263\n",
            "Step  14300: eval  CrossEntropyLoss |  0.26823580\n",
            "Step  14300: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  14400: Ran 100 train steps in 15.39 secs\n",
            "Step  14400: train CrossEntropyLoss |  0.47431549\n",
            "Step  14400: eval  CrossEntropyLoss |  0.30221048\n",
            "Step  14400: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  14500: Ran 100 train steps in 15.38 secs\n",
            "Step  14500: train CrossEntropyLoss |  0.48883882\n",
            "Step  14500: eval  CrossEntropyLoss |  0.56052661\n",
            "Step  14500: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  14600: Ran 100 train steps in 15.59 secs\n",
            "Step  14600: train CrossEntropyLoss |  0.48949704\n",
            "Step  14600: eval  CrossEntropyLoss |  0.57186127\n",
            "Step  14600: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  14700: Ran 100 train steps in 15.45 secs\n",
            "Step  14700: train CrossEntropyLoss |  0.49686423\n",
            "Step  14700: eval  CrossEntropyLoss |  0.25987011\n",
            "Step  14700: eval          Accuracy |  1.00000000\n",
            "\n",
            "Step  14800: Ran 100 train steps in 15.62 secs\n",
            "Step  14800: train CrossEntropyLoss |  0.51568389\n",
            "Step  14800: eval  CrossEntropyLoss |  0.60014307\n",
            "Step  14800: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  14900: Ran 100 train steps in 15.74 secs\n",
            "Step  14900: train CrossEntropyLoss |  0.50292581\n",
            "Step  14900: eval  CrossEntropyLoss |  0.45771921\n",
            "Step  14900: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  15000: Ran 100 train steps in 15.49 secs\n",
            "Step  15000: train CrossEntropyLoss |  0.50598907\n",
            "Step  15000: eval  CrossEntropyLoss |  0.44371924\n",
            "Step  15000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  15100: Ran 100 train steps in 15.68 secs\n",
            "Step  15100: train CrossEntropyLoss |  0.49477732\n",
            "Step  15100: eval  CrossEntropyLoss |  0.54311562\n",
            "Step  15100: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  15200: Ran 100 train steps in 15.54 secs\n",
            "Step  15200: train CrossEntropyLoss |  0.49430671\n",
            "Step  15200: eval  CrossEntropyLoss |  0.43148315\n",
            "Step  15200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  15300: Ran 100 train steps in 16.29 secs\n",
            "Step  15300: train CrossEntropyLoss |  0.50984460\n",
            "Step  15300: eval  CrossEntropyLoss |  0.45960724\n",
            "Step  15300: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  15400: Ran 100 train steps in 15.84 secs\n",
            "Step  15400: train CrossEntropyLoss |  0.47686562\n",
            "Step  15400: eval  CrossEntropyLoss |  0.63319504\n",
            "Step  15400: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  15500: Ran 100 train steps in 15.53 secs\n",
            "Step  15500: train CrossEntropyLoss |  0.49060938\n",
            "Step  15500: eval  CrossEntropyLoss |  0.39789897\n",
            "Step  15500: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  15600: Ran 100 train steps in 15.62 secs\n",
            "Step  15600: train CrossEntropyLoss |  0.49485385\n",
            "Step  15600: eval  CrossEntropyLoss |  0.37190208\n",
            "Step  15600: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  15700: Ran 100 train steps in 15.61 secs\n",
            "Step  15700: train CrossEntropyLoss |  0.50161004\n",
            "Step  15700: eval  CrossEntropyLoss |  0.38280737\n",
            "Step  15700: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  15800: Ran 100 train steps in 15.50 secs\n",
            "Step  15800: train CrossEntropyLoss |  0.49159628\n",
            "Step  15800: eval  CrossEntropyLoss |  0.64441419\n",
            "Step  15800: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  15900: Ran 100 train steps in 15.58 secs\n",
            "Step  15900: train CrossEntropyLoss |  0.50445843\n",
            "Step  15900: eval  CrossEntropyLoss |  0.34868631\n",
            "Step  15900: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  16000: Ran 100 train steps in 15.53 secs\n",
            "Step  16000: train CrossEntropyLoss |  0.46169096\n",
            "Step  16000: eval  CrossEntropyLoss |  0.37394276\n",
            "Step  16000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  16100: Ran 100 train steps in 15.69 secs\n",
            "Step  16100: train CrossEntropyLoss |  0.49795824\n",
            "Step  16100: eval  CrossEntropyLoss |  0.60367912\n",
            "Step  16100: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  16200: Ran 100 train steps in 15.65 secs\n",
            "Step  16200: train CrossEntropyLoss |  0.49118567\n",
            "Step  16200: eval  CrossEntropyLoss |  0.69750029\n",
            "Step  16200: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  16300: Ran 100 train steps in 15.91 secs\n",
            "Step  16300: train CrossEntropyLoss |  0.49813819\n",
            "Step  16300: eval  CrossEntropyLoss |  0.79474425\n",
            "Step  16300: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  16400: Ran 100 train steps in 15.62 secs\n",
            "Step  16400: train CrossEntropyLoss |  0.48669028\n",
            "Step  16400: eval  CrossEntropyLoss |  0.65323007\n",
            "Step  16400: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step  16500: Ran 100 train steps in 15.66 secs\n",
            "Step  16500: train CrossEntropyLoss |  0.48641086\n",
            "Step  16500: eval  CrossEntropyLoss |  0.42083356\n",
            "Step  16500: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  16600: Ran 100 train steps in 15.70 secs\n",
            "Step  16600: train CrossEntropyLoss |  0.45747927\n",
            "Step  16600: eval  CrossEntropyLoss |  0.45464009\n",
            "Step  16600: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  16700: Ran 100 train steps in 16.65 secs\n",
            "Step  16700: train CrossEntropyLoss |  0.46994707\n",
            "Step  16700: eval  CrossEntropyLoss |  0.52227449\n",
            "Step  16700: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  16800: Ran 100 train steps in 15.72 secs\n",
            "Step  16800: train CrossEntropyLoss |  0.48755178\n",
            "Step  16800: eval  CrossEntropyLoss |  0.61998188\n",
            "Step  16800: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  16900: Ran 100 train steps in 15.74 secs\n",
            "Step  16900: train CrossEntropyLoss |  0.50122124\n",
            "Step  16900: eval  CrossEntropyLoss |  0.45756209\n",
            "Step  16900: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  17000: Ran 100 train steps in 15.76 secs\n",
            "Step  17000: train CrossEntropyLoss |  0.49311602\n",
            "Step  17000: eval  CrossEntropyLoss |  0.69512165\n",
            "Step  17000: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  17100: Ran 100 train steps in 15.69 secs\n",
            "Step  17100: train CrossEntropyLoss |  0.46515819\n",
            "Step  17100: eval  CrossEntropyLoss |  0.36150041\n",
            "Step  17100: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  17200: Ran 100 train steps in 15.72 secs\n",
            "Step  17200: train CrossEntropyLoss |  0.48254478\n",
            "Step  17200: eval  CrossEntropyLoss |  0.41542640\n",
            "Step  17200: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  17300: Ran 100 train steps in 15.69 secs\n",
            "Step  17300: train CrossEntropyLoss |  0.47298241\n",
            "Step  17300: eval  CrossEntropyLoss |  0.46006414\n",
            "Step  17300: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  17400: Ran 100 train steps in 15.69 secs\n",
            "Step  17400: train CrossEntropyLoss |  0.50306803\n",
            "Step  17400: eval  CrossEntropyLoss |  0.33296633\n",
            "Step  17400: eval          Accuracy |  1.00000000\n",
            "\n",
            "Step  17500: Ran 100 train steps in 15.63 secs\n",
            "Step  17500: train CrossEntropyLoss |  0.48930135\n",
            "Step  17500: eval  CrossEntropyLoss |  0.34555960\n",
            "Step  17500: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  17600: Ran 100 train steps in 15.78 secs\n",
            "Step  17600: train CrossEntropyLoss |  0.49885893\n",
            "Step  17600: eval  CrossEntropyLoss |  0.36673316\n",
            "Step  17600: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  17700: Ran 100 train steps in 15.76 secs\n",
            "Step  17700: train CrossEntropyLoss |  0.47816813\n",
            "Step  17700: eval  CrossEntropyLoss |  0.26513430\n",
            "Step  17700: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  17800: Ran 100 train steps in 15.74 secs\n",
            "Step  17800: train CrossEntropyLoss |  0.47728702\n",
            "Step  17800: eval  CrossEntropyLoss |  0.54719287\n",
            "Step  17800: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  17900: Ran 100 train steps in 15.71 secs\n",
            "Step  17900: train CrossEntropyLoss |  0.49145132\n",
            "Step  17900: eval  CrossEntropyLoss |  0.61221856\n",
            "Step  17900: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  18000: Ran 100 train steps in 15.79 secs\n",
            "Step  18000: train CrossEntropyLoss |  0.49806309\n",
            "Step  18000: eval  CrossEntropyLoss |  0.44406801\n",
            "Step  18000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  18100: Ran 100 train steps in 15.80 secs\n",
            "Step  18100: train CrossEntropyLoss |  0.45527506\n",
            "Step  18100: eval  CrossEntropyLoss |  0.56472933\n",
            "Step  18100: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  18200: Ran 100 train steps in 16.66 secs\n",
            "Step  18200: train CrossEntropyLoss |  0.49333626\n",
            "Step  18200: eval  CrossEntropyLoss |  0.79619211\n",
            "Step  18200: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step  18300: Ran 100 train steps in 15.76 secs\n",
            "Step  18300: train CrossEntropyLoss |  0.47240543\n",
            "Step  18300: eval  CrossEntropyLoss |  0.46709415\n",
            "Step  18300: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  18400: Ran 100 train steps in 15.77 secs\n",
            "Step  18400: train CrossEntropyLoss |  0.48551840\n",
            "Step  18400: eval  CrossEntropyLoss |  0.47797382\n",
            "Step  18400: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  18500: Ran 100 train steps in 15.86 secs\n",
            "Step  18500: train CrossEntropyLoss |  0.47789061\n",
            "Step  18500: eval  CrossEntropyLoss |  0.51599157\n",
            "Step  18500: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  18600: Ran 100 train steps in 15.90 secs\n",
            "Step  18600: train CrossEntropyLoss |  0.48198080\n",
            "Step  18600: eval  CrossEntropyLoss |  0.39226905\n",
            "Step  18600: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  18700: Ran 100 train steps in 16.84 secs\n",
            "Step  18700: train CrossEntropyLoss |  0.45770279\n",
            "Step  18700: eval  CrossEntropyLoss |  0.42024940\n",
            "Step  18700: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  18800: Ran 100 train steps in 15.84 secs\n",
            "Step  18800: train CrossEntropyLoss |  0.47864848\n",
            "Step  18800: eval  CrossEntropyLoss |  0.33957690\n",
            "Step  18800: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  18900: Ran 100 train steps in 15.87 secs\n",
            "Step  18900: train CrossEntropyLoss |  0.46477211\n",
            "Step  18900: eval  CrossEntropyLoss |  0.64825106\n",
            "Step  18900: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  19000: Ran 100 train steps in 15.81 secs\n",
            "Step  19000: train CrossEntropyLoss |  0.47682503\n",
            "Step  19000: eval  CrossEntropyLoss |  0.49245098\n",
            "Step  19000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  19100: Ran 100 train steps in 15.86 secs\n",
            "Step  19100: train CrossEntropyLoss |  0.48277113\n",
            "Step  19100: eval  CrossEntropyLoss |  0.47592178\n",
            "Step  19100: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  19200: Ran 100 train steps in 15.96 secs\n",
            "Step  19200: train CrossEntropyLoss |  0.47733551\n",
            "Step  19200: eval  CrossEntropyLoss |  0.66590434\n",
            "Step  19200: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  19300: Ran 100 train steps in 15.90 secs\n",
            "Step  19300: train CrossEntropyLoss |  0.46884230\n",
            "Step  19300: eval  CrossEntropyLoss |  0.34031782\n",
            "Step  19300: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  19400: Ran 100 train steps in 15.84 secs\n",
            "Step  19400: train CrossEntropyLoss |  0.50371683\n",
            "Step  19400: eval  CrossEntropyLoss |  0.45248032\n",
            "Step  19400: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  19500: Ran 100 train steps in 15.97 secs\n",
            "Step  19500: train CrossEntropyLoss |  0.48992270\n",
            "Step  19500: eval  CrossEntropyLoss |  0.52115977\n",
            "Step  19500: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  19600: Ran 100 train steps in 15.86 secs\n",
            "Step  19600: train CrossEntropyLoss |  0.48387828\n",
            "Step  19600: eval  CrossEntropyLoss |  0.41885430\n",
            "Step  19600: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  19700: Ran 100 train steps in 16.05 secs\n",
            "Step  19700: train CrossEntropyLoss |  0.48150668\n",
            "Step  19700: eval  CrossEntropyLoss |  0.34331414\n",
            "Step  19700: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  19800: Ran 100 train steps in 15.93 secs\n",
            "Step  19800: train CrossEntropyLoss |  0.48636141\n",
            "Step  19800: eval  CrossEntropyLoss |  0.41089466\n",
            "Step  19800: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  19900: Ran 100 train steps in 15.87 secs\n",
            "Step  19900: train CrossEntropyLoss |  0.48613563\n",
            "Step  19900: eval  CrossEntropyLoss |  0.58878505\n",
            "Step  19900: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  20000: Ran 100 train steps in 15.93 secs\n",
            "Step  20000: train CrossEntropyLoss |  0.48489100\n",
            "Step  20000: eval  CrossEntropyLoss |  0.44683999\n",
            "Step  20000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  20100: Ran 100 train steps in 16.77 secs\n",
            "Step  20100: train CrossEntropyLoss |  0.48023447\n",
            "Step  20100: eval  CrossEntropyLoss |  0.33880770\n",
            "Step  20100: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  20200: Ran 100 train steps in 16.94 secs\n",
            "Step  20200: train CrossEntropyLoss |  0.49459499\n",
            "Step  20200: eval  CrossEntropyLoss |  0.34356183\n",
            "Step  20200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  20300: Ran 100 train steps in 15.92 secs\n",
            "Step  20300: train CrossEntropyLoss |  0.47898397\n",
            "Step  20300: eval  CrossEntropyLoss |  0.43455446\n",
            "Step  20300: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  20400: Ran 100 train steps in 15.97 secs\n",
            "Step  20400: train CrossEntropyLoss |  0.48569673\n",
            "Step  20400: eval  CrossEntropyLoss |  0.46937823\n",
            "Step  20400: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  20500: Ran 100 train steps in 15.95 secs\n",
            "Step  20500: train CrossEntropyLoss |  0.49655092\n",
            "Step  20500: eval  CrossEntropyLoss |  0.54761916\n",
            "Step  20500: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  20600: Ran 100 train steps in 16.11 secs\n",
            "Step  20600: train CrossEntropyLoss |  0.48840484\n",
            "Step  20600: eval  CrossEntropyLoss |  0.57420760\n",
            "Step  20600: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  20700: Ran 100 train steps in 16.23 secs\n",
            "Step  20700: train CrossEntropyLoss |  0.48303106\n",
            "Step  20700: eval  CrossEntropyLoss |  0.31359941\n",
            "Step  20700: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  20800: Ran 100 train steps in 16.01 secs\n",
            "Step  20800: train CrossEntropyLoss |  0.50293446\n",
            "Step  20800: eval  CrossEntropyLoss |  0.51559687\n",
            "Step  20800: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  20900: Ran 100 train steps in 16.79 secs\n",
            "Step  20900: train CrossEntropyLoss |  0.48137701\n",
            "Step  20900: eval  CrossEntropyLoss |  0.53335965\n",
            "Step  20900: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  21000: Ran 100 train steps in 16.03 secs\n",
            "Step  21000: train CrossEntropyLoss |  0.51039720\n",
            "Step  21000: eval  CrossEntropyLoss |  0.36349034\n",
            "Step  21000: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  21100: Ran 100 train steps in 16.14 secs\n",
            "Step  21100: train CrossEntropyLoss |  0.48309216\n",
            "Step  21100: eval  CrossEntropyLoss |  0.65066898\n",
            "Step  21100: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  21200: Ran 100 train steps in 16.13 secs\n",
            "Step  21200: train CrossEntropyLoss |  0.48356551\n",
            "Step  21200: eval  CrossEntropyLoss |  0.59048575\n",
            "Step  21200: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  21300: Ran 100 train steps in 16.02 secs\n",
            "Step  21300: train CrossEntropyLoss |  0.48522291\n",
            "Step  21300: eval  CrossEntropyLoss |  0.64196497\n",
            "Step  21300: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step  21400: Ran 100 train steps in 16.09 secs\n",
            "Step  21400: train CrossEntropyLoss |  0.46810523\n",
            "Step  21400: eval  CrossEntropyLoss |  0.41638285\n",
            "Step  21400: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  21500: Ran 100 train steps in 16.07 secs\n",
            "Step  21500: train CrossEntropyLoss |  0.48116192\n",
            "Step  21500: eval  CrossEntropyLoss |  0.51288605\n",
            "Step  21500: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  21600: Ran 100 train steps in 16.19 secs\n",
            "Step  21600: train CrossEntropyLoss |  0.49048156\n",
            "Step  21600: eval  CrossEntropyLoss |  0.48587167\n",
            "Step  21600: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  21700: Ran 100 train steps in 16.11 secs\n",
            "Step  21700: train CrossEntropyLoss |  0.47334105\n",
            "Step  21700: eval  CrossEntropyLoss |  0.68535209\n",
            "Step  21700: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step  21800: Ran 100 train steps in 16.06 secs\n",
            "Step  21800: train CrossEntropyLoss |  0.49063647\n",
            "Step  21800: eval  CrossEntropyLoss |  0.42580992\n",
            "Step  21800: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  21900: Ran 100 train steps in 16.17 secs\n",
            "Step  21900: train CrossEntropyLoss |  0.48132190\n",
            "Step  21900: eval  CrossEntropyLoss |  0.81882787\n",
            "Step  21900: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step  22000: Ran 100 train steps in 16.06 secs\n",
            "Step  22000: train CrossEntropyLoss |  0.50066853\n",
            "Step  22000: eval  CrossEntropyLoss |  0.53374869\n",
            "Step  22000: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  22100: Ran 100 train steps in 16.15 secs\n",
            "Step  22100: train CrossEntropyLoss |  0.48630691\n",
            "Step  22100: eval  CrossEntropyLoss |  0.51693863\n",
            "Step  22100: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  22200: Ran 100 train steps in 16.18 secs\n",
            "Step  22200: train CrossEntropyLoss |  0.49437010\n",
            "Step  22200: eval  CrossEntropyLoss |  0.68549269\n",
            "Step  22200: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  22300: Ran 100 train steps in 16.09 secs\n",
            "Step  22300: train CrossEntropyLoss |  0.47767866\n",
            "Step  22300: eval  CrossEntropyLoss |  0.35297701\n",
            "Step  22300: eval          Accuracy |  1.00000000\n",
            "\n",
            "Step  22400: Ran 100 train steps in 16.08 secs\n",
            "Step  22400: train CrossEntropyLoss |  0.48914817\n",
            "Step  22400: eval  CrossEntropyLoss |  0.68941545\n",
            "Step  22400: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  22500: Ran 100 train steps in 16.27 secs\n",
            "Step  22500: train CrossEntropyLoss |  0.49135971\n",
            "Step  22500: eval  CrossEntropyLoss |  0.55247670\n",
            "Step  22500: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  22600: Ran 100 train steps in 16.33 secs\n",
            "Step  22600: train CrossEntropyLoss |  0.46730599\n",
            "Step  22600: eval  CrossEntropyLoss |  0.34374529\n",
            "Step  22600: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  22700: Ran 100 train steps in 16.25 secs\n",
            "Step  22700: train CrossEntropyLoss |  0.50203019\n",
            "Step  22700: eval  CrossEntropyLoss |  0.38827494\n",
            "Step  22700: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  22800: Ran 100 train steps in 16.08 secs\n",
            "Step  22800: train CrossEntropyLoss |  0.46946567\n",
            "Step  22800: eval  CrossEntropyLoss |  0.41583997\n",
            "Step  22800: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  22900: Ran 100 train steps in 16.23 secs\n",
            "Step  22900: train CrossEntropyLoss |  0.50702977\n",
            "Step  22900: eval  CrossEntropyLoss |  0.69497359\n",
            "Step  22900: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  23000: Ran 100 train steps in 16.11 secs\n",
            "Step  23000: train CrossEntropyLoss |  0.48695695\n",
            "Step  23000: eval  CrossEntropyLoss |  0.51545006\n",
            "Step  23000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  23100: Ran 100 train steps in 16.24 secs\n",
            "Step  23100: train CrossEntropyLoss |  0.48084021\n",
            "Step  23100: eval  CrossEntropyLoss |  0.52388972\n",
            "Step  23100: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  23200: Ran 100 train steps in 16.25 secs\n",
            "Step  23200: train CrossEntropyLoss |  0.45978498\n",
            "Step  23200: eval  CrossEntropyLoss |  0.53847337\n",
            "Step  23200: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  23300: Ran 100 train steps in 16.20 secs\n",
            "Step  23300: train CrossEntropyLoss |  0.46782005\n",
            "Step  23300: eval  CrossEntropyLoss |  0.53083515\n",
            "Step  23300: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  23400: Ran 100 train steps in 16.19 secs\n",
            "Step  23400: train CrossEntropyLoss |  0.47697434\n",
            "Step  23400: eval  CrossEntropyLoss |  0.30330625\n",
            "Step  23400: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  23500: Ran 100 train steps in 16.23 secs\n",
            "Step  23500: train CrossEntropyLoss |  0.48264462\n",
            "Step  23500: eval  CrossEntropyLoss |  0.63696712\n",
            "Step  23500: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  23600: Ran 100 train steps in 16.29 secs\n",
            "Step  23600: train CrossEntropyLoss |  0.50032151\n",
            "Step  23600: eval  CrossEntropyLoss |  0.53424650\n",
            "Step  23600: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  23700: Ran 100 train steps in 16.23 secs\n",
            "Step  23700: train CrossEntropyLoss |  0.47266281\n",
            "Step  23700: eval  CrossEntropyLoss |  0.68357706\n",
            "Step  23700: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  23800: Ran 100 train steps in 16.22 secs\n",
            "Step  23800: train CrossEntropyLoss |  0.47122404\n",
            "Step  23800: eval  CrossEntropyLoss |  0.33377758\n",
            "Step  23800: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  23900: Ran 100 train steps in 16.16 secs\n",
            "Step  23900: train CrossEntropyLoss |  0.47747621\n",
            "Step  23900: eval  CrossEntropyLoss |  0.39081264\n",
            "Step  23900: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  24000: Ran 100 train steps in 16.23 secs\n",
            "Step  24000: train CrossEntropyLoss |  0.48255381\n",
            "Step  24000: eval  CrossEntropyLoss |  0.48508874\n",
            "Step  24000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  24100: Ran 100 train steps in 16.31 secs\n",
            "Step  24100: train CrossEntropyLoss |  0.45952937\n",
            "Step  24100: eval  CrossEntropyLoss |  0.59279186\n",
            "Step  24100: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  24200: Ran 100 train steps in 16.33 secs\n",
            "Step  24200: train CrossEntropyLoss |  0.49844310\n",
            "Step  24200: eval  CrossEntropyLoss |  0.53605956\n",
            "Step  24200: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  24300: Ran 100 train steps in 16.21 secs\n",
            "Step  24300: train CrossEntropyLoss |  0.45656726\n",
            "Step  24300: eval  CrossEntropyLoss |  0.48860011\n",
            "Step  24300: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  24400: Ran 100 train steps in 16.39 secs\n",
            "Step  24400: train CrossEntropyLoss |  0.50181758\n",
            "Step  24400: eval  CrossEntropyLoss |  0.28244206\n",
            "Step  24400: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  24500: Ran 100 train steps in 16.49 secs\n",
            "Step  24500: train CrossEntropyLoss |  0.46929204\n",
            "Step  24500: eval  CrossEntropyLoss |  0.57277435\n",
            "Step  24500: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  24600: Ran 100 train steps in 16.57 secs\n",
            "Step  24600: train CrossEntropyLoss |  0.48697439\n",
            "Step  24600: eval  CrossEntropyLoss |  0.44074002\n",
            "Step  24600: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  24700: Ran 100 train steps in 16.34 secs\n",
            "Step  24700: train CrossEntropyLoss |  0.47018087\n",
            "Step  24700: eval  CrossEntropyLoss |  0.71244746\n",
            "Step  24700: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  24800: Ran 100 train steps in 16.31 secs\n",
            "Step  24800: train CrossEntropyLoss |  0.50449699\n",
            "Step  24800: eval  CrossEntropyLoss |  0.40365189\n",
            "Step  24800: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  24900: Ran 100 train steps in 16.37 secs\n",
            "Step  24900: train CrossEntropyLoss |  0.45921138\n",
            "Step  24900: eval  CrossEntropyLoss |  0.40999448\n",
            "Step  24900: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  25000: Ran 100 train steps in 16.35 secs\n",
            "Step  25000: train CrossEntropyLoss |  0.48457834\n",
            "Step  25000: eval  CrossEntropyLoss |  0.39535767\n",
            "Step  25000: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  25100: Ran 100 train steps in 16.46 secs\n",
            "Step  25100: train CrossEntropyLoss |  0.45143762\n",
            "Step  25100: eval  CrossEntropyLoss |  0.55730808\n",
            "Step  25100: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  25200: Ran 100 train steps in 17.24 secs\n",
            "Step  25200: train CrossEntropyLoss |  0.47919253\n",
            "Step  25200: eval  CrossEntropyLoss |  0.57527393\n",
            "Step  25200: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  25300: Ran 100 train steps in 16.41 secs\n",
            "Step  25300: train CrossEntropyLoss |  0.50007248\n",
            "Step  25300: eval  CrossEntropyLoss |  0.37112638\n",
            "Step  25300: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  25400: Ran 100 train steps in 16.41 secs\n",
            "Step  25400: train CrossEntropyLoss |  0.48316401\n",
            "Step  25400: eval  CrossEntropyLoss |  0.54307938\n",
            "Step  25400: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  25500: Ran 100 train steps in 16.47 secs\n",
            "Step  25500: train CrossEntropyLoss |  0.47972313\n",
            "Step  25500: eval  CrossEntropyLoss |  0.34740120\n",
            "Step  25500: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  25600: Ran 100 train steps in 16.53 secs\n",
            "Step  25600: train CrossEntropyLoss |  0.45393267\n",
            "Step  25600: eval  CrossEntropyLoss |  0.50684863\n",
            "Step  25600: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  25700: Ran 100 train steps in 16.43 secs\n",
            "Step  25700: train CrossEntropyLoss |  0.47226202\n",
            "Step  25700: eval  CrossEntropyLoss |  0.44710892\n",
            "Step  25700: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  25800: Ran 100 train steps in 17.23 secs\n",
            "Step  25800: train CrossEntropyLoss |  0.48307461\n",
            "Step  25800: eval  CrossEntropyLoss |  0.45192608\n",
            "Step  25800: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  25900: Ran 100 train steps in 16.43 secs\n",
            "Step  25900: train CrossEntropyLoss |  0.46091574\n",
            "Step  25900: eval  CrossEntropyLoss |  0.48779765\n",
            "Step  25900: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  26000: Ran 100 train steps in 16.45 secs\n",
            "Step  26000: train CrossEntropyLoss |  0.46073124\n",
            "Step  26000: eval  CrossEntropyLoss |  0.42106330\n",
            "Step  26000: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  26100: Ran 100 train steps in 16.59 secs\n",
            "Step  26100: train CrossEntropyLoss |  0.46000543\n",
            "Step  26100: eval  CrossEntropyLoss |  0.29562047\n",
            "Step  26100: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  26200: Ran 100 train steps in 16.46 secs\n",
            "Step  26200: train CrossEntropyLoss |  0.50511205\n",
            "Step  26200: eval  CrossEntropyLoss |  0.42097169\n",
            "Step  26200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  26300: Ran 100 train steps in 16.73 secs\n",
            "Step  26300: train CrossEntropyLoss |  0.46115169\n",
            "Step  26300: eval  CrossEntropyLoss |  0.69085062\n",
            "Step  26300: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  26400: Ran 100 train steps in 16.72 secs\n",
            "Step  26400: train CrossEntropyLoss |  0.46196660\n",
            "Step  26400: eval  CrossEntropyLoss |  0.38138753\n",
            "Step  26400: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  26500: Ran 100 train steps in 16.52 secs\n",
            "Step  26500: train CrossEntropyLoss |  0.46709570\n",
            "Step  26500: eval  CrossEntropyLoss |  0.28927559\n",
            "Step  26500: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  26600: Ran 100 train steps in 16.57 secs\n",
            "Step  26600: train CrossEntropyLoss |  0.47406366\n",
            "Step  26600: eval  CrossEntropyLoss |  0.61527085\n",
            "Step  26600: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  26700: Ran 100 train steps in 16.51 secs\n",
            "Step  26700: train CrossEntropyLoss |  0.45170367\n",
            "Step  26700: eval  CrossEntropyLoss |  0.46669859\n",
            "Step  26700: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  26800: Ran 100 train steps in 16.49 secs\n",
            "Step  26800: train CrossEntropyLoss |  0.45030418\n",
            "Step  26800: eval  CrossEntropyLoss |  0.54063171\n",
            "Step  26800: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  26900: Ran 100 train steps in 16.50 secs\n",
            "Step  26900: train CrossEntropyLoss |  0.48303372\n",
            "Step  26900: eval  CrossEntropyLoss |  0.61070061\n",
            "Step  26900: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  27000: Ran 100 train steps in 16.57 secs\n",
            "Step  27000: train CrossEntropyLoss |  0.45907640\n",
            "Step  27000: eval  CrossEntropyLoss |  0.30568492\n",
            "Step  27000: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  27100: Ran 100 train steps in 16.61 secs\n",
            "Step  27100: train CrossEntropyLoss |  0.48712367\n",
            "Step  27100: eval  CrossEntropyLoss |  0.40867552\n",
            "Step  27100: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  27200: Ran 100 train steps in 16.51 secs\n",
            "Step  27200: train CrossEntropyLoss |  0.47377029\n",
            "Step  27200: eval  CrossEntropyLoss |  0.45533484\n",
            "Step  27200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  27300: Ran 100 train steps in 16.50 secs\n",
            "Step  27300: train CrossEntropyLoss |  0.49201033\n",
            "Step  27300: eval  CrossEntropyLoss |  0.33632690\n",
            "Step  27300: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  27400: Ran 100 train steps in 16.51 secs\n",
            "Step  27400: train CrossEntropyLoss |  0.47507253\n",
            "Step  27400: eval  CrossEntropyLoss |  0.43911567\n",
            "Step  27400: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  27500: Ran 100 train steps in 16.69 secs\n",
            "Step  27500: train CrossEntropyLoss |  0.46530479\n",
            "Step  27500: eval  CrossEntropyLoss |  0.43736342\n",
            "Step  27500: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  27600: Ran 100 train steps in 16.59 secs\n",
            "Step  27600: train CrossEntropyLoss |  0.50184500\n",
            "Step  27600: eval  CrossEntropyLoss |  0.27251452\n",
            "Step  27600: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  27700: Ran 100 train steps in 16.58 secs\n",
            "Step  27700: train CrossEntropyLoss |  0.45073816\n",
            "Step  27700: eval  CrossEntropyLoss |  0.33220029\n",
            "Step  27700: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  27800: Ran 100 train steps in 16.51 secs\n",
            "Step  27800: train CrossEntropyLoss |  0.46154299\n",
            "Step  27800: eval  CrossEntropyLoss |  0.45015365\n",
            "Step  27800: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  27900: Ran 100 train steps in 16.62 secs\n",
            "Step  27900: train CrossEntropyLoss |  0.49962386\n",
            "Step  27900: eval  CrossEntropyLoss |  0.38919181\n",
            "Step  27900: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  28000: Ran 100 train steps in 16.74 secs\n",
            "Step  28000: train CrossEntropyLoss |  0.47635370\n",
            "Step  28000: eval  CrossEntropyLoss |  0.41935861\n",
            "Step  28000: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  28100: Ran 100 train steps in 16.61 secs\n",
            "Step  28100: train CrossEntropyLoss |  0.49306655\n",
            "Step  28100: eval  CrossEntropyLoss |  0.31655380\n",
            "Step  28100: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  28200: Ran 100 train steps in 16.84 secs\n",
            "Step  28200: train CrossEntropyLoss |  0.47373888\n",
            "Step  28200: eval  CrossEntropyLoss |  0.49357778\n",
            "Step  28200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  28300: Ran 100 train steps in 17.50 secs\n",
            "Step  28300: train CrossEntropyLoss |  0.46364707\n",
            "Step  28300: eval  CrossEntropyLoss |  0.45659354\n",
            "Step  28300: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  28400: Ran 100 train steps in 16.68 secs\n",
            "Step  28400: train CrossEntropyLoss |  0.47976229\n",
            "Step  28400: eval  CrossEntropyLoss |  0.33657756\n",
            "Step  28400: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  28500: Ran 100 train steps in 16.76 secs\n",
            "Step  28500: train CrossEntropyLoss |  0.47426277\n",
            "Step  28500: eval  CrossEntropyLoss |  0.65759033\n",
            "Step  28500: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  28600: Ran 100 train steps in 16.73 secs\n",
            "Step  28600: train CrossEntropyLoss |  0.48038062\n",
            "Step  28600: eval  CrossEntropyLoss |  0.36887583\n",
            "Step  28600: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  28700: Ran 100 train steps in 17.47 secs\n",
            "Step  28700: train CrossEntropyLoss |  0.49486533\n",
            "Step  28700: eval  CrossEntropyLoss |  0.52885294\n",
            "Step  28700: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  28800: Ran 100 train steps in 17.50 secs\n",
            "Step  28800: train CrossEntropyLoss |  0.48012528\n",
            "Step  28800: eval  CrossEntropyLoss |  0.37089884\n",
            "Step  28800: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  28900: Ran 100 train steps in 16.71 secs\n",
            "Step  28900: train CrossEntropyLoss |  0.46848080\n",
            "Step  28900: eval  CrossEntropyLoss |  0.57309598\n",
            "Step  28900: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  29000: Ran 100 train steps in 16.84 secs\n",
            "Step  29000: train CrossEntropyLoss |  0.46202061\n",
            "Step  29000: eval  CrossEntropyLoss |  0.50611055\n",
            "Step  29000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  29100: Ran 100 train steps in 16.71 secs\n",
            "Step  29100: train CrossEntropyLoss |  0.48484188\n",
            "Step  29100: eval  CrossEntropyLoss |  0.73635674\n",
            "Step  29100: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  29200: Ran 100 train steps in 16.74 secs\n",
            "Step  29200: train CrossEntropyLoss |  0.50316423\n",
            "Step  29200: eval  CrossEntropyLoss |  0.55448449\n",
            "Step  29200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  29300: Ran 100 train steps in 16.69 secs\n",
            "Step  29300: train CrossEntropyLoss |  0.48287979\n",
            "Step  29300: eval  CrossEntropyLoss |  0.44769502\n",
            "Step  29300: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  29400: Ran 100 train steps in 16.72 secs\n",
            "Step  29400: train CrossEntropyLoss |  0.49409613\n",
            "Step  29400: eval  CrossEntropyLoss |  1.08411396\n",
            "Step  29400: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  29500: Ran 100 train steps in 16.87 secs\n",
            "Step  29500: train CrossEntropyLoss |  0.49492595\n",
            "Step  29500: eval  CrossEntropyLoss |  0.66526747\n",
            "Step  29500: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  29600: Ran 100 train steps in 16.74 secs\n",
            "Step  29600: train CrossEntropyLoss |  0.48277000\n",
            "Step  29600: eval  CrossEntropyLoss |  0.78674495\n",
            "Step  29600: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step  29700: Ran 100 train steps in 16.74 secs\n",
            "Step  29700: train CrossEntropyLoss |  0.49861294\n",
            "Step  29700: eval  CrossEntropyLoss |  0.43774655\n",
            "Step  29700: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  29800: Ran 100 train steps in 16.76 secs\n",
            "Step  29800: train CrossEntropyLoss |  0.47458899\n",
            "Step  29800: eval  CrossEntropyLoss |  0.36966303\n",
            "Step  29800: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  29900: Ran 100 train steps in 16.83 secs\n",
            "Step  29900: train CrossEntropyLoss |  0.45183671\n",
            "Step  29900: eval  CrossEntropyLoss |  0.41383815\n",
            "Step  29900: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  30000: Ran 100 train steps in 16.95 secs\n",
            "Step  30000: train CrossEntropyLoss |  0.46329048\n",
            "Step  30000: eval  CrossEntropyLoss |  0.59150976\n",
            "Step  30000: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  30100: Ran 100 train steps in 16.94 secs\n",
            "Step  30100: train CrossEntropyLoss |  0.46002138\n",
            "Step  30100: eval  CrossEntropyLoss |  0.49512595\n",
            "Step  30100: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  30200: Ran 100 train steps in 16.76 secs\n",
            "Step  30200: train CrossEntropyLoss |  0.45814994\n",
            "Step  30200: eval  CrossEntropyLoss |  0.44041896\n",
            "Step  30200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  30300: Ran 100 train steps in 16.79 secs\n",
            "Step  30300: train CrossEntropyLoss |  0.45950666\n",
            "Step  30300: eval  CrossEntropyLoss |  0.58686882\n",
            "Step  30300: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  30400: Ran 100 train steps in 16.92 secs\n",
            "Step  30400: train CrossEntropyLoss |  0.47784102\n",
            "Step  30400: eval  CrossEntropyLoss |  0.86726880\n",
            "Step  30400: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step  30500: Ran 100 train steps in 16.84 secs\n",
            "Step  30500: train CrossEntropyLoss |  0.48026970\n",
            "Step  30500: eval  CrossEntropyLoss |  0.78912795\n",
            "Step  30500: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  30600: Ran 100 train steps in 16.81 secs\n",
            "Step  30600: train CrossEntropyLoss |  0.48670009\n",
            "Step  30600: eval  CrossEntropyLoss |  0.45894241\n",
            "Step  30600: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  30700: Ran 100 train steps in 16.77 secs\n",
            "Step  30700: train CrossEntropyLoss |  0.45798457\n",
            "Step  30700: eval  CrossEntropyLoss |  0.77413785\n",
            "Step  30700: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step  30800: Ran 100 train steps in 16.86 secs\n",
            "Step  30800: train CrossEntropyLoss |  0.46571296\n",
            "Step  30800: eval  CrossEntropyLoss |  0.43872464\n",
            "Step  30800: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  30900: Ran 100 train steps in 16.96 secs\n",
            "Step  30900: train CrossEntropyLoss |  0.48266581\n",
            "Step  30900: eval  CrossEntropyLoss |  0.46366000\n",
            "Step  30900: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  31000: Ran 100 train steps in 16.91 secs\n",
            "Step  31000: train CrossEntropyLoss |  0.47883907\n",
            "Step  31000: eval  CrossEntropyLoss |  0.82262987\n",
            "Step  31000: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  31100: Ran 100 train steps in 16.89 secs\n",
            "Step  31100: train CrossEntropyLoss |  0.47259432\n",
            "Step  31100: eval  CrossEntropyLoss |  0.45249641\n",
            "Step  31100: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  31200: Ran 100 train steps in 16.83 secs\n",
            "Step  31200: train CrossEntropyLoss |  0.46575373\n",
            "Step  31200: eval  CrossEntropyLoss |  0.59474576\n",
            "Step  31200: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  31300: Ran 100 train steps in 16.89 secs\n",
            "Step  31300: train CrossEntropyLoss |  0.48441988\n",
            "Step  31300: eval  CrossEntropyLoss |  0.61419517\n",
            "Step  31300: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  31400: Ran 100 train steps in 16.95 secs\n",
            "Step  31400: train CrossEntropyLoss |  0.46909282\n",
            "Step  31400: eval  CrossEntropyLoss |  0.57805997\n",
            "Step  31400: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  31500: Ran 100 train steps in 16.91 secs\n",
            "Step  31500: train CrossEntropyLoss |  0.46627325\n",
            "Step  31500: eval  CrossEntropyLoss |  0.44288540\n",
            "Step  31500: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  31600: Ran 100 train steps in 16.91 secs\n",
            "Step  31600: train CrossEntropyLoss |  0.46984729\n",
            "Step  31600: eval  CrossEntropyLoss |  0.36600506\n",
            "Step  31600: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  31700: Ran 100 train steps in 16.96 secs\n",
            "Step  31700: train CrossEntropyLoss |  0.47070491\n",
            "Step  31700: eval  CrossEntropyLoss |  0.37268898\n",
            "Step  31700: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  31800: Ran 100 train steps in 17.09 secs\n",
            "Step  31800: train CrossEntropyLoss |  0.47978300\n",
            "Step  31800: eval  CrossEntropyLoss |  0.60163671\n",
            "Step  31800: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  31900: Ran 100 train steps in 17.14 secs\n",
            "Step  31900: train CrossEntropyLoss |  0.46802104\n",
            "Step  31900: eval  CrossEntropyLoss |  0.39223483\n",
            "Step  31900: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  32000: Ran 100 train steps in 16.85 secs\n",
            "Step  32000: train CrossEntropyLoss |  0.46622360\n",
            "Step  32000: eval  CrossEntropyLoss |  0.55700547\n",
            "Step  32000: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  32100: Ran 100 train steps in 16.95 secs\n",
            "Step  32100: train CrossEntropyLoss |  0.46151736\n",
            "Step  32100: eval  CrossEntropyLoss |  0.60553861\n",
            "Step  32100: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  32200: Ran 100 train steps in 16.91 secs\n",
            "Step  32200: train CrossEntropyLoss |  0.49657238\n",
            "Step  32200: eval  CrossEntropyLoss |  0.82644200\n",
            "Step  32200: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  32300: Ran 100 train steps in 17.03 secs\n",
            "Step  32300: train CrossEntropyLoss |  0.47466293\n",
            "Step  32300: eval  CrossEntropyLoss |  0.59284204\n",
            "Step  32300: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  32400: Ran 100 train steps in 17.00 secs\n",
            "Step  32400: train CrossEntropyLoss |  0.46357763\n",
            "Step  32400: eval  CrossEntropyLoss |  0.39472005\n",
            "Step  32400: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  32500: Ran 100 train steps in 16.95 secs\n",
            "Step  32500: train CrossEntropyLoss |  0.46736598\n",
            "Step  32500: eval  CrossEntropyLoss |  0.47691548\n",
            "Step  32500: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  32600: Ran 100 train steps in 17.02 secs\n",
            "Step  32600: train CrossEntropyLoss |  0.47540426\n",
            "Step  32600: eval  CrossEntropyLoss |  0.61773109\n",
            "Step  32600: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  32700: Ran 100 train steps in 17.02 secs\n",
            "Step  32700: train CrossEntropyLoss |  0.49362209\n",
            "Step  32700: eval  CrossEntropyLoss |  0.53889966\n",
            "Step  32700: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  32800: Ran 100 train steps in 17.99 secs\n",
            "Step  32800: train CrossEntropyLoss |  0.49374756\n",
            "Step  32800: eval  CrossEntropyLoss |  0.50439250\n",
            "Step  32800: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  32900: Ran 100 train steps in 17.27 secs\n",
            "Step  32900: train CrossEntropyLoss |  0.47612619\n",
            "Step  32900: eval  CrossEntropyLoss |  0.50788003\n",
            "Step  32900: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  33000: Ran 100 train steps in 18.38 secs\n",
            "Step  33000: train CrossEntropyLoss |  0.49289787\n",
            "Step  33000: eval  CrossEntropyLoss |  0.26952505\n",
            "Step  33000: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  33100: Ran 100 train steps in 17.83 secs\n",
            "Step  33100: train CrossEntropyLoss |  0.46168813\n",
            "Step  33100: eval  CrossEntropyLoss |  0.81179631\n",
            "Step  33100: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  33200: Ran 100 train steps in 17.53 secs\n",
            "Step  33200: train CrossEntropyLoss |  0.46523851\n",
            "Step  33200: eval  CrossEntropyLoss |  0.57295901\n",
            "Step  33200: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  33300: Ran 100 train steps in 17.19 secs\n",
            "Step  33300: train CrossEntropyLoss |  0.49679717\n",
            "Step  33300: eval  CrossEntropyLoss |  0.49500778\n",
            "Step  33300: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  33400: Ran 100 train steps in 17.13 secs\n",
            "Step  33400: train CrossEntropyLoss |  0.47801745\n",
            "Step  33400: eval  CrossEntropyLoss |  0.53462952\n",
            "Step  33400: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  33500: Ran 100 train steps in 17.16 secs\n",
            "Step  33500: train CrossEntropyLoss |  0.48925951\n",
            "Step  33500: eval  CrossEntropyLoss |  0.37594759\n",
            "Step  33500: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  33600: Ran 100 train steps in 17.39 secs\n",
            "Step  33600: train CrossEntropyLoss |  0.47123247\n",
            "Step  33600: eval  CrossEntropyLoss |  0.30211392\n",
            "Step  33600: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  33700: Ran 100 train steps in 17.45 secs\n",
            "Step  33700: train CrossEntropyLoss |  0.47154453\n",
            "Step  33700: eval  CrossEntropyLoss |  0.48054948\n",
            "Step  33700: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  33800: Ran 100 train steps in 17.17 secs\n",
            "Step  33800: train CrossEntropyLoss |  0.49525437\n",
            "Step  33800: eval  CrossEntropyLoss |  0.37145460\n",
            "Step  33800: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  33900: Ran 100 train steps in 17.18 secs\n",
            "Step  33900: train CrossEntropyLoss |  0.49027801\n",
            "Step  33900: eval  CrossEntropyLoss |  0.46504581\n",
            "Step  33900: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  34000: Ran 100 train steps in 17.11 secs\n",
            "Step  34000: train CrossEntropyLoss |  0.48180994\n",
            "Step  34000: eval  CrossEntropyLoss |  0.54526567\n",
            "Step  34000: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  34100: Ran 100 train steps in 17.16 secs\n",
            "Step  34100: train CrossEntropyLoss |  0.49668330\n",
            "Step  34100: eval  CrossEntropyLoss |  0.60555595\n",
            "Step  34100: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  34200: Ran 100 train steps in 17.19 secs\n",
            "Step  34200: train CrossEntropyLoss |  0.47567844\n",
            "Step  34200: eval  CrossEntropyLoss |  0.34450710\n",
            "Step  34200: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  34300: Ran 100 train steps in 17.13 secs\n",
            "Step  34300: train CrossEntropyLoss |  0.49644315\n",
            "Step  34300: eval  CrossEntropyLoss |  0.51070571\n",
            "Step  34300: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  34400: Ran 100 train steps in 17.12 secs\n",
            "Step  34400: train CrossEntropyLoss |  0.49591953\n",
            "Step  34400: eval  CrossEntropyLoss |  0.61352253\n",
            "Step  34400: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  34500: Ran 100 train steps in 17.20 secs\n",
            "Step  34500: train CrossEntropyLoss |  0.49394098\n",
            "Step  34500: eval  CrossEntropyLoss |  0.37631688\n",
            "Step  34500: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  34600: Ran 100 train steps in 17.30 secs\n",
            "Step  34600: train CrossEntropyLoss |  0.49144158\n",
            "Step  34600: eval  CrossEntropyLoss |  0.35988814\n",
            "Step  34600: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  34700: Ran 100 train steps in 17.24 secs\n",
            "Step  34700: train CrossEntropyLoss |  0.48551795\n",
            "Step  34700: eval  CrossEntropyLoss |  0.27629808\n",
            "Step  34700: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  34800: Ran 100 train steps in 17.20 secs\n",
            "Step  34800: train CrossEntropyLoss |  0.47208217\n",
            "Step  34800: eval  CrossEntropyLoss |  0.56154948\n",
            "Step  34800: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  34900: Ran 100 train steps in 17.18 secs\n",
            "Step  34900: train CrossEntropyLoss |  0.45716491\n",
            "Step  34900: eval  CrossEntropyLoss |  0.55062079\n",
            "Step  34900: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  35000: Ran 100 train steps in 17.20 secs\n",
            "Step  35000: train CrossEntropyLoss |  0.47177359\n",
            "Step  35000: eval  CrossEntropyLoss |  0.71148479\n",
            "Step  35000: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  35100: Ran 100 train steps in 17.31 secs\n",
            "Step  35100: train CrossEntropyLoss |  0.47066280\n",
            "Step  35100: eval  CrossEntropyLoss |  0.40957204\n",
            "Step  35100: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  35200: Ran 100 train steps in 17.19 secs\n",
            "Step  35200: train CrossEntropyLoss |  0.46635517\n",
            "Step  35200: eval  CrossEntropyLoss |  0.23102044\n",
            "Step  35200: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  35300: Ran 100 train steps in 18.01 secs\n",
            "Step  35300: train CrossEntropyLoss |  0.47814867\n",
            "Step  35300: eval  CrossEntropyLoss |  0.41156065\n",
            "Step  35300: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  35400: Ran 100 train steps in 17.31 secs\n",
            "Step  35400: train CrossEntropyLoss |  0.50643629\n",
            "Step  35400: eval  CrossEntropyLoss |  0.35459358\n",
            "Step  35400: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  35500: Ran 100 train steps in 17.47 secs\n",
            "Step  35500: train CrossEntropyLoss |  0.46219772\n",
            "Step  35500: eval  CrossEntropyLoss |  0.74968678\n",
            "Step  35500: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  35600: Ran 100 train steps in 17.31 secs\n",
            "Step  35600: train CrossEntropyLoss |  0.49462679\n",
            "Step  35600: eval  CrossEntropyLoss |  0.51731038\n",
            "Step  35600: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  35700: Ran 100 train steps in 17.28 secs\n",
            "Step  35700: train CrossEntropyLoss |  0.45128086\n",
            "Step  35700: eval  CrossEntropyLoss |  0.67639905\n",
            "Step  35700: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step  35800: Ran 100 train steps in 17.20 secs\n",
            "Step  35800: train CrossEntropyLoss |  0.45489252\n",
            "Step  35800: eval  CrossEntropyLoss |  0.52800483\n",
            "Step  35800: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  35900: Ran 100 train steps in 17.21 secs\n",
            "Step  35900: train CrossEntropyLoss |  0.47146299\n",
            "Step  35900: eval  CrossEntropyLoss |  0.38541532\n",
            "Step  35900: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  36000: Ran 100 train steps in 17.31 secs\n",
            "Step  36000: train CrossEntropyLoss |  0.48075145\n",
            "Step  36000: eval  CrossEntropyLoss |  0.30037418\n",
            "Step  36000: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  36100: Ran 100 train steps in 17.29 secs\n",
            "Step  36100: train CrossEntropyLoss |  0.47021213\n",
            "Step  36100: eval  CrossEntropyLoss |  0.30671304\n",
            "Step  36100: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  36200: Ran 100 train steps in 17.20 secs\n",
            "Step  36200: train CrossEntropyLoss |  0.46816352\n",
            "Step  36200: eval  CrossEntropyLoss |  0.36232513\n",
            "Step  36200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  36300: Ran 100 train steps in 17.26 secs\n",
            "Step  36300: train CrossEntropyLoss |  0.47713295\n",
            "Step  36300: eval  CrossEntropyLoss |  0.38452572\n",
            "Step  36300: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  36400: Ran 100 train steps in 17.26 secs\n",
            "Step  36400: train CrossEntropyLoss |  0.47095093\n",
            "Step  36400: eval  CrossEntropyLoss |  0.40193185\n",
            "Step  36400: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  36500: Ran 100 train steps in 17.37 secs\n",
            "Step  36500: train CrossEntropyLoss |  0.48348209\n",
            "Step  36500: eval  CrossEntropyLoss |  0.49712420\n",
            "Step  36500: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  36600: Ran 100 train steps in 17.56 secs\n",
            "Step  36600: train CrossEntropyLoss |  0.44770396\n",
            "Step  36600: eval  CrossEntropyLoss |  0.52804732\n",
            "Step  36600: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  36700: Ran 100 train steps in 17.32 secs\n",
            "Step  36700: train CrossEntropyLoss |  0.47340700\n",
            "Step  36700: eval  CrossEntropyLoss |  0.66039562\n",
            "Step  36700: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  36800: Ran 100 train steps in 17.29 secs\n",
            "Step  36800: train CrossEntropyLoss |  0.49767753\n",
            "Step  36800: eval  CrossEntropyLoss |  0.60686070\n",
            "Step  36800: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  36900: Ran 100 train steps in 17.30 secs\n",
            "Step  36900: train CrossEntropyLoss |  0.48575720\n",
            "Step  36900: eval  CrossEntropyLoss |  0.34481600\n",
            "Step  36900: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  37000: Ran 100 train steps in 17.31 secs\n",
            "Step  37000: train CrossEntropyLoss |  0.45276225\n",
            "Step  37000: eval  CrossEntropyLoss |  0.47928318\n",
            "Step  37000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  37100: Ran 100 train steps in 17.32 secs\n",
            "Step  37100: train CrossEntropyLoss |  0.47885221\n",
            "Step  37100: eval  CrossEntropyLoss |  0.39863890\n",
            "Step  37100: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  37200: Ran 100 train steps in 17.45 secs\n",
            "Step  37200: train CrossEntropyLoss |  0.46369979\n",
            "Step  37200: eval  CrossEntropyLoss |  0.77455115\n",
            "Step  37200: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  37300: Ran 100 train steps in 17.48 secs\n",
            "Step  37300: train CrossEntropyLoss |  0.46831366\n",
            "Step  37300: eval  CrossEntropyLoss |  0.42606476\n",
            "Step  37300: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  37400: Ran 100 train steps in 17.46 secs\n",
            "Step  37400: train CrossEntropyLoss |  0.46627671\n",
            "Step  37400: eval  CrossEntropyLoss |  0.43515158\n",
            "Step  37400: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  37500: Ran 100 train steps in 17.33 secs\n",
            "Step  37500: train CrossEntropyLoss |  0.47829702\n",
            "Step  37500: eval  CrossEntropyLoss |  0.48913482\n",
            "Step  37500: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  37600: Ran 100 train steps in 17.33 secs\n",
            "Step  37600: train CrossEntropyLoss |  0.45952797\n",
            "Step  37600: eval  CrossEntropyLoss |  0.51133490\n",
            "Step  37600: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  37700: Ran 100 train steps in 17.29 secs\n",
            "Step  37700: train CrossEntropyLoss |  0.46312293\n",
            "Step  37700: eval  CrossEntropyLoss |  0.69456863\n",
            "Step  37700: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  37800: Ran 100 train steps in 17.30 secs\n",
            "Step  37800: train CrossEntropyLoss |  0.46625459\n",
            "Step  37800: eval  CrossEntropyLoss |  0.63804662\n",
            "Step  37800: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  37900: Ran 100 train steps in 17.41 secs\n",
            "Step  37900: train CrossEntropyLoss |  0.47666523\n",
            "Step  37900: eval  CrossEntropyLoss |  0.33285499\n",
            "Step  37900: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  38000: Ran 100 train steps in 17.27 secs\n",
            "Step  38000: train CrossEntropyLoss |  0.45985594\n",
            "Step  38000: eval  CrossEntropyLoss |  0.48630053\n",
            "Step  38000: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  38100: Ran 100 train steps in 17.33 secs\n",
            "Step  38100: train CrossEntropyLoss |  0.47341591\n",
            "Step  38100: eval  CrossEntropyLoss |  0.41673577\n",
            "Step  38100: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  38200: Ran 100 train steps in 17.28 secs\n",
            "Step  38200: train CrossEntropyLoss |  0.45644230\n",
            "Step  38200: eval  CrossEntropyLoss |  0.71508211\n",
            "Step  38200: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  38300: Ran 100 train steps in 17.41 secs\n",
            "Step  38300: train CrossEntropyLoss |  0.48285678\n",
            "Step  38300: eval  CrossEntropyLoss |  0.34888068\n",
            "Step  38300: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  38400: Ran 100 train steps in 17.42 secs\n",
            "Step  38400: train CrossEntropyLoss |  0.46681452\n",
            "Step  38400: eval  CrossEntropyLoss |  0.28799933\n",
            "Step  38400: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  38500: Ran 100 train steps in 17.40 secs\n",
            "Step  38500: train CrossEntropyLoss |  0.47549045\n",
            "Step  38500: eval  CrossEntropyLoss |  0.49535415\n",
            "Step  38500: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  38600: Ran 100 train steps in 17.33 secs\n",
            "Step  38600: train CrossEntropyLoss |  0.49035758\n",
            "Step  38600: eval  CrossEntropyLoss |  0.45052361\n",
            "Step  38600: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  38700: Ran 100 train steps in 17.39 secs\n",
            "Step  38700: train CrossEntropyLoss |  0.47863838\n",
            "Step  38700: eval  CrossEntropyLoss |  0.38654953\n",
            "Step  38700: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  38800: Ran 100 train steps in 17.51 secs\n",
            "Step  38800: train CrossEntropyLoss |  0.48254168\n",
            "Step  38800: eval  CrossEntropyLoss |  0.28741744\n",
            "Step  38800: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  38900: Ran 100 train steps in 17.45 secs\n",
            "Step  38900: train CrossEntropyLoss |  0.47966066\n",
            "Step  38900: eval  CrossEntropyLoss |  0.55572170\n",
            "Step  38900: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  39000: Ran 100 train steps in 17.57 secs\n",
            "Step  39000: train CrossEntropyLoss |  0.46116680\n",
            "Step  39000: eval  CrossEntropyLoss |  0.29484448\n",
            "Step  39000: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  39100: Ran 100 train steps in 17.48 secs\n",
            "Step  39100: train CrossEntropyLoss |  0.46648341\n",
            "Step  39100: eval  CrossEntropyLoss |  0.37411344\n",
            "Step  39100: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  39200: Ran 100 train steps in 17.42 secs\n",
            "Step  39200: train CrossEntropyLoss |  0.47301424\n",
            "Step  39200: eval  CrossEntropyLoss |  0.45079970\n",
            "Step  39200: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  39300: Ran 100 train steps in 17.44 secs\n",
            "Step  39300: train CrossEntropyLoss |  0.49363258\n",
            "Step  39300: eval  CrossEntropyLoss |  0.42193863\n",
            "Step  39300: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  39400: Ran 100 train steps in 17.39 secs\n",
            "Step  39400: train CrossEntropyLoss |  0.46267840\n",
            "Step  39400: eval  CrossEntropyLoss |  0.37443301\n",
            "Step  39400: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  39500: Ran 100 train steps in 17.44 secs\n",
            "Step  39500: train CrossEntropyLoss |  0.46873069\n",
            "Step  39500: eval  CrossEntropyLoss |  0.32858944\n",
            "Step  39500: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  39600: Ran 100 train steps in 17.43 secs\n",
            "Step  39600: train CrossEntropyLoss |  0.47743076\n",
            "Step  39600: eval  CrossEntropyLoss |  0.28715706\n",
            "Step  39600: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  39700: Ran 100 train steps in 17.50 secs\n",
            "Step  39700: train CrossEntropyLoss |  0.46317327\n",
            "Step  39700: eval  CrossEntropyLoss |  0.54374939\n",
            "Step  39700: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  39800: Ran 100 train steps in 17.48 secs\n",
            "Step  39800: train CrossEntropyLoss |  0.47245410\n",
            "Step  39800: eval  CrossEntropyLoss |  0.24973011\n",
            "Step  39800: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  39900: Ran 100 train steps in 17.48 secs\n",
            "Step  39900: train CrossEntropyLoss |  0.48743820\n",
            "Step  39900: eval  CrossEntropyLoss |  0.59472942\n",
            "Step  39900: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  40000: Ran 100 train steps in 17.47 secs\n",
            "Step  40000: train CrossEntropyLoss |  0.48750284\n",
            "Step  40000: eval  CrossEntropyLoss |  0.32389283\n",
            "Step  40000: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  40100: Ran 100 train steps in 17.53 secs\n",
            "Step  40100: train CrossEntropyLoss |  0.47822830\n",
            "Step  40100: eval  CrossEntropyLoss |  0.50423312\n",
            "Step  40100: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  40200: Ran 100 train steps in 17.63 secs\n",
            "Step  40200: train CrossEntropyLoss |  0.47102278\n",
            "Step  40200: eval  CrossEntropyLoss |  0.58207512\n",
            "Step  40200: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  40300: Ran 100 train steps in 17.51 secs\n",
            "Step  40300: train CrossEntropyLoss |  0.46678045\n",
            "Step  40300: eval  CrossEntropyLoss |  0.39881992\n",
            "Step  40300: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  40400: Ran 100 train steps in 17.56 secs\n",
            "Step  40400: train CrossEntropyLoss |  0.45958498\n",
            "Step  40400: eval  CrossEntropyLoss |  0.40802112\n",
            "Step  40400: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  40500: Ran 100 train steps in 17.57 secs\n",
            "Step  40500: train CrossEntropyLoss |  0.47135833\n",
            "Step  40500: eval  CrossEntropyLoss |  0.60091025\n",
            "Step  40500: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  40600: Ran 100 train steps in 17.70 secs\n",
            "Step  40600: train CrossEntropyLoss |  0.48881516\n",
            "Step  40600: eval  CrossEntropyLoss |  0.53296751\n",
            "Step  40600: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  40700: Ran 100 train steps in 17.64 secs\n",
            "Step  40700: train CrossEntropyLoss |  0.45876297\n",
            "Step  40700: eval  CrossEntropyLoss |  0.48981959\n",
            "Step  40700: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  40800: Ran 100 train steps in 17.71 secs\n",
            "Step  40800: train CrossEntropyLoss |  0.47819152\n",
            "Step  40800: eval  CrossEntropyLoss |  0.37267449\n",
            "Step  40800: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  40900: Ran 100 train steps in 17.53 secs\n",
            "Step  40900: train CrossEntropyLoss |  0.47146431\n",
            "Step  40900: eval  CrossEntropyLoss |  0.45025018\n",
            "Step  40900: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  41000: Ran 100 train steps in 17.47 secs\n",
            "Step  41000: train CrossEntropyLoss |  0.48536304\n",
            "Step  41000: eval  CrossEntropyLoss |  0.53145784\n",
            "Step  41000: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  41100: Ran 100 train steps in 17.64 secs\n",
            "Step  41100: train CrossEntropyLoss |  0.47971934\n",
            "Step  41100: eval  CrossEntropyLoss |  0.64195859\n",
            "Step  41100: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  41200: Ran 100 train steps in 17.54 secs\n",
            "Step  41200: train CrossEntropyLoss |  0.50174844\n",
            "Step  41200: eval  CrossEntropyLoss |  0.65304810\n",
            "Step  41200: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  41300: Ran 100 train steps in 17.56 secs\n",
            "Step  41300: train CrossEntropyLoss |  0.48266724\n",
            "Step  41300: eval  CrossEntropyLoss |  0.78110611\n",
            "Step  41300: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step  41400: Ran 100 train steps in 17.54 secs\n",
            "Step  41400: train CrossEntropyLoss |  0.46096891\n",
            "Step  41400: eval  CrossEntropyLoss |  0.45459419\n",
            "Step  41400: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  41500: Ran 100 train steps in 17.63 secs\n",
            "Step  41500: train CrossEntropyLoss |  0.44710031\n",
            "Step  41500: eval  CrossEntropyLoss |  0.39501423\n",
            "Step  41500: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  41600: Ran 100 train steps in 17.58 secs\n",
            "Step  41600: train CrossEntropyLoss |  0.45233494\n",
            "Step  41600: eval  CrossEntropyLoss |  0.41171318\n",
            "Step  41600: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  41700: Ran 100 train steps in 17.55 secs\n",
            "Step  41700: train CrossEntropyLoss |  0.51590401\n",
            "Step  41700: eval  CrossEntropyLoss |  0.66751593\n",
            "Step  41700: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  41800: Ran 100 train steps in 17.56 secs\n",
            "Step  41800: train CrossEntropyLoss |  0.48524264\n",
            "Step  41800: eval  CrossEntropyLoss |  0.38375106\n",
            "Step  41800: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  41900: Ran 100 train steps in 17.56 secs\n",
            "Step  41900: train CrossEntropyLoss |  0.48872092\n",
            "Step  41900: eval  CrossEntropyLoss |  0.38358682\n",
            "Step  41900: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  42000: Ran 100 train steps in 17.76 secs\n",
            "Step  42000: train CrossEntropyLoss |  0.48024914\n",
            "Step  42000: eval  CrossEntropyLoss |  0.38060582\n",
            "Step  42000: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  42100: Ran 100 train steps in 17.58 secs\n",
            "Step  42100: train CrossEntropyLoss |  0.47291130\n",
            "Step  42100: eval  CrossEntropyLoss |  0.61897248\n",
            "Step  42100: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  42200: Ran 100 train steps in 17.54 secs\n",
            "Step  42200: train CrossEntropyLoss |  0.46493179\n",
            "Step  42200: eval  CrossEntropyLoss |  0.49160567\n",
            "Step  42200: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  42300: Ran 100 train steps in 17.66 secs\n",
            "Step  42300: train CrossEntropyLoss |  0.47871688\n",
            "Step  42300: eval  CrossEntropyLoss |  0.58624977\n",
            "Step  42300: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  42400: Ran 100 train steps in 17.75 secs\n",
            "Step  42400: train CrossEntropyLoss |  0.47367069\n",
            "Step  42400: eval  CrossEntropyLoss |  0.40494633\n",
            "Step  42400: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  42500: Ran 100 train steps in 17.78 secs\n",
            "Step  42500: train CrossEntropyLoss |  0.49098375\n",
            "Step  42500: eval  CrossEntropyLoss |  0.39346933\n",
            "Step  42500: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  42600: Ran 100 train steps in 17.83 secs\n",
            "Step  42600: train CrossEntropyLoss |  0.47156185\n",
            "Step  42600: eval  CrossEntropyLoss |  0.54393679\n",
            "Step  42600: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  42700: Ran 100 train steps in 18.53 secs\n",
            "Step  42700: train CrossEntropyLoss |  0.46962151\n",
            "Step  42700: eval  CrossEntropyLoss |  0.37538424\n",
            "Step  42700: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  42800: Ran 100 train steps in 17.66 secs\n",
            "Step  42800: train CrossEntropyLoss |  0.47213030\n",
            "Step  42800: eval  CrossEntropyLoss |  0.55391711\n",
            "Step  42800: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  42900: Ran 100 train steps in 17.73 secs\n",
            "Step  42900: train CrossEntropyLoss |  0.46497154\n",
            "Step  42900: eval  CrossEntropyLoss |  0.65083075\n",
            "Step  42900: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  43000: Ran 100 train steps in 17.90 secs\n",
            "Step  43000: train CrossEntropyLoss |  0.47991589\n",
            "Step  43000: eval  CrossEntropyLoss |  0.40866727\n",
            "Step  43000: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  43100: Ran 100 train steps in 17.59 secs\n",
            "Step  43100: train CrossEntropyLoss |  0.48284265\n",
            "Step  43100: eval  CrossEntropyLoss |  0.74949425\n",
            "Step  43100: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  43200: Ran 100 train steps in 17.66 secs\n",
            "Step  43200: train CrossEntropyLoss |  0.45776317\n",
            "Step  43200: eval  CrossEntropyLoss |  0.42407113\n",
            "Step  43200: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  43300: Ran 100 train steps in 17.75 secs\n",
            "Step  43300: train CrossEntropyLoss |  0.45112848\n",
            "Step  43300: eval  CrossEntropyLoss |  0.43490240\n",
            "Step  43300: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  43400: Ran 100 train steps in 17.76 secs\n",
            "Step  43400: train CrossEntropyLoss |  0.48259830\n",
            "Step  43400: eval  CrossEntropyLoss |  0.47469789\n",
            "Step  43400: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  43500: Ran 100 train steps in 17.74 secs\n",
            "Step  43500: train CrossEntropyLoss |  0.47507843\n",
            "Step  43500: eval  CrossEntropyLoss |  0.44212961\n",
            "Step  43500: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  43600: Ran 100 train steps in 17.69 secs\n",
            "Step  43600: train CrossEntropyLoss |  0.46282905\n",
            "Step  43600: eval  CrossEntropyLoss |  0.47300199\n",
            "Step  43600: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  43700: Ran 100 train steps in 17.68 secs\n",
            "Step  43700: train CrossEntropyLoss |  0.46827921\n",
            "Step  43700: eval  CrossEntropyLoss |  0.59573251\n",
            "Step  43700: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  43800: Ran 100 train steps in 17.82 secs\n",
            "Step  43800: train CrossEntropyLoss |  0.48442379\n",
            "Step  43800: eval  CrossEntropyLoss |  0.47962421\n",
            "Step  43800: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  43900: Ran 100 train steps in 17.74 secs\n",
            "Step  43900: train CrossEntropyLoss |  0.47283849\n",
            "Step  43900: eval  CrossEntropyLoss |  0.44257757\n",
            "Step  43900: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  44000: Ran 100 train steps in 17.78 secs\n",
            "Step  44000: train CrossEntropyLoss |  0.49617004\n",
            "Step  44000: eval  CrossEntropyLoss |  0.38426107\n",
            "Step  44000: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  44100: Ran 100 train steps in 17.85 secs\n",
            "Step  44100: train CrossEntropyLoss |  0.46438614\n",
            "Step  44100: eval  CrossEntropyLoss |  0.56131518\n",
            "Step  44100: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  44200: Ran 100 train steps in 17.76 secs\n",
            "Step  44200: train CrossEntropyLoss |  0.47071588\n",
            "Step  44200: eval  CrossEntropyLoss |  0.36207220\n",
            "Step  44200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  44300: Ran 100 train steps in 18.11 secs\n",
            "Step  44300: train CrossEntropyLoss |  0.49300334\n",
            "Step  44300: eval  CrossEntropyLoss |  0.37878305\n",
            "Step  44300: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  44400: Ran 100 train steps in 17.77 secs\n",
            "Step  44400: train CrossEntropyLoss |  0.43974441\n",
            "Step  44400: eval  CrossEntropyLoss |  0.63907504\n",
            "Step  44400: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  44500: Ran 100 train steps in 17.71 secs\n",
            "Step  44500: train CrossEntropyLoss |  0.48896739\n",
            "Step  44500: eval  CrossEntropyLoss |  0.64007145\n",
            "Step  44500: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  44600: Ran 100 train steps in 17.82 secs\n",
            "Step  44600: train CrossEntropyLoss |  0.46681657\n",
            "Step  44600: eval  CrossEntropyLoss |  0.49531549\n",
            "Step  44600: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  44700: Ran 100 train steps in 17.94 secs\n",
            "Step  44700: train CrossEntropyLoss |  0.45905516\n",
            "Step  44700: eval  CrossEntropyLoss |  0.37165207\n",
            "Step  44700: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  44800: Ran 100 train steps in 17.83 secs\n",
            "Step  44800: train CrossEntropyLoss |  0.47655827\n",
            "Step  44800: eval  CrossEntropyLoss |  0.36313856\n",
            "Step  44800: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  44900: Ran 100 train steps in 17.79 secs\n",
            "Step  44900: train CrossEntropyLoss |  0.46322599\n",
            "Step  44900: eval  CrossEntropyLoss |  0.55559957\n",
            "Step  44900: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  45000: Ran 100 train steps in 17.74 secs\n",
            "Step  45000: train CrossEntropyLoss |  0.43450010\n",
            "Step  45000: eval  CrossEntropyLoss |  0.48250610\n",
            "Step  45000: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  45100: Ran 100 train steps in 17.89 secs\n",
            "Step  45100: train CrossEntropyLoss |  0.47072983\n",
            "Step  45100: eval  CrossEntropyLoss |  0.43828005\n",
            "Step  45100: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  45200: Ran 100 train steps in 17.91 secs\n",
            "Step  45200: train CrossEntropyLoss |  0.48288494\n",
            "Step  45200: eval  CrossEntropyLoss |  0.40507424\n",
            "Step  45200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  45300: Ran 100 train steps in 18.74 secs\n",
            "Step  45300: train CrossEntropyLoss |  0.48494443\n",
            "Step  45300: eval  CrossEntropyLoss |  0.38877630\n",
            "Step  45300: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  45400: Ran 100 train steps in 17.83 secs\n",
            "Step  45400: train CrossEntropyLoss |  0.47749862\n",
            "Step  45400: eval  CrossEntropyLoss |  0.47819784\n",
            "Step  45400: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  45500: Ran 100 train steps in 17.75 secs\n",
            "Step  45500: train CrossEntropyLoss |  0.48717111\n",
            "Step  45500: eval  CrossEntropyLoss |  0.41740078\n",
            "Step  45500: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  45600: Ran 100 train steps in 17.97 secs\n",
            "Step  45600: train CrossEntropyLoss |  0.47173595\n",
            "Step  45600: eval  CrossEntropyLoss |  0.39231959\n",
            "Step  45600: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  45700: Ran 100 train steps in 17.79 secs\n",
            "Step  45700: train CrossEntropyLoss |  0.48857176\n",
            "Step  45700: eval  CrossEntropyLoss |  0.66702747\n",
            "Step  45700: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  45800: Ran 100 train steps in 17.96 secs\n",
            "Step  45800: train CrossEntropyLoss |  0.46456391\n",
            "Step  45800: eval  CrossEntropyLoss |  0.37018690\n",
            "Step  45800: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  45900: Ran 100 train steps in 17.75 secs\n",
            "Step  45900: train CrossEntropyLoss |  0.46072811\n",
            "Step  45900: eval  CrossEntropyLoss |  0.28746986\n",
            "Step  45900: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  46000: Ran 100 train steps in 17.92 secs\n",
            "Step  46000: train CrossEntropyLoss |  0.45638451\n",
            "Step  46000: eval  CrossEntropyLoss |  0.52984160\n",
            "Step  46000: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  46100: Ran 100 train steps in 18.07 secs\n",
            "Step  46100: train CrossEntropyLoss |  0.46173236\n",
            "Step  46100: eval  CrossEntropyLoss |  0.99134684\n",
            "Step  46100: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  46200: Ran 100 train steps in 17.85 secs\n",
            "Step  46200: train CrossEntropyLoss |  0.45562041\n",
            "Step  46200: eval  CrossEntropyLoss |  0.49405587\n",
            "Step  46200: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  46300: Ran 100 train steps in 17.87 secs\n",
            "Step  46300: train CrossEntropyLoss |  0.45487806\n",
            "Step  46300: eval  CrossEntropyLoss |  0.45024291\n",
            "Step  46300: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  46400: Ran 100 train steps in 17.83 secs\n",
            "Step  46400: train CrossEntropyLoss |  0.46531704\n",
            "Step  46400: eval  CrossEntropyLoss |  0.36487147\n",
            "Step  46400: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  46500: Ran 100 train steps in 17.95 secs\n",
            "Step  46500: train CrossEntropyLoss |  0.47731605\n",
            "Step  46500: eval  CrossEntropyLoss |  0.46465376\n",
            "Step  46500: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  46600: Ran 100 train steps in 17.81 secs\n",
            "Step  46600: train CrossEntropyLoss |  0.45774564\n",
            "Step  46600: eval  CrossEntropyLoss |  0.56926388\n",
            "Step  46600: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  46700: Ran 100 train steps in 17.80 secs\n",
            "Step  46700: train CrossEntropyLoss |  0.48042235\n",
            "Step  46700: eval  CrossEntropyLoss |  0.51212823\n",
            "Step  46700: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  46800: Ran 100 train steps in 17.79 secs\n",
            "Step  46800: train CrossEntropyLoss |  0.47626880\n",
            "Step  46800: eval  CrossEntropyLoss |  0.60351568\n",
            "Step  46800: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  46900: Ran 100 train steps in 17.90 secs\n",
            "Step  46900: train CrossEntropyLoss |  0.47685465\n",
            "Step  46900: eval  CrossEntropyLoss |  0.53695941\n",
            "Step  46900: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  47000: Ran 100 train steps in 17.89 secs\n",
            "Step  47000: train CrossEntropyLoss |  0.45124915\n",
            "Step  47000: eval  CrossEntropyLoss |  0.60322875\n",
            "Step  47000: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  47100: Ran 100 train steps in 17.82 secs\n",
            "Step  47100: train CrossEntropyLoss |  0.47221842\n",
            "Step  47100: eval  CrossEntropyLoss |  0.47935227\n",
            "Step  47100: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  47200: Ran 100 train steps in 17.77 secs\n",
            "Step  47200: train CrossEntropyLoss |  0.44912654\n",
            "Step  47200: eval  CrossEntropyLoss |  0.27656287\n",
            "Step  47200: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  47300: Ran 100 train steps in 17.76 secs\n",
            "Step  47300: train CrossEntropyLoss |  0.46395987\n",
            "Step  47300: eval  CrossEntropyLoss |  0.48799282\n",
            "Step  47300: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  47400: Ran 100 train steps in 17.97 secs\n",
            "Step  47400: train CrossEntropyLoss |  0.47672525\n",
            "Step  47400: eval  CrossEntropyLoss |  0.49672502\n",
            "Step  47400: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  47500: Ran 100 train steps in 17.90 secs\n",
            "Step  47500: train CrossEntropyLoss |  0.47301027\n",
            "Step  47500: eval  CrossEntropyLoss |  0.48631623\n",
            "Step  47500: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  47600: Ran 100 train steps in 17.79 secs\n",
            "Step  47600: train CrossEntropyLoss |  0.48017898\n",
            "Step  47600: eval  CrossEntropyLoss |  0.35953331\n",
            "Step  47600: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  47700: Ran 100 train steps in 17.85 secs\n",
            "Step  47700: train CrossEntropyLoss |  0.48462325\n",
            "Step  47700: eval  CrossEntropyLoss |  0.52219409\n",
            "Step  47700: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  47800: Ran 100 train steps in 18.12 secs\n",
            "Step  47800: train CrossEntropyLoss |  0.45133871\n",
            "Step  47800: eval  CrossEntropyLoss |  0.50269854\n",
            "Step  47800: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  47900: Ran 100 train steps in 17.92 secs\n",
            "Step  47900: train CrossEntropyLoss |  0.47653121\n",
            "Step  47900: eval  CrossEntropyLoss |  0.64102197\n",
            "Step  47900: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  48000: Ran 100 train steps in 17.82 secs\n",
            "Step  48000: train CrossEntropyLoss |  0.47447586\n",
            "Step  48000: eval  CrossEntropyLoss |  0.46999669\n",
            "Step  48000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  48100: Ran 100 train steps in 17.80 secs\n",
            "Step  48100: train CrossEntropyLoss |  0.45385993\n",
            "Step  48100: eval  CrossEntropyLoss |  0.34260923\n",
            "Step  48100: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  48200: Ran 100 train steps in 17.88 secs\n",
            "Step  48200: train CrossEntropyLoss |  0.48050648\n",
            "Step  48200: eval  CrossEntropyLoss |  0.45404184\n",
            "Step  48200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  48300: Ran 100 train steps in 18.03 secs\n",
            "Step  48300: train CrossEntropyLoss |  0.48104736\n",
            "Step  48300: eval  CrossEntropyLoss |  0.43211174\n",
            "Step  48300: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  48400: Ran 100 train steps in 17.92 secs\n",
            "Step  48400: train CrossEntropyLoss |  0.47771353\n",
            "Step  48400: eval  CrossEntropyLoss |  0.81643289\n",
            "Step  48400: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  48500: Ran 100 train steps in 17.91 secs\n",
            "Step  48500: train CrossEntropyLoss |  0.46685264\n",
            "Step  48500: eval  CrossEntropyLoss |  0.41617990\n",
            "Step  48500: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  48600: Ran 100 train steps in 17.94 secs\n",
            "Step  48600: train CrossEntropyLoss |  0.48076379\n",
            "Step  48600: eval  CrossEntropyLoss |  0.46123779\n",
            "Step  48600: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  48700: Ran 100 train steps in 17.95 secs\n",
            "Step  48700: train CrossEntropyLoss |  0.45566168\n",
            "Step  48700: eval  CrossEntropyLoss |  0.75914294\n",
            "Step  48700: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  48800: Ran 100 train steps in 18.03 secs\n",
            "Step  48800: train CrossEntropyLoss |  0.47112986\n",
            "Step  48800: eval  CrossEntropyLoss |  0.51107591\n",
            "Step  48800: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  48900: Ran 100 train steps in 17.89 secs\n",
            "Step  48900: train CrossEntropyLoss |  0.46342364\n",
            "Step  48900: eval  CrossEntropyLoss |  0.51388758\n",
            "Step  48900: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  49000: Ran 100 train steps in 17.88 secs\n",
            "Step  49000: train CrossEntropyLoss |  0.47411808\n",
            "Step  49000: eval  CrossEntropyLoss |  0.63696367\n",
            "Step  49000: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step  49100: Ran 100 train steps in 17.82 secs\n",
            "Step  49100: train CrossEntropyLoss |  0.46181411\n",
            "Step  49100: eval  CrossEntropyLoss |  0.61501598\n",
            "Step  49100: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  49200: Ran 100 train steps in 18.08 secs\n",
            "Step  49200: train CrossEntropyLoss |  0.45985126\n",
            "Step  49200: eval  CrossEntropyLoss |  0.20469728\n",
            "Step  49200: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  49300: Ran 100 train steps in 17.91 secs\n",
            "Step  49300: train CrossEntropyLoss |  0.45388854\n",
            "Step  49300: eval  CrossEntropyLoss |  0.23314306\n",
            "Step  49300: eval          Accuracy |  1.00000000\n",
            "\n",
            "Step  49400: Ran 100 train steps in 17.96 secs\n",
            "Step  49400: train CrossEntropyLoss |  0.46729663\n",
            "Step  49400: eval  CrossEntropyLoss |  0.53165632\n",
            "Step  49400: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  49500: Ran 100 train steps in 18.07 secs\n",
            "Step  49500: train CrossEntropyLoss |  0.47715053\n",
            "Step  49500: eval  CrossEntropyLoss |  0.47998554\n",
            "Step  49500: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  49600: Ran 100 train steps in 18.02 secs\n",
            "Step  49600: train CrossEntropyLoss |  0.48301917\n",
            "Step  49600: eval  CrossEntropyLoss |  0.31654084\n",
            "Step  49600: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  49700: Ran 100 train steps in 18.04 secs\n",
            "Step  49700: train CrossEntropyLoss |  0.44647849\n",
            "Step  49700: eval  CrossEntropyLoss |  0.55540437\n",
            "Step  49700: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  49800: Ran 100 train steps in 17.98 secs\n",
            "Step  49800: train CrossEntropyLoss |  0.47963268\n",
            "Step  49800: eval  CrossEntropyLoss |  0.40331075\n",
            "Step  49800: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  49900: Ran 100 train steps in 17.91 secs\n",
            "Step  49900: train CrossEntropyLoss |  0.46523550\n",
            "Step  49900: eval  CrossEntropyLoss |  0.78575981\n",
            "Step  49900: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  50000: Ran 100 train steps in 18.00 secs\n",
            "Step  50000: train CrossEntropyLoss |  0.47421649\n",
            "Step  50000: eval  CrossEntropyLoss |  0.72476405\n",
            "Step  50000: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  50100: Ran 100 train steps in 18.11 secs\n",
            "Step  50100: train CrossEntropyLoss |  0.46702930\n",
            "Step  50100: eval  CrossEntropyLoss |  0.36719131\n",
            "Step  50100: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  50200: Ran 100 train steps in 17.91 secs\n",
            "Step  50200: train CrossEntropyLoss |  0.49667162\n",
            "Step  50200: eval  CrossEntropyLoss |  0.38743958\n",
            "Step  50200: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  50300: Ran 100 train steps in 17.94 secs\n",
            "Step  50300: train CrossEntropyLoss |  0.47040659\n",
            "Step  50300: eval  CrossEntropyLoss |  0.41592208\n",
            "Step  50300: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  50400: Ran 100 train steps in 17.93 secs\n",
            "Step  50400: train CrossEntropyLoss |  0.45096371\n",
            "Step  50400: eval  CrossEntropyLoss |  0.34536707\n",
            "Step  50400: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  50500: Ran 100 train steps in 18.00 secs\n",
            "Step  50500: train CrossEntropyLoss |  0.48000067\n",
            "Step  50500: eval  CrossEntropyLoss |  0.36363298\n",
            "Step  50500: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  50600: Ran 100 train steps in 18.07 secs\n",
            "Step  50600: train CrossEntropyLoss |  0.46871760\n",
            "Step  50600: eval  CrossEntropyLoss |  0.37070870\n",
            "Step  50600: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  50700: Ran 100 train steps in 18.00 secs\n",
            "Step  50700: train CrossEntropyLoss |  0.45285410\n",
            "Step  50700: eval  CrossEntropyLoss |  0.43252841\n",
            "Step  50700: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  50800: Ran 100 train steps in 18.02 secs\n",
            "Step  50800: train CrossEntropyLoss |  0.48352587\n",
            "Step  50800: eval  CrossEntropyLoss |  0.76566309\n",
            "Step  50800: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  50900: Ran 100 train steps in 18.07 secs\n",
            "Step  50900: train CrossEntropyLoss |  0.48148292\n",
            "Step  50900: eval  CrossEntropyLoss |  0.38800094\n",
            "Step  50900: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  51000: Ran 100 train steps in 18.16 secs\n",
            "Step  51000: train CrossEntropyLoss |  0.49542755\n",
            "Step  51000: eval  CrossEntropyLoss |  0.55216157\n",
            "Step  51000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  51100: Ran 100 train steps in 17.96 secs\n",
            "Step  51100: train CrossEntropyLoss |  0.48277426\n",
            "Step  51100: eval  CrossEntropyLoss |  0.28821647\n",
            "Step  51100: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  51200: Ran 100 train steps in 18.11 secs\n",
            "Step  51200: train CrossEntropyLoss |  0.46510333\n",
            "Step  51200: eval  CrossEntropyLoss |  0.49347657\n",
            "Step  51200: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  51300: Ran 100 train steps in 18.05 secs\n",
            "Step  51300: train CrossEntropyLoss |  0.46925235\n",
            "Step  51300: eval  CrossEntropyLoss |  0.47979981\n",
            "Step  51300: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  51400: Ran 100 train steps in 18.08 secs\n",
            "Step  51400: train CrossEntropyLoss |  0.47324762\n",
            "Step  51400: eval  CrossEntropyLoss |  0.38056493\n",
            "Step  51400: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  51500: Ran 100 train steps in 17.99 secs\n",
            "Step  51500: train CrossEntropyLoss |  0.46552461\n",
            "Step  51500: eval  CrossEntropyLoss |  0.68973136\n",
            "Step  51500: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  51600: Ran 100 train steps in 17.95 secs\n",
            "Step  51600: train CrossEntropyLoss |  0.47049463\n",
            "Step  51600: eval  CrossEntropyLoss |  0.35895634\n",
            "Step  51600: eval          Accuracy |  1.00000000\n",
            "\n",
            "Step  51700: Ran 100 train steps in 18.02 secs\n",
            "Step  51700: train CrossEntropyLoss |  0.48522940\n",
            "Step  51700: eval  CrossEntropyLoss |  0.32725778\n",
            "Step  51700: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  51800: Ran 100 train steps in 18.02 secs\n",
            "Step  51800: train CrossEntropyLoss |  0.47219428\n",
            "Step  51800: eval  CrossEntropyLoss |  0.51036781\n",
            "Step  51800: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  51900: Ran 100 train steps in 18.10 secs\n",
            "Step  51900: train CrossEntropyLoss |  0.48451248\n",
            "Step  51900: eval  CrossEntropyLoss |  0.35226929\n",
            "Step  51900: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  52000: Ran 100 train steps in 18.04 secs\n",
            "Step  52000: train CrossEntropyLoss |  0.45553482\n",
            "Step  52000: eval  CrossEntropyLoss |  0.48773390\n",
            "Step  52000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  52100: Ran 100 train steps in 18.04 secs\n",
            "Step  52100: train CrossEntropyLoss |  0.44712475\n",
            "Step  52100: eval  CrossEntropyLoss |  0.59233385\n",
            "Step  52100: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  52200: Ran 100 train steps in 17.99 secs\n",
            "Step  52200: train CrossEntropyLoss |  0.47147247\n",
            "Step  52200: eval  CrossEntropyLoss |  0.44677231\n",
            "Step  52200: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  52300: Ran 100 train steps in 18.15 secs\n",
            "Step  52300: train CrossEntropyLoss |  0.44277409\n",
            "Step  52300: eval  CrossEntropyLoss |  0.39383793\n",
            "Step  52300: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  52400: Ran 100 train steps in 18.05 secs\n",
            "Step  52400: train CrossEntropyLoss |  0.47024095\n",
            "Step  52400: eval  CrossEntropyLoss |  0.42571136\n",
            "Step  52400: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  52500: Ran 100 train steps in 18.04 secs\n",
            "Step  52500: train CrossEntropyLoss |  0.44376701\n",
            "Step  52500: eval  CrossEntropyLoss |  0.48892170\n",
            "Step  52500: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  52600: Ran 100 train steps in 18.13 secs\n",
            "Step  52600: train CrossEntropyLoss |  0.45400757\n",
            "Step  52600: eval  CrossEntropyLoss |  0.54988885\n",
            "Step  52600: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  52700: Ran 100 train steps in 18.16 secs\n",
            "Step  52700: train CrossEntropyLoss |  0.50332379\n",
            "Step  52700: eval  CrossEntropyLoss |  0.49111989\n",
            "Step  52700: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  52800: Ran 100 train steps in 18.13 secs\n",
            "Step  52800: train CrossEntropyLoss |  0.46176696\n",
            "Step  52800: eval  CrossEntropyLoss |  0.59717166\n",
            "Step  52800: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  52900: Ran 100 train steps in 18.10 secs\n",
            "Step  52900: train CrossEntropyLoss |  0.47217825\n",
            "Step  52900: eval  CrossEntropyLoss |  0.42122212\n",
            "Step  52900: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  53000: Ran 100 train steps in 18.21 secs\n",
            "Step  53000: train CrossEntropyLoss |  0.46195850\n",
            "Step  53000: eval  CrossEntropyLoss |  0.42249095\n",
            "Step  53000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  53100: Ran 100 train steps in 18.11 secs\n",
            "Step  53100: train CrossEntropyLoss |  0.48342615\n",
            "Step  53100: eval  CrossEntropyLoss |  0.51921797\n",
            "Step  53100: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  53200: Ran 100 train steps in 18.23 secs\n",
            "Step  53200: train CrossEntropyLoss |  0.44142413\n",
            "Step  53200: eval  CrossEntropyLoss |  0.31016955\n",
            "Step  53200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  53300: Ran 100 train steps in 18.08 secs\n",
            "Step  53300: train CrossEntropyLoss |  0.46781150\n",
            "Step  53300: eval  CrossEntropyLoss |  0.33842659\n",
            "Step  53300: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  53400: Ran 100 train steps in 18.12 secs\n",
            "Step  53400: train CrossEntropyLoss |  0.48534876\n",
            "Step  53400: eval  CrossEntropyLoss |  0.72401565\n",
            "Step  53400: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  53500: Ran 100 train steps in 18.07 secs\n",
            "Step  53500: train CrossEntropyLoss |  0.47791201\n",
            "Step  53500: eval  CrossEntropyLoss |  0.34996420\n",
            "Step  53500: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  53600: Ran 100 train steps in 18.13 secs\n",
            "Step  53600: train CrossEntropyLoss |  0.47920996\n",
            "Step  53600: eval  CrossEntropyLoss |  0.47883710\n",
            "Step  53600: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  53700: Ran 100 train steps in 18.16 secs\n",
            "Step  53700: train CrossEntropyLoss |  0.49203044\n",
            "Step  53700: eval  CrossEntropyLoss |  0.38298470\n",
            "Step  53700: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  53800: Ran 100 train steps in 18.05 secs\n",
            "Step  53800: train CrossEntropyLoss |  0.47836199\n",
            "Step  53800: eval  CrossEntropyLoss |  0.33888143\n",
            "Step  53800: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  53900: Ran 100 train steps in 18.02 secs\n",
            "Step  53900: train CrossEntropyLoss |  0.45971674\n",
            "Step  53900: eval  CrossEntropyLoss |  0.29096869\n",
            "Step  53900: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  54000: Ran 100 train steps in 18.06 secs\n",
            "Step  54000: train CrossEntropyLoss |  0.46622372\n",
            "Step  54000: eval  CrossEntropyLoss |  0.27599788\n",
            "Step  54000: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  54100: Ran 100 train steps in 18.18 secs\n",
            "Step  54100: train CrossEntropyLoss |  0.44235405\n",
            "Step  54100: eval  CrossEntropyLoss |  0.62184823\n",
            "Step  54100: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  54200: Ran 100 train steps in 18.07 secs\n",
            "Step  54200: train CrossEntropyLoss |  0.49454316\n",
            "Step  54200: eval  CrossEntropyLoss |  0.37771082\n",
            "Step  54200: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  54300: Ran 100 train steps in 18.11 secs\n",
            "Step  54300: train CrossEntropyLoss |  0.44937003\n",
            "Step  54300: eval  CrossEntropyLoss |  0.46755180\n",
            "Step  54300: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  54400: Ran 100 train steps in 18.26 secs\n",
            "Step  54400: train CrossEntropyLoss |  0.45963779\n",
            "Step  54400: eval  CrossEntropyLoss |  0.28615749\n",
            "Step  54400: eval          Accuracy |  1.00000000\n",
            "\n",
            "Step  54500: Ran 100 train steps in 18.14 secs\n",
            "Step  54500: train CrossEntropyLoss |  0.45809799\n",
            "Step  54500: eval  CrossEntropyLoss |  0.41823897\n",
            "Step  54500: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  54600: Ran 100 train steps in 18.14 secs\n",
            "Step  54600: train CrossEntropyLoss |  0.48648089\n",
            "Step  54600: eval  CrossEntropyLoss |  0.42126071\n",
            "Step  54600: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  54700: Ran 100 train steps in 18.20 secs\n",
            "Step  54700: train CrossEntropyLoss |  0.45579925\n",
            "Step  54700: eval  CrossEntropyLoss |  0.39566681\n",
            "Step  54700: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  54800: Ran 100 train steps in 18.05 secs\n",
            "Step  54800: train CrossEntropyLoss |  0.47398609\n",
            "Step  54800: eval  CrossEntropyLoss |  0.61954534\n",
            "Step  54800: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  54900: Ran 100 train steps in 18.11 secs\n",
            "Step  54900: train CrossEntropyLoss |  0.46842551\n",
            "Step  54900: eval  CrossEntropyLoss |  0.36889184\n",
            "Step  54900: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  55000: Ran 100 train steps in 18.19 secs\n",
            "Step  55000: train CrossEntropyLoss |  0.45875198\n",
            "Step  55000: eval  CrossEntropyLoss |  0.39561498\n",
            "Step  55000: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  55100: Ran 100 train steps in 18.10 secs\n",
            "Step  55100: train CrossEntropyLoss |  0.46531042\n",
            "Step  55100: eval  CrossEntropyLoss |  0.71030366\n",
            "Step  55100: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  55200: Ran 100 train steps in 18.15 secs\n",
            "Step  55200: train CrossEntropyLoss |  0.47940812\n",
            "Step  55200: eval  CrossEntropyLoss |  0.43593618\n",
            "Step  55200: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  55300: Ran 100 train steps in 18.07 secs\n",
            "Step  55300: train CrossEntropyLoss |  0.48514384\n",
            "Step  55300: eval  CrossEntropyLoss |  0.46200150\n",
            "Step  55300: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  55400: Ran 100 train steps in 18.24 secs\n",
            "Step  55400: train CrossEntropyLoss |  0.46466181\n",
            "Step  55400: eval  CrossEntropyLoss |  0.35268909\n",
            "Step  55400: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  55500: Ran 100 train steps in 18.21 secs\n",
            "Step  55500: train CrossEntropyLoss |  0.48683971\n",
            "Step  55500: eval  CrossEntropyLoss |  0.52575296\n",
            "Step  55500: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  55600: Ran 100 train steps in 18.09 secs\n",
            "Step  55600: train CrossEntropyLoss |  0.46216300\n",
            "Step  55600: eval  CrossEntropyLoss |  0.36887288\n",
            "Step  55600: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  55700: Ran 100 train steps in 18.15 secs\n",
            "Step  55700: train CrossEntropyLoss |  0.47579584\n",
            "Step  55700: eval  CrossEntropyLoss |  0.39896283\n",
            "Step  55700: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  55800: Ran 100 train steps in 18.12 secs\n",
            "Step  55800: train CrossEntropyLoss |  0.47333908\n",
            "Step  55800: eval  CrossEntropyLoss |  0.35137594\n",
            "Step  55800: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  55900: Ran 100 train steps in 18.23 secs\n",
            "Step  55900: train CrossEntropyLoss |  0.46256229\n",
            "Step  55900: eval  CrossEntropyLoss |  0.55870736\n",
            "Step  55900: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  56000: Ran 100 train steps in 18.11 secs\n",
            "Step  56000: train CrossEntropyLoss |  0.48883623\n",
            "Step  56000: eval  CrossEntropyLoss |  0.46594846\n",
            "Step  56000: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  56100: Ran 100 train steps in 18.06 secs\n",
            "Step  56100: train CrossEntropyLoss |  0.46164024\n",
            "Step  56100: eval  CrossEntropyLoss |  0.70917475\n",
            "Step  56100: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  56200: Ran 100 train steps in 18.10 secs\n",
            "Step  56200: train CrossEntropyLoss |  0.47479621\n",
            "Step  56200: eval  CrossEntropyLoss |  0.40124545\n",
            "Step  56200: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  56300: Ran 100 train steps in 18.24 secs\n",
            "Step  56300: train CrossEntropyLoss |  0.46098143\n",
            "Step  56300: eval  CrossEntropyLoss |  0.32065955\n",
            "Step  56300: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  56400: Ran 100 train steps in 18.24 secs\n",
            "Step  56400: train CrossEntropyLoss |  0.45274889\n",
            "Step  56400: eval  CrossEntropyLoss |  0.50208718\n",
            "Step  56400: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  56500: Ran 100 train steps in 18.08 secs\n",
            "Step  56500: train CrossEntropyLoss |  0.45413107\n",
            "Step  56500: eval  CrossEntropyLoss |  0.43855879\n",
            "Step  56500: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  56600: Ran 100 train steps in 18.12 secs\n",
            "Step  56600: train CrossEntropyLoss |  0.47076538\n",
            "Step  56600: eval  CrossEntropyLoss |  0.47477061\n",
            "Step  56600: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  56700: Ran 100 train steps in 18.18 secs\n",
            "Step  56700: train CrossEntropyLoss |  0.48404136\n",
            "Step  56700: eval  CrossEntropyLoss |  0.46741420\n",
            "Step  56700: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  56800: Ran 100 train steps in 18.22 secs\n",
            "Step  56800: train CrossEntropyLoss |  0.47649774\n",
            "Step  56800: eval  CrossEntropyLoss |  0.47069329\n",
            "Step  56800: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  56900: Ran 100 train steps in 18.07 secs\n",
            "Step  56900: train CrossEntropyLoss |  0.47457823\n",
            "Step  56900: eval  CrossEntropyLoss |  0.54571867\n",
            "Step  56900: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step  57000: Ran 100 train steps in 18.11 secs\n",
            "Step  57000: train CrossEntropyLoss |  0.46361169\n",
            "Step  57000: eval  CrossEntropyLoss |  0.32468462\n",
            "Step  57000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  57100: Ran 100 train steps in 18.13 secs\n",
            "Step  57100: train CrossEntropyLoss |  0.50144953\n",
            "Step  57100: eval  CrossEntropyLoss |  0.33171439\n",
            "Step  57100: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  57200: Ran 100 train steps in 18.26 secs\n",
            "Step  57200: train CrossEntropyLoss |  0.45899877\n",
            "Step  57200: eval  CrossEntropyLoss |  0.59419227\n",
            "Step  57200: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  57300: Ran 100 train steps in 18.14 secs\n",
            "Step  57300: train CrossEntropyLoss |  0.46968058\n",
            "Step  57300: eval  CrossEntropyLoss |  0.49299061\n",
            "Step  57300: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  57400: Ran 100 train steps in 18.17 secs\n",
            "Step  57400: train CrossEntropyLoss |  0.46485656\n",
            "Step  57400: eval  CrossEntropyLoss |  0.43110237\n",
            "Step  57400: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  57500: Ran 100 train steps in 18.19 secs\n",
            "Step  57500: train CrossEntropyLoss |  0.46174586\n",
            "Step  57500: eval  CrossEntropyLoss |  0.42349970\n",
            "Step  57500: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  57600: Ran 100 train steps in 18.20 secs\n",
            "Step  57600: train CrossEntropyLoss |  0.46834931\n",
            "Step  57600: eval  CrossEntropyLoss |  0.30700117\n",
            "Step  57600: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  57700: Ran 100 train steps in 18.21 secs\n",
            "Step  57700: train CrossEntropyLoss |  0.47348219\n",
            "Step  57700: eval  CrossEntropyLoss |  0.27743378\n",
            "Step  57700: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  57800: Ran 100 train steps in 18.21 secs\n",
            "Step  57800: train CrossEntropyLoss |  0.49664122\n",
            "Step  57800: eval  CrossEntropyLoss |  0.51027501\n",
            "Step  57800: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  57900: Ran 100 train steps in 18.20 secs\n",
            "Step  57900: train CrossEntropyLoss |  0.47641864\n",
            "Step  57900: eval  CrossEntropyLoss |  0.49407691\n",
            "Step  57900: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  58000: Ran 100 train steps in 18.22 secs\n",
            "Step  58000: train CrossEntropyLoss |  0.48013940\n",
            "Step  58000: eval  CrossEntropyLoss |  0.35452199\n",
            "Step  58000: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  58100: Ran 100 train steps in 18.50 secs\n",
            "Step  58100: train CrossEntropyLoss |  0.48434001\n",
            "Step  58100: eval  CrossEntropyLoss |  0.48422909\n",
            "Step  58100: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  58200: Ran 100 train steps in 18.18 secs\n",
            "Step  58200: train CrossEntropyLoss |  0.46381435\n",
            "Step  58200: eval  CrossEntropyLoss |  0.33025730\n",
            "Step  58200: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  58300: Ran 100 train steps in 18.18 secs\n",
            "Step  58300: train CrossEntropyLoss |  0.47980186\n",
            "Step  58300: eval  CrossEntropyLoss |  0.41149294\n",
            "Step  58300: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  58400: Ran 100 train steps in 18.17 secs\n",
            "Step  58400: train CrossEntropyLoss |  0.45721048\n",
            "Step  58400: eval  CrossEntropyLoss |  0.60573381\n",
            "Step  58400: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  58500: Ran 100 train steps in 18.35 secs\n",
            "Step  58500: train CrossEntropyLoss |  0.46970683\n",
            "Step  58500: eval  CrossEntropyLoss |  0.72650725\n",
            "Step  58500: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  58600: Ran 100 train steps in 18.15 secs\n",
            "Step  58600: train CrossEntropyLoss |  0.45732501\n",
            "Step  58600: eval  CrossEntropyLoss |  0.43657652\n",
            "Step  58600: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  58700: Ran 100 train steps in 18.17 secs\n",
            "Step  58700: train CrossEntropyLoss |  0.44489127\n",
            "Step  58700: eval  CrossEntropyLoss |  0.44778466\n",
            "Step  58700: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  58800: Ran 100 train steps in 18.12 secs\n",
            "Step  58800: train CrossEntropyLoss |  0.46449694\n",
            "Step  58800: eval  CrossEntropyLoss |  0.43302435\n",
            "Step  58800: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  58900: Ran 100 train steps in 18.20 secs\n",
            "Step  58900: train CrossEntropyLoss |  0.47955003\n",
            "Step  58900: eval  CrossEntropyLoss |  0.45331442\n",
            "Step  58900: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  59000: Ran 100 train steps in 18.18 secs\n",
            "Step  59000: train CrossEntropyLoss |  0.45699385\n",
            "Step  59000: eval  CrossEntropyLoss |  0.52078372\n",
            "Step  59000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  59100: Ran 100 train steps in 18.22 secs\n",
            "Step  59100: train CrossEntropyLoss |  0.46619552\n",
            "Step  59100: eval  CrossEntropyLoss |  0.29544908\n",
            "Step  59100: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  59200: Ran 100 train steps in 18.13 secs\n",
            "Step  59200: train CrossEntropyLoss |  0.46513274\n",
            "Step  59200: eval  CrossEntropyLoss |  0.49550161\n",
            "Step  59200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  59300: Ran 100 train steps in 18.20 secs\n",
            "Step  59300: train CrossEntropyLoss |  0.46185753\n",
            "Step  59300: eval  CrossEntropyLoss |  0.81067592\n",
            "Step  59300: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  59400: Ran 100 train steps in 18.32 secs\n",
            "Step  59400: train CrossEntropyLoss |  0.47561404\n",
            "Step  59400: eval  CrossEntropyLoss |  0.54295886\n",
            "Step  59400: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  59500: Ran 100 train steps in 18.18 secs\n",
            "Step  59500: train CrossEntropyLoss |  0.46393776\n",
            "Step  59500: eval  CrossEntropyLoss |  0.60679471\n",
            "Step  59500: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  59600: Ran 100 train steps in 18.23 secs\n",
            "Step  59600: train CrossEntropyLoss |  0.46881992\n",
            "Step  59600: eval  CrossEntropyLoss |  0.43434754\n",
            "Step  59600: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  59700: Ran 100 train steps in 18.11 secs\n",
            "Step  59700: train CrossEntropyLoss |  0.48531702\n",
            "Step  59700: eval  CrossEntropyLoss |  0.28601581\n",
            "Step  59700: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  59800: Ran 100 train steps in 18.43 secs\n",
            "Step  59800: train CrossEntropyLoss |  0.45637062\n",
            "Step  59800: eval  CrossEntropyLoss |  0.31194994\n",
            "Step  59800: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  59900: Ran 100 train steps in 18.27 secs\n",
            "Step  59900: train CrossEntropyLoss |  0.47195107\n",
            "Step  59900: eval  CrossEntropyLoss |  0.31957468\n",
            "Step  59900: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  60000: Ran 100 train steps in 18.17 secs\n",
            "Step  60000: train CrossEntropyLoss |  0.46165970\n",
            "Step  60000: eval  CrossEntropyLoss |  0.48129901\n",
            "Step  60000: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  60100: Ran 100 train steps in 18.25 secs\n",
            "Step  60100: train CrossEntropyLoss |  0.47104773\n",
            "Step  60100: eval  CrossEntropyLoss |  0.55068988\n",
            "Step  60100: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  60200: Ran 100 train steps in 18.19 secs\n",
            "Step  60200: train CrossEntropyLoss |  0.47936931\n",
            "Step  60200: eval  CrossEntropyLoss |  0.41597262\n",
            "Step  60200: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  60300: Ran 100 train steps in 18.35 secs\n",
            "Step  60300: train CrossEntropyLoss |  0.46133888\n",
            "Step  60300: eval  CrossEntropyLoss |  0.79861230\n",
            "Step  60300: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  60400: Ran 100 train steps in 18.21 secs\n",
            "Step  60400: train CrossEntropyLoss |  0.46810234\n",
            "Step  60400: eval  CrossEntropyLoss |  0.29113609\n",
            "Step  60400: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  60500: Ran 100 train steps in 18.21 secs\n",
            "Step  60500: train CrossEntropyLoss |  0.46120596\n",
            "Step  60500: eval  CrossEntropyLoss |  0.51476026\n",
            "Step  60500: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  60600: Ran 100 train steps in 18.20 secs\n",
            "Step  60600: train CrossEntropyLoss |  0.47318098\n",
            "Step  60600: eval  CrossEntropyLoss |  0.59534067\n",
            "Step  60600: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  60700: Ran 100 train steps in 18.33 secs\n",
            "Step  60700: train CrossEntropyLoss |  0.46907529\n",
            "Step  60700: eval  CrossEntropyLoss |  0.43167186\n",
            "Step  60700: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  60800: Ran 100 train steps in 18.22 secs\n",
            "Step  60800: train CrossEntropyLoss |  0.44255006\n",
            "Step  60800: eval  CrossEntropyLoss |  0.43356109\n",
            "Step  60800: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  60900: Ran 100 train steps in 18.25 secs\n",
            "Step  60900: train CrossEntropyLoss |  0.46551299\n",
            "Step  60900: eval  CrossEntropyLoss |  0.54700857\n",
            "Step  60900: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  61000: Ran 100 train steps in 18.38 secs\n",
            "Step  61000: train CrossEntropyLoss |  0.43174312\n",
            "Step  61000: eval  CrossEntropyLoss |  0.43353423\n",
            "Step  61000: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  61100: Ran 100 train steps in 18.32 secs\n",
            "Step  61100: train CrossEntropyLoss |  0.48965427\n",
            "Step  61100: eval  CrossEntropyLoss |  0.41710991\n",
            "Step  61100: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  61200: Ran 100 train steps in 18.28 secs\n",
            "Step  61200: train CrossEntropyLoss |  0.45593405\n",
            "Step  61200: eval  CrossEntropyLoss |  0.67522430\n",
            "Step  61200: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  61300: Ran 100 train steps in 18.32 secs\n",
            "Step  61300: train CrossEntropyLoss |  0.46189898\n",
            "Step  61300: eval  CrossEntropyLoss |  0.61476666\n",
            "Step  61300: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  61400: Ran 100 train steps in 18.26 secs\n",
            "Step  61400: train CrossEntropyLoss |  0.48284572\n",
            "Step  61400: eval  CrossEntropyLoss |  0.56513661\n",
            "Step  61400: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  61500: Ran 100 train steps in 18.48 secs\n",
            "Step  61500: train CrossEntropyLoss |  0.45600104\n",
            "Step  61500: eval  CrossEntropyLoss |  0.42854658\n",
            "Step  61500: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  61600: Ran 100 train steps in 18.33 secs\n",
            "Step  61600: train CrossEntropyLoss |  0.47344142\n",
            "Step  61600: eval  CrossEntropyLoss |  0.92023546\n",
            "Step  61600: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  61700: Ran 100 train steps in 18.25 secs\n",
            "Step  61700: train CrossEntropyLoss |  0.47462374\n",
            "Step  61700: eval  CrossEntropyLoss |  0.59631419\n",
            "Step  61700: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  61800: Ran 100 train steps in 18.30 secs\n",
            "Step  61800: train CrossEntropyLoss |  0.45395565\n",
            "Step  61800: eval  CrossEntropyLoss |  0.69130200\n",
            "Step  61800: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step  61900: Ran 100 train steps in 18.20 secs\n",
            "Step  61900: train CrossEntropyLoss |  0.48343673\n",
            "Step  61900: eval  CrossEntropyLoss |  0.31921950\n",
            "Step  61900: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  62000: Ran 100 train steps in 18.31 secs\n",
            "Step  62000: train CrossEntropyLoss |  0.44959903\n",
            "Step  62000: eval  CrossEntropyLoss |  0.67018592\n",
            "Step  62000: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step  62100: Ran 100 train steps in 18.26 secs\n",
            "Step  62100: train CrossEntropyLoss |  0.46481535\n",
            "Step  62100: eval  CrossEntropyLoss |  0.46942604\n",
            "Step  62100: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  62200: Ran 100 train steps in 18.28 secs\n",
            "Step  62200: train CrossEntropyLoss |  0.45839766\n",
            "Step  62200: eval  CrossEntropyLoss |  0.59066743\n",
            "Step  62200: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  62300: Ran 100 train steps in 18.27 secs\n",
            "Step  62300: train CrossEntropyLoss |  0.44918746\n",
            "Step  62300: eval  CrossEntropyLoss |  0.34298688\n",
            "Step  62300: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  62400: Ran 100 train steps in 18.32 secs\n",
            "Step  62400: train CrossEntropyLoss |  0.45555308\n",
            "Step  62400: eval  CrossEntropyLoss |  0.49262786\n",
            "Step  62400: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  62500: Ran 100 train steps in 18.47 secs\n",
            "Step  62500: train CrossEntropyLoss |  0.48201236\n",
            "Step  62500: eval  CrossEntropyLoss |  0.60311794\n",
            "Step  62500: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  62600: Ran 100 train steps in 18.29 secs\n",
            "Step  62600: train CrossEntropyLoss |  0.44560814\n",
            "Step  62600: eval  CrossEntropyLoss |  0.41602442\n",
            "Step  62600: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  62700: Ran 100 train steps in 18.32 secs\n",
            "Step  62700: train CrossEntropyLoss |  0.44732720\n",
            "Step  62700: eval  CrossEntropyLoss |  0.39671466\n",
            "Step  62700: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  62800: Ran 100 train steps in 18.31 secs\n",
            "Step  62800: train CrossEntropyLoss |  0.49037716\n",
            "Step  62800: eval  CrossEntropyLoss |  0.62458515\n",
            "Step  62800: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  62900: Ran 100 train steps in 18.42 secs\n",
            "Step  62900: train CrossEntropyLoss |  0.47510147\n",
            "Step  62900: eval  CrossEntropyLoss |  0.44437006\n",
            "Step  62900: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  63000: Ran 100 train steps in 18.30 secs\n",
            "Step  63000: train CrossEntropyLoss |  0.47792178\n",
            "Step  63000: eval  CrossEntropyLoss |  0.50992215\n",
            "Step  63000: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  63100: Ran 100 train steps in 19.33 secs\n",
            "Step  63100: train CrossEntropyLoss |  0.46674833\n",
            "Step  63100: eval  CrossEntropyLoss |  0.37008137\n",
            "Step  63100: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  63200: Ran 100 train steps in 18.57 secs\n",
            "Step  63200: train CrossEntropyLoss |  0.49172759\n",
            "Step  63200: eval  CrossEntropyLoss |  0.24163041\n",
            "Step  63200: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  63300: Ran 100 train steps in 18.46 secs\n",
            "Step  63300: train CrossEntropyLoss |  0.44844028\n",
            "Step  63300: eval  CrossEntropyLoss |  0.52570963\n",
            "Step  63300: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  63400: Ran 100 train steps in 18.41 secs\n",
            "Step  63400: train CrossEntropyLoss |  0.46977270\n",
            "Step  63400: eval  CrossEntropyLoss |  0.31372419\n",
            "Step  63400: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  63500: Ran 100 train steps in 18.31 secs\n",
            "Step  63500: train CrossEntropyLoss |  0.46408272\n",
            "Step  63500: eval  CrossEntropyLoss |  0.25716949\n",
            "Step  63500: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  63600: Ran 100 train steps in 18.31 secs\n",
            "Step  63600: train CrossEntropyLoss |  0.48641559\n",
            "Step  63600: eval  CrossEntropyLoss |  0.41935086\n",
            "Step  63600: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  63700: Ran 100 train steps in 18.35 secs\n",
            "Step  63700: train CrossEntropyLoss |  0.46584910\n",
            "Step  63700: eval  CrossEntropyLoss |  0.38440421\n",
            "Step  63700: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  63800: Ran 100 train steps in 18.55 secs\n",
            "Step  63800: train CrossEntropyLoss |  0.45750377\n",
            "Step  63800: eval  CrossEntropyLoss |  0.46278960\n",
            "Step  63800: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  63900: Ran 100 train steps in 18.39 secs\n",
            "Step  63900: train CrossEntropyLoss |  0.47110769\n",
            "Step  63900: eval  CrossEntropyLoss |  0.42396736\n",
            "Step  63900: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  64000: Ran 100 train steps in 18.31 secs\n",
            "Step  64000: train CrossEntropyLoss |  0.50419325\n",
            "Step  64000: eval  CrossEntropyLoss |  0.35617033\n",
            "Step  64000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  64100: Ran 100 train steps in 18.32 secs\n",
            "Step  64100: train CrossEntropyLoss |  0.48264721\n",
            "Step  64100: eval  CrossEntropyLoss |  0.66319674\n",
            "Step  64100: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  64200: Ran 100 train steps in 18.49 secs\n",
            "Step  64200: train CrossEntropyLoss |  0.44845980\n",
            "Step  64200: eval  CrossEntropyLoss |  0.55327791\n",
            "Step  64200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  64300: Ran 100 train steps in 18.41 secs\n",
            "Step  64300: train CrossEntropyLoss |  0.46365398\n",
            "Step  64300: eval  CrossEntropyLoss |  0.58627379\n",
            "Step  64300: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  64400: Ran 100 train steps in 18.40 secs\n",
            "Step  64400: train CrossEntropyLoss |  0.46126020\n",
            "Step  64400: eval  CrossEntropyLoss |  0.75240767\n",
            "Step  64400: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  64500: Ran 100 train steps in 18.35 secs\n",
            "Step  64500: train CrossEntropyLoss |  0.46548533\n",
            "Step  64500: eval  CrossEntropyLoss |  0.53379345\n",
            "Step  64500: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  64600: Ran 100 train steps in 18.42 secs\n",
            "Step  64600: train CrossEntropyLoss |  0.46271887\n",
            "Step  64600: eval  CrossEntropyLoss |  0.33699101\n",
            "Step  64600: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  64700: Ran 100 train steps in 19.30 secs\n",
            "Step  64700: train CrossEntropyLoss |  0.45878005\n",
            "Step  64700: eval  CrossEntropyLoss |  0.38139886\n",
            "Step  64700: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  64800: Ran 100 train steps in 18.47 secs\n",
            "Step  64800: train CrossEntropyLoss |  0.47386295\n",
            "Step  64800: eval  CrossEntropyLoss |  0.49035573\n",
            "Step  64800: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  64900: Ran 100 train steps in 18.56 secs\n",
            "Step  64900: train CrossEntropyLoss |  0.47871807\n",
            "Step  64900: eval  CrossEntropyLoss |  0.49066073\n",
            "Step  64900: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  65000: Ran 100 train steps in 18.37 secs\n",
            "Step  65000: train CrossEntropyLoss |  0.45718685\n",
            "Step  65000: eval  CrossEntropyLoss |  0.40507752\n",
            "Step  65000: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  65100: Ran 100 train steps in 18.47 secs\n",
            "Step  65100: train CrossEntropyLoss |  0.47430101\n",
            "Step  65100: eval  CrossEntropyLoss |  0.46762168\n",
            "Step  65100: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  65200: Ran 100 train steps in 18.42 secs\n",
            "Step  65200: train CrossEntropyLoss |  0.50000530\n",
            "Step  65200: eval  CrossEntropyLoss |  0.36043769\n",
            "Step  65200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  65300: Ran 100 train steps in 18.40 secs\n",
            "Step  65300: train CrossEntropyLoss |  0.45354754\n",
            "Step  65300: eval  CrossEntropyLoss |  0.37720698\n",
            "Step  65300: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  65400: Ran 100 train steps in 18.39 secs\n",
            "Step  65400: train CrossEntropyLoss |  0.47018072\n",
            "Step  65400: eval  CrossEntropyLoss |  0.38445762\n",
            "Step  65400: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  65500: Ran 100 train steps in 18.46 secs\n",
            "Step  65500: train CrossEntropyLoss |  0.47672811\n",
            "Step  65500: eval  CrossEntropyLoss |  0.44969431\n",
            "Step  65500: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  65600: Ran 100 train steps in 18.41 secs\n",
            "Step  65600: train CrossEntropyLoss |  0.48067397\n",
            "Step  65600: eval  CrossEntropyLoss |  0.47234994\n",
            "Step  65600: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  65700: Ran 100 train steps in 18.33 secs\n",
            "Step  65700: train CrossEntropyLoss |  0.46252882\n",
            "Step  65700: eval  CrossEntropyLoss |  0.45884937\n",
            "Step  65700: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  65800: Ran 100 train steps in 18.42 secs\n",
            "Step  65800: train CrossEntropyLoss |  0.47242606\n",
            "Step  65800: eval  CrossEntropyLoss |  0.59109151\n",
            "Step  65800: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  65900: Ran 100 train steps in 18.46 secs\n",
            "Step  65900: train CrossEntropyLoss |  0.47082821\n",
            "Step  65900: eval  CrossEntropyLoss |  0.31081438\n",
            "Step  65900: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  66000: Ran 100 train steps in 18.54 secs\n",
            "Step  66000: train CrossEntropyLoss |  0.46355814\n",
            "Step  66000: eval  CrossEntropyLoss |  0.71439725\n",
            "Step  66000: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  66100: Ran 100 train steps in 18.33 secs\n",
            "Step  66100: train CrossEntropyLoss |  0.46605808\n",
            "Step  66100: eval  CrossEntropyLoss |  0.68196523\n",
            "Step  66100: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  66200: Ran 100 train steps in 18.35 secs\n",
            "Step  66200: train CrossEntropyLoss |  0.49130911\n",
            "Step  66200: eval  CrossEntropyLoss |  0.39337403\n",
            "Step  66200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  66300: Ran 100 train steps in 18.42 secs\n",
            "Step  66300: train CrossEntropyLoss |  0.46628651\n",
            "Step  66300: eval  CrossEntropyLoss |  0.43502614\n",
            "Step  66300: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  66400: Ran 100 train steps in 18.51 secs\n",
            "Step  66400: train CrossEntropyLoss |  0.47221038\n",
            "Step  66400: eval  CrossEntropyLoss |  0.46568081\n",
            "Step  66400: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  66500: Ran 100 train steps in 18.64 secs\n",
            "Step  66500: train CrossEntropyLoss |  0.46820140\n",
            "Step  66500: eval  CrossEntropyLoss |  0.53735286\n",
            "Step  66500: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  66600: Ran 100 train steps in 18.49 secs\n",
            "Step  66600: train CrossEntropyLoss |  0.44666868\n",
            "Step  66600: eval  CrossEntropyLoss |  0.66304135\n",
            "Step  66600: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  66700: Ran 100 train steps in 18.32 secs\n",
            "Step  66700: train CrossEntropyLoss |  0.47972137\n",
            "Step  66700: eval  CrossEntropyLoss |  0.84766251\n",
            "Step  66700: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step  66800: Ran 100 train steps in 18.47 secs\n",
            "Step  66800: train CrossEntropyLoss |  0.47490004\n",
            "Step  66800: eval  CrossEntropyLoss |  0.23064075\n",
            "Step  66800: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  66900: Ran 100 train steps in 18.41 secs\n",
            "Step  66900: train CrossEntropyLoss |  0.47937402\n",
            "Step  66900: eval  CrossEntropyLoss |  0.55208945\n",
            "Step  66900: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  67000: Ran 100 train steps in 18.38 secs\n",
            "Step  67000: train CrossEntropyLoss |  0.45445147\n",
            "Step  67000: eval  CrossEntropyLoss |  0.36450541\n",
            "Step  67000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  67100: Ran 100 train steps in 18.40 secs\n",
            "Step  67100: train CrossEntropyLoss |  0.42820418\n",
            "Step  67100: eval  CrossEntropyLoss |  0.85818154\n",
            "Step  67100: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step  67200: Ran 100 train steps in 18.42 secs\n",
            "Step  67200: train CrossEntropyLoss |  0.45163018\n",
            "Step  67200: eval  CrossEntropyLoss |  0.34299046\n",
            "Step  67200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  67300: Ran 100 train steps in 18.58 secs\n",
            "Step  67300: train CrossEntropyLoss |  0.46756491\n",
            "Step  67300: eval  CrossEntropyLoss |  0.66440737\n",
            "Step  67300: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  67400: Ran 100 train steps in 18.36 secs\n",
            "Step  67400: train CrossEntropyLoss |  0.47075438\n",
            "Step  67400: eval  CrossEntropyLoss |  0.35095510\n",
            "Step  67400: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  67500: Ran 100 train steps in 18.42 secs\n",
            "Step  67500: train CrossEntropyLoss |  0.48166487\n",
            "Step  67500: eval  CrossEntropyLoss |  0.32297719\n",
            "Step  67500: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  67600: Ran 100 train steps in 18.58 secs\n",
            "Step  67600: train CrossEntropyLoss |  0.46075019\n",
            "Step  67600: eval  CrossEntropyLoss |  0.54395366\n",
            "Step  67600: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  67700: Ran 100 train steps in 19.41 secs\n",
            "Step  67700: train CrossEntropyLoss |  0.47867465\n",
            "Step  67700: eval  CrossEntropyLoss |  0.35526386\n",
            "Step  67700: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  67800: Ran 100 train steps in 18.49 secs\n",
            "Step  67800: train CrossEntropyLoss |  0.44946140\n",
            "Step  67800: eval  CrossEntropyLoss |  0.38066098\n",
            "Step  67800: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  67900: Ran 100 train steps in 18.48 secs\n",
            "Step  67900: train CrossEntropyLoss |  0.47402239\n",
            "Step  67900: eval  CrossEntropyLoss |  0.39659563\n",
            "Step  67900: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  68000: Ran 100 train steps in 18.40 secs\n",
            "Step  68000: train CrossEntropyLoss |  0.47680777\n",
            "Step  68000: eval  CrossEntropyLoss |  0.44191119\n",
            "Step  68000: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  68100: Ran 100 train steps in 18.60 secs\n",
            "Step  68100: train CrossEntropyLoss |  0.44603270\n",
            "Step  68100: eval  CrossEntropyLoss |  0.42480981\n",
            "Step  68100: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  68200: Ran 100 train steps in 18.78 secs\n",
            "Step  68200: train CrossEntropyLoss |  0.46348196\n",
            "Step  68200: eval  CrossEntropyLoss |  0.37906393\n",
            "Step  68200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  68300: Ran 100 train steps in 18.47 secs\n",
            "Step  68300: train CrossEntropyLoss |  0.47031176\n",
            "Step  68300: eval  CrossEntropyLoss |  0.58460003\n",
            "Step  68300: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  68400: Ran 100 train steps in 18.46 secs\n",
            "Step  68400: train CrossEntropyLoss |  0.47762260\n",
            "Step  68400: eval  CrossEntropyLoss |  0.39494550\n",
            "Step  68400: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  68500: Ran 100 train steps in 18.48 secs\n",
            "Step  68500: train CrossEntropyLoss |  0.48255774\n",
            "Step  68500: eval  CrossEntropyLoss |  0.71514636\n",
            "Step  68500: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step  68600: Ran 100 train steps in 18.56 secs\n",
            "Step  68600: train CrossEntropyLoss |  0.45537677\n",
            "Step  68600: eval  CrossEntropyLoss |  0.66503620\n",
            "Step  68600: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  68700: Ran 100 train steps in 18.42 secs\n",
            "Step  68700: train CrossEntropyLoss |  0.46523640\n",
            "Step  68700: eval  CrossEntropyLoss |  0.62197876\n",
            "Step  68700: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  68800: Ran 100 train steps in 18.46 secs\n",
            "Step  68800: train CrossEntropyLoss |  0.44973210\n",
            "Step  68800: eval  CrossEntropyLoss |  0.92898309\n",
            "Step  68800: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step  68900: Ran 100 train steps in 18.61 secs\n",
            "Step  68900: train CrossEntropyLoss |  0.47931767\n",
            "Step  68900: eval  CrossEntropyLoss |  0.37648129\n",
            "Step  68900: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  69000: Ran 100 train steps in 18.71 secs\n",
            "Step  69000: train CrossEntropyLoss |  0.45045352\n",
            "Step  69000: eval  CrossEntropyLoss |  0.61601597\n",
            "Step  69000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  69100: Ran 100 train steps in 18.67 secs\n",
            "Step  69100: train CrossEntropyLoss |  0.47835118\n",
            "Step  69100: eval  CrossEntropyLoss |  0.44054499\n",
            "Step  69100: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  69200: Ran 100 train steps in 18.86 secs\n",
            "Step  69200: train CrossEntropyLoss |  0.48057735\n",
            "Step  69200: eval  CrossEntropyLoss |  0.53069335\n",
            "Step  69200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  69300: Ran 100 train steps in 18.90 secs\n",
            "Step  69300: train CrossEntropyLoss |  0.46523830\n",
            "Step  69300: eval  CrossEntropyLoss |  0.41863394\n",
            "Step  69300: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  69400: Ran 100 train steps in 20.06 secs\n",
            "Step  69400: train CrossEntropyLoss |  0.47870481\n",
            "Step  69400: eval  CrossEntropyLoss |  0.46269208\n",
            "Step  69400: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  69500: Ran 100 train steps in 19.21 secs\n",
            "Step  69500: train CrossEntropyLoss |  0.48308462\n",
            "Step  69500: eval  CrossEntropyLoss |  0.36994404\n",
            "Step  69500: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  69600: Ran 100 train steps in 19.15 secs\n",
            "Step  69600: train CrossEntropyLoss |  0.45801380\n",
            "Step  69600: eval  CrossEntropyLoss |  0.38643146\n",
            "Step  69600: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  69700: Ran 100 train steps in 19.26 secs\n",
            "Step  69700: train CrossEntropyLoss |  0.46615919\n",
            "Step  69700: eval  CrossEntropyLoss |  0.50328445\n",
            "Step  69700: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  69800: Ran 100 train steps in 19.74 secs\n",
            "Step  69800: train CrossEntropyLoss |  0.46516407\n",
            "Step  69800: eval  CrossEntropyLoss |  0.30422747\n",
            "Step  69800: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  69900: Ran 100 train steps in 19.34 secs\n",
            "Step  69900: train CrossEntropyLoss |  0.48476928\n",
            "Step  69900: eval  CrossEntropyLoss |  0.37848741\n",
            "Step  69900: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  70000: Ran 100 train steps in 19.23 secs\n",
            "Step  70000: train CrossEntropyLoss |  0.45715749\n",
            "Step  70000: eval  CrossEntropyLoss |  0.64652723\n",
            "Step  70000: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  70100: Ran 100 train steps in 19.30 secs\n",
            "Step  70100: train CrossEntropyLoss |  0.45323685\n",
            "Step  70100: eval  CrossEntropyLoss |  0.50115436\n",
            "Step  70100: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  70200: Ran 100 train steps in 19.51 secs\n",
            "Step  70200: train CrossEntropyLoss |  0.47962093\n",
            "Step  70200: eval  CrossEntropyLoss |  0.40509269\n",
            "Step  70200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  70300: Ran 100 train steps in 19.54 secs\n",
            "Step  70300: train CrossEntropyLoss |  0.45075846\n",
            "Step  70300: eval  CrossEntropyLoss |  0.46945480\n",
            "Step  70300: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  70400: Ran 100 train steps in 19.59 secs\n",
            "Step  70400: train CrossEntropyLoss |  0.48308584\n",
            "Step  70400: eval  CrossEntropyLoss |  0.69416851\n",
            "Step  70400: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  70500: Ran 100 train steps in 19.53 secs\n",
            "Step  70500: train CrossEntropyLoss |  0.45623276\n",
            "Step  70500: eval  CrossEntropyLoss |  0.53128397\n",
            "Step  70500: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  70600: Ran 100 train steps in 19.50 secs\n",
            "Step  70600: train CrossEntropyLoss |  0.46759892\n",
            "Step  70600: eval  CrossEntropyLoss |  0.59370059\n",
            "Step  70600: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  70700: Ran 100 train steps in 19.58 secs\n",
            "Step  70700: train CrossEntropyLoss |  0.45952961\n",
            "Step  70700: eval  CrossEntropyLoss |  0.50898701\n",
            "Step  70700: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  70800: Ran 100 train steps in 19.62 secs\n",
            "Step  70800: train CrossEntropyLoss |  0.47317177\n",
            "Step  70800: eval  CrossEntropyLoss |  0.75954324\n",
            "Step  70800: eval          Accuracy |  0.43750000\n",
            "\n",
            "Step  70900: Ran 100 train steps in 19.62 secs\n",
            "Step  70900: train CrossEntropyLoss |  0.48100296\n",
            "Step  70900: eval  CrossEntropyLoss |  0.54991388\n",
            "Step  70900: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  71000: Ran 100 train steps in 19.86 secs\n",
            "Step  71000: train CrossEntropyLoss |  0.47345057\n",
            "Step  71000: eval  CrossEntropyLoss |  0.49155331\n",
            "Step  71000: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  71100: Ran 100 train steps in 19.88 secs\n",
            "Step  71100: train CrossEntropyLoss |  0.46764761\n",
            "Step  71100: eval  CrossEntropyLoss |  0.71218252\n",
            "Step  71100: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  71200: Ran 100 train steps in 19.84 secs\n",
            "Step  71200: train CrossEntropyLoss |  0.45226413\n",
            "Step  71200: eval  CrossEntropyLoss |  0.54070932\n",
            "Step  71200: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  71300: Ran 100 train steps in 19.92 secs\n",
            "Step  71300: train CrossEntropyLoss |  0.45833239\n",
            "Step  71300: eval  CrossEntropyLoss |  0.32759836\n",
            "Step  71300: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  71400: Ran 100 train steps in 20.21 secs\n",
            "Step  71400: train CrossEntropyLoss |  0.47040060\n",
            "Step  71400: eval  CrossEntropyLoss |  0.36891109\n",
            "Step  71400: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  71500: Ran 100 train steps in 20.09 secs\n",
            "Step  71500: train CrossEntropyLoss |  0.47111416\n",
            "Step  71500: eval  CrossEntropyLoss |  0.74778342\n",
            "Step  71500: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  71600: Ran 100 train steps in 19.90 secs\n",
            "Step  71600: train CrossEntropyLoss |  0.46629360\n",
            "Step  71600: eval  CrossEntropyLoss |  0.47018725\n",
            "Step  71600: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  71700: Ran 100 train steps in 20.10 secs\n",
            "Step  71700: train CrossEntropyLoss |  0.47634321\n",
            "Step  71700: eval  CrossEntropyLoss |  0.34920263\n",
            "Step  71700: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  71800: Ran 100 train steps in 20.03 secs\n",
            "Step  71800: train CrossEntropyLoss |  0.49923855\n",
            "Step  71800: eval  CrossEntropyLoss |  0.68801677\n",
            "Step  71800: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  71900: Ran 100 train steps in 20.17 secs\n",
            "Step  71900: train CrossEntropyLoss |  0.46785554\n",
            "Step  71900: eval  CrossEntropyLoss |  0.42950588\n",
            "Step  71900: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  72000: Ran 100 train steps in 20.03 secs\n",
            "Step  72000: train CrossEntropyLoss |  0.46269998\n",
            "Step  72000: eval  CrossEntropyLoss |  0.67534661\n",
            "Step  72000: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  72100: Ran 100 train steps in 20.05 secs\n",
            "Step  72100: train CrossEntropyLoss |  0.49025187\n",
            "Step  72100: eval  CrossEntropyLoss |  0.91130561\n",
            "Step  72100: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step  72200: Ran 100 train steps in 20.15 secs\n",
            "Step  72200: train CrossEntropyLoss |  0.46612495\n",
            "Step  72200: eval  CrossEntropyLoss |  0.48576120\n",
            "Step  72200: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  72300: Ran 100 train steps in 20.40 secs\n",
            "Step  72300: train CrossEntropyLoss |  0.47303158\n",
            "Step  72300: eval  CrossEntropyLoss |  0.47269729\n",
            "Step  72300: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  72400: Ran 100 train steps in 20.18 secs\n",
            "Step  72400: train CrossEntropyLoss |  0.46220255\n",
            "Step  72400: eval  CrossEntropyLoss |  0.55474895\n",
            "Step  72400: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  72500: Ran 100 train steps in 20.13 secs\n",
            "Step  72500: train CrossEntropyLoss |  0.48895717\n",
            "Step  72500: eval  CrossEntropyLoss |  0.35251480\n",
            "Step  72500: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  72600: Ran 100 train steps in 20.09 secs\n",
            "Step  72600: train CrossEntropyLoss |  0.45879343\n",
            "Step  72600: eval  CrossEntropyLoss |  0.39429611\n",
            "Step  72600: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  72700: Ran 100 train steps in 20.26 secs\n",
            "Step  72700: train CrossEntropyLoss |  0.46885803\n",
            "Step  72700: eval  CrossEntropyLoss |  0.53672379\n",
            "Step  72700: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  72800: Ran 100 train steps in 20.32 secs\n",
            "Step  72800: train CrossEntropyLoss |  0.46610266\n",
            "Step  72800: eval  CrossEntropyLoss |  0.38089615\n",
            "Step  72800: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  72900: Ran 100 train steps in 20.13 secs\n",
            "Step  72900: train CrossEntropyLoss |  0.45691812\n",
            "Step  72900: eval  CrossEntropyLoss |  0.57288700\n",
            "Step  72900: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  73000: Ran 100 train steps in 20.42 secs\n",
            "Step  73000: train CrossEntropyLoss |  0.47183546\n",
            "Step  73000: eval  CrossEntropyLoss |  0.26940733\n",
            "Step  73000: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  73100: Ran 100 train steps in 20.18 secs\n",
            "Step  73100: train CrossEntropyLoss |  0.47859618\n",
            "Step  73100: eval  CrossEntropyLoss |  0.29995722\n",
            "Step  73100: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  73200: Ran 100 train steps in 20.10 secs\n",
            "Step  73200: train CrossEntropyLoss |  0.48478115\n",
            "Step  73200: eval  CrossEntropyLoss |  0.65257895\n",
            "Step  73200: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  73300: Ran 100 train steps in 20.06 secs\n",
            "Step  73300: train CrossEntropyLoss |  0.44681495\n",
            "Step  73300: eval  CrossEntropyLoss |  0.66325009\n",
            "Step  73300: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  73400: Ran 100 train steps in 21.07 secs\n",
            "Step  73400: train CrossEntropyLoss |  0.46524948\n",
            "Step  73400: eval  CrossEntropyLoss |  0.47303322\n",
            "Step  73400: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  73500: Ran 100 train steps in 20.00 secs\n",
            "Step  73500: train CrossEntropyLoss |  0.46264607\n",
            "Step  73500: eval  CrossEntropyLoss |  0.28866762\n",
            "Step  73500: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  73600: Ran 100 train steps in 19.57 secs\n",
            "Step  73600: train CrossEntropyLoss |  0.47065955\n",
            "Step  73600: eval  CrossEntropyLoss |  0.35111663\n",
            "Step  73600: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  73700: Ran 100 train steps in 19.72 secs\n",
            "Step  73700: train CrossEntropyLoss |  0.46023601\n",
            "Step  73700: eval  CrossEntropyLoss |  0.45604426\n",
            "Step  73700: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  73800: Ran 100 train steps in 19.72 secs\n",
            "Step  73800: train CrossEntropyLoss |  0.50284588\n",
            "Step  73800: eval  CrossEntropyLoss |  0.20875052\n",
            "Step  73800: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  73900: Ran 100 train steps in 19.86 secs\n",
            "Step  73900: train CrossEntropyLoss |  0.46770084\n",
            "Step  73900: eval  CrossEntropyLoss |  0.29723445\n",
            "Step  73900: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  74000: Ran 100 train steps in 19.72 secs\n",
            "Step  74000: train CrossEntropyLoss |  0.46545824\n",
            "Step  74000: eval  CrossEntropyLoss |  0.46702850\n",
            "Step  74000: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  74100: Ran 100 train steps in 19.67 secs\n",
            "Step  74100: train CrossEntropyLoss |  0.45933962\n",
            "Step  74100: eval  CrossEntropyLoss |  0.33533633\n",
            "Step  74100: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  74200: Ran 100 train steps in 19.93 secs\n",
            "Step  74200: train CrossEntropyLoss |  0.47255203\n",
            "Step  74200: eval  CrossEntropyLoss |  0.42342985\n",
            "Step  74200: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  74300: Ran 100 train steps in 19.87 secs\n",
            "Step  74300: train CrossEntropyLoss |  0.46066558\n",
            "Step  74300: eval  CrossEntropyLoss |  0.52271897\n",
            "Step  74300: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  74400: Ran 100 train steps in 19.57 secs\n",
            "Step  74400: train CrossEntropyLoss |  0.46846539\n",
            "Step  74400: eval  CrossEntropyLoss |  0.29428333\n",
            "Step  74400: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  74500: Ran 100 train steps in 20.16 secs\n",
            "Step  74500: train CrossEntropyLoss |  0.46614084\n",
            "Step  74500: eval  CrossEntropyLoss |  0.44521061\n",
            "Step  74500: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  74600: Ran 100 train steps in 19.83 secs\n",
            "Step  74600: train CrossEntropyLoss |  0.43933365\n",
            "Step  74600: eval  CrossEntropyLoss |  1.22228432\n",
            "Step  74600: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  74700: Ran 100 train steps in 20.08 secs\n",
            "Step  74700: train CrossEntropyLoss |  0.48105010\n",
            "Step  74700: eval  CrossEntropyLoss |  0.39440405\n",
            "Step  74700: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  74800: Ran 100 train steps in 20.11 secs\n",
            "Step  74800: train CrossEntropyLoss |  0.44321114\n",
            "Step  74800: eval  CrossEntropyLoss |  0.62503612\n",
            "Step  74800: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  74900: Ran 100 train steps in 19.99 secs\n",
            "Step  74900: train CrossEntropyLoss |  0.46633887\n",
            "Step  74900: eval  CrossEntropyLoss |  0.50559390\n",
            "Step  74900: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  75000: Ran 100 train steps in 19.97 secs\n",
            "Step  75000: train CrossEntropyLoss |  0.47740221\n",
            "Step  75000: eval  CrossEntropyLoss |  0.38487780\n",
            "Step  75000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  75100: Ran 100 train steps in 19.87 secs\n",
            "Step  75100: train CrossEntropyLoss |  0.45797402\n",
            "Step  75100: eval  CrossEntropyLoss |  0.69664317\n",
            "Step  75100: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  75200: Ran 100 train steps in 19.59 secs\n",
            "Step  75200: train CrossEntropyLoss |  0.46673355\n",
            "Step  75200: eval  CrossEntropyLoss |  0.59233040\n",
            "Step  75200: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step  75300: Ran 100 train steps in 19.57 secs\n",
            "Step  75300: train CrossEntropyLoss |  0.48666248\n",
            "Step  75300: eval  CrossEntropyLoss |  0.67121267\n",
            "Step  75300: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  75400: Ran 100 train steps in 19.64 secs\n",
            "Step  75400: train CrossEntropyLoss |  0.45719352\n",
            "Step  75400: eval  CrossEntropyLoss |  0.60618031\n",
            "Step  75400: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  75500: Ran 100 train steps in 19.92 secs\n",
            "Step  75500: train CrossEntropyLoss |  0.46493739\n",
            "Step  75500: eval  CrossEntropyLoss |  0.36892733\n",
            "Step  75500: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  75600: Ran 100 train steps in 19.89 secs\n",
            "Step  75600: train CrossEntropyLoss |  0.48324844\n",
            "Step  75600: eval  CrossEntropyLoss |  0.39871320\n",
            "Step  75600: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  75700: Ran 100 train steps in 19.50 secs\n",
            "Step  75700: train CrossEntropyLoss |  0.47627649\n",
            "Step  75700: eval  CrossEntropyLoss |  0.32942203\n",
            "Step  75700: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  75800: Ran 100 train steps in 19.58 secs\n",
            "Step  75800: train CrossEntropyLoss |  0.48629692\n",
            "Step  75800: eval  CrossEntropyLoss |  0.59860152\n",
            "Step  75800: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  75900: Ran 100 train steps in 19.75 secs\n",
            "Step  75900: train CrossEntropyLoss |  0.46395907\n",
            "Step  75900: eval  CrossEntropyLoss |  0.54483253\n",
            "Step  75900: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  76000: Ran 100 train steps in 19.57 secs\n",
            "Step  76000: train CrossEntropyLoss |  0.46038255\n",
            "Step  76000: eval  CrossEntropyLoss |  0.47825935\n",
            "Step  76000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  76100: Ran 100 train steps in 20.05 secs\n",
            "Step  76100: train CrossEntropyLoss |  0.47878483\n",
            "Step  76100: eval  CrossEntropyLoss |  0.66575956\n",
            "Step  76100: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  76200: Ran 100 train steps in 19.90 secs\n",
            "Step  76200: train CrossEntropyLoss |  0.46013084\n",
            "Step  76200: eval  CrossEntropyLoss |  0.35246921\n",
            "Step  76200: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  76300: Ran 100 train steps in 20.06 secs\n",
            "Step  76300: train CrossEntropyLoss |  0.46063995\n",
            "Step  76300: eval  CrossEntropyLoss |  0.38019860\n",
            "Step  76300: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  76400: Ran 100 train steps in 19.92 secs\n",
            "Step  76400: train CrossEntropyLoss |  0.47213379\n",
            "Step  76400: eval  CrossEntropyLoss |  0.59366071\n",
            "Step  76400: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  76500: Ran 100 train steps in 19.54 secs\n",
            "Step  76500: train CrossEntropyLoss |  0.47422498\n",
            "Step  76500: eval  CrossEntropyLoss |  0.35616574\n",
            "Step  76500: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  76600: Ran 100 train steps in 19.30 secs\n",
            "Step  76600: train CrossEntropyLoss |  0.45117235\n",
            "Step  76600: eval  CrossEntropyLoss |  0.36989352\n",
            "Step  76600: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  76700: Ran 100 train steps in 19.46 secs\n",
            "Step  76700: train CrossEntropyLoss |  0.47875398\n",
            "Step  76700: eval  CrossEntropyLoss |  0.70813864\n",
            "Step  76700: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  76800: Ran 100 train steps in 19.09 secs\n",
            "Step  76800: train CrossEntropyLoss |  0.46916902\n",
            "Step  76800: eval  CrossEntropyLoss |  0.57652223\n",
            "Step  76800: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  76900: Ran 100 train steps in 19.16 secs\n",
            "Step  76900: train CrossEntropyLoss |  0.45049617\n",
            "Step  76900: eval  CrossEntropyLoss |  0.42513701\n",
            "Step  76900: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  77000: Ran 100 train steps in 18.97 secs\n",
            "Step  77000: train CrossEntropyLoss |  0.46978462\n",
            "Step  77000: eval  CrossEntropyLoss |  0.31962809\n",
            "Step  77000: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  77100: Ran 100 train steps in 19.01 secs\n",
            "Step  77100: train CrossEntropyLoss |  0.48440501\n",
            "Step  77100: eval  CrossEntropyLoss |  0.56546187\n",
            "Step  77100: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  77200: Ran 100 train steps in 18.81 secs\n",
            "Step  77200: train CrossEntropyLoss |  0.44685555\n",
            "Step  77200: eval  CrossEntropyLoss |  0.68351972\n",
            "Step  77200: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  77300: Ran 100 train steps in 18.86 secs\n",
            "Step  77300: train CrossEntropyLoss |  0.45006049\n",
            "Step  77300: eval  CrossEntropyLoss |  0.28352675\n",
            "Step  77300: eval          Accuracy |  1.00000000\n",
            "\n",
            "Step  77400: Ran 100 train steps in 18.74 secs\n",
            "Step  77400: train CrossEntropyLoss |  0.47467172\n",
            "Step  77400: eval  CrossEntropyLoss |  0.28716126\n",
            "Step  77400: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  77500: Ran 100 train steps in 18.79 secs\n",
            "Step  77500: train CrossEntropyLoss |  0.47917861\n",
            "Step  77500: eval  CrossEntropyLoss |  0.55840313\n",
            "Step  77500: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  77600: Ran 100 train steps in 18.83 secs\n",
            "Step  77600: train CrossEntropyLoss |  0.46346080\n",
            "Step  77600: eval  CrossEntropyLoss |  0.37819278\n",
            "Step  77600: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  77700: Ran 100 train steps in 19.02 secs\n",
            "Step  77700: train CrossEntropyLoss |  0.43602160\n",
            "Step  77700: eval  CrossEntropyLoss |  0.68313682\n",
            "Step  77700: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step  77800: Ran 100 train steps in 18.68 secs\n",
            "Step  77800: train CrossEntropyLoss |  0.45408544\n",
            "Step  77800: eval  CrossEntropyLoss |  0.40394628\n",
            "Step  77800: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  77900: Ran 100 train steps in 18.67 secs\n",
            "Step  77900: train CrossEntropyLoss |  0.44464996\n",
            "Step  77900: eval  CrossEntropyLoss |  0.54700792\n",
            "Step  77900: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  78000: Ran 100 train steps in 18.78 secs\n",
            "Step  78000: train CrossEntropyLoss |  0.47277588\n",
            "Step  78000: eval  CrossEntropyLoss |  0.55710948\n",
            "Step  78000: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  78100: Ran 100 train steps in 18.68 secs\n",
            "Step  78100: train CrossEntropyLoss |  0.49106628\n",
            "Step  78100: eval  CrossEntropyLoss |  0.39888889\n",
            "Step  78100: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  78200: Ran 100 train steps in 18.74 secs\n",
            "Step  78200: train CrossEntropyLoss |  0.48325640\n",
            "Step  78200: eval  CrossEntropyLoss |  0.43904632\n",
            "Step  78200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  78300: Ran 100 train steps in 18.65 secs\n",
            "Step  78300: train CrossEntropyLoss |  0.46591955\n",
            "Step  78300: eval  CrossEntropyLoss |  0.67657095\n",
            "Step  78300: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  78400: Ran 100 train steps in 18.88 secs\n",
            "Step  78400: train CrossEntropyLoss |  0.46285841\n",
            "Step  78400: eval  CrossEntropyLoss |  0.18252930\n",
            "Step  78400: eval          Accuracy |  1.00000000\n",
            "\n",
            "Step  78500: Ran 100 train steps in 18.70 secs\n",
            "Step  78500: train CrossEntropyLoss |  0.48838782\n",
            "Step  78500: eval  CrossEntropyLoss |  0.38033739\n",
            "Step  78500: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  78600: Ran 100 train steps in 18.97 secs\n",
            "Step  78600: train CrossEntropyLoss |  0.48581916\n",
            "Step  78600: eval  CrossEntropyLoss |  0.62183946\n",
            "Step  78600: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  78700: Ran 100 train steps in 18.73 secs\n",
            "Step  78700: train CrossEntropyLoss |  0.47610611\n",
            "Step  78700: eval  CrossEntropyLoss |  0.60043859\n",
            "Step  78700: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  78800: Ran 100 train steps in 18.77 secs\n",
            "Step  78800: train CrossEntropyLoss |  0.46573406\n",
            "Step  78800: eval  CrossEntropyLoss |  0.53842306\n",
            "Step  78800: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  78900: Ran 100 train steps in 18.78 secs\n",
            "Step  78900: train CrossEntropyLoss |  0.48956183\n",
            "Step  78900: eval  CrossEntropyLoss |  0.54927564\n",
            "Step  78900: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  79000: Ran 100 train steps in 18.69 secs\n",
            "Step  79000: train CrossEntropyLoss |  0.46352884\n",
            "Step  79000: eval  CrossEntropyLoss |  0.30229783\n",
            "Step  79000: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  79100: Ran 100 train steps in 18.70 secs\n",
            "Step  79100: train CrossEntropyLoss |  0.43892846\n",
            "Step  79100: eval  CrossEntropyLoss |  0.39252061\n",
            "Step  79100: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  79200: Ran 100 train steps in 18.78 secs\n",
            "Step  79200: train CrossEntropyLoss |  0.47544470\n",
            "Step  79200: eval  CrossEntropyLoss |  0.46671748\n",
            "Step  79200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  79300: Ran 100 train steps in 19.01 secs\n",
            "Step  79300: train CrossEntropyLoss |  0.47906530\n",
            "Step  79300: eval  CrossEntropyLoss |  0.55134028\n",
            "Step  79300: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  79400: Ran 100 train steps in 18.89 secs\n",
            "Step  79400: train CrossEntropyLoss |  0.48573157\n",
            "Step  79400: eval  CrossEntropyLoss |  0.28499663\n",
            "Step  79400: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  79500: Ran 100 train steps in 18.77 secs\n",
            "Step  79500: train CrossEntropyLoss |  0.44685712\n",
            "Step  79500: eval  CrossEntropyLoss |  0.31258553\n",
            "Step  79500: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  79600: Ran 100 train steps in 18.79 secs\n",
            "Step  79600: train CrossEntropyLoss |  0.46344894\n",
            "Step  79600: eval  CrossEntropyLoss |  0.45587063\n",
            "Step  79600: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  79700: Ran 100 train steps in 18.90 secs\n",
            "Step  79700: train CrossEntropyLoss |  0.46353912\n",
            "Step  79700: eval  CrossEntropyLoss |  0.66037333\n",
            "Step  79700: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  79800: Ran 100 train steps in 18.72 secs\n",
            "Step  79800: train CrossEntropyLoss |  0.47193590\n",
            "Step  79800: eval  CrossEntropyLoss |  0.93672454\n",
            "Step  79800: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  79900: Ran 100 train steps in 18.62 secs\n",
            "Step  79900: train CrossEntropyLoss |  0.46654710\n",
            "Step  79900: eval  CrossEntropyLoss |  0.38419616\n",
            "Step  79900: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  80000: Ran 100 train steps in 18.68 secs\n",
            "Step  80000: train CrossEntropyLoss |  0.44794154\n",
            "Step  80000: eval  CrossEntropyLoss |  0.57038313\n",
            "Step  80000: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  80100: Ran 100 train steps in 18.75 secs\n",
            "Step  80100: train CrossEntropyLoss |  0.45427588\n",
            "Step  80100: eval  CrossEntropyLoss |  0.42208534\n",
            "Step  80100: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  80200: Ran 100 train steps in 18.81 secs\n",
            "Step  80200: train CrossEntropyLoss |  0.48253244\n",
            "Step  80200: eval  CrossEntropyLoss |  0.47369462\n",
            "Step  80200: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  80300: Ran 100 train steps in 18.64 secs\n",
            "Step  80300: train CrossEntropyLoss |  0.44669169\n",
            "Step  80300: eval  CrossEntropyLoss |  0.49296242\n",
            "Step  80300: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  80400: Ran 100 train steps in 18.67 secs\n",
            "Step  80400: train CrossEntropyLoss |  0.46013600\n",
            "Step  80400: eval  CrossEntropyLoss |  0.42081654\n",
            "Step  80400: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  80500: Ran 100 train steps in 18.75 secs\n",
            "Step  80500: train CrossEntropyLoss |  0.47932515\n",
            "Step  80500: eval  CrossEntropyLoss |  0.52242965\n",
            "Step  80500: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  80600: Ran 100 train steps in 18.76 secs\n",
            "Step  80600: train CrossEntropyLoss |  0.45473370\n",
            "Step  80600: eval  CrossEntropyLoss |  0.44810551\n",
            "Step  80600: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  80700: Ran 100 train steps in 18.64 secs\n",
            "Step  80700: train CrossEntropyLoss |  0.48034960\n",
            "Step  80700: eval  CrossEntropyLoss |  0.39922726\n",
            "Step  80700: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  80800: Ran 100 train steps in 18.70 secs\n",
            "Step  80800: train CrossEntropyLoss |  0.48631540\n",
            "Step  80800: eval  CrossEntropyLoss |  0.49500209\n",
            "Step  80800: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  80900: Ran 100 train steps in 18.69 secs\n",
            "Step  80900: train CrossEntropyLoss |  0.46225297\n",
            "Step  80900: eval  CrossEntropyLoss |  0.38412991\n",
            "Step  80900: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  81000: Ran 100 train steps in 18.94 secs\n",
            "Step  81000: train CrossEntropyLoss |  0.46411917\n",
            "Step  81000: eval  CrossEntropyLoss |  0.62029517\n",
            "Step  81000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  81100: Ran 100 train steps in 18.61 secs\n",
            "Step  81100: train CrossEntropyLoss |  0.45705041\n",
            "Step  81100: eval  CrossEntropyLoss |  0.34525833\n",
            "Step  81100: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  81200: Ran 100 train steps in 18.68 secs\n",
            "Step  81200: train CrossEntropyLoss |  0.47168082\n",
            "Step  81200: eval  CrossEntropyLoss |  0.33370447\n",
            "Step  81200: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  81300: Ran 100 train steps in 18.68 secs\n",
            "Step  81300: train CrossEntropyLoss |  0.46337774\n",
            "Step  81300: eval  CrossEntropyLoss |  0.36885026\n",
            "Step  81300: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  81400: Ran 100 train steps in 18.87 secs\n",
            "Step  81400: train CrossEntropyLoss |  0.46804804\n",
            "Step  81400: eval  CrossEntropyLoss |  0.40676957\n",
            "Step  81400: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  81500: Ran 100 train steps in 18.66 secs\n",
            "Step  81500: train CrossEntropyLoss |  0.46663365\n",
            "Step  81500: eval  CrossEntropyLoss |  0.44658509\n",
            "Step  81500: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  81600: Ran 100 train steps in 18.65 secs\n",
            "Step  81600: train CrossEntropyLoss |  0.48242736\n",
            "Step  81600: eval  CrossEntropyLoss |  0.34953088\n",
            "Step  81600: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  81700: Ran 100 train steps in 18.64 secs\n",
            "Step  81700: train CrossEntropyLoss |  0.47668201\n",
            "Step  81700: eval  CrossEntropyLoss |  0.69632041\n",
            "Step  81700: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  81800: Ran 100 train steps in 18.85 secs\n",
            "Step  81800: train CrossEntropyLoss |  0.47977558\n",
            "Step  81800: eval  CrossEntropyLoss |  0.34867257\n",
            "Step  81800: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  81900: Ran 100 train steps in 18.66 secs\n",
            "Step  81900: train CrossEntropyLoss |  0.48114669\n",
            "Step  81900: eval  CrossEntropyLoss |  0.51722443\n",
            "Step  81900: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  82000: Ran 100 train steps in 18.62 secs\n",
            "Step  82000: train CrossEntropyLoss |  0.46352908\n",
            "Step  82000: eval  CrossEntropyLoss |  0.43405351\n",
            "Step  82000: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  82100: Ran 100 train steps in 18.72 secs\n",
            "Step  82100: train CrossEntropyLoss |  0.46917444\n",
            "Step  82100: eval  CrossEntropyLoss |  0.36676431\n",
            "Step  82100: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  82200: Ran 100 train steps in 18.69 secs\n",
            "Step  82200: train CrossEntropyLoss |  0.44304779\n",
            "Step  82200: eval  CrossEntropyLoss |  0.52465934\n",
            "Step  82200: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  82300: Ran 100 train steps in 18.78 secs\n",
            "Step  82300: train CrossEntropyLoss |  0.47629783\n",
            "Step  82300: eval  CrossEntropyLoss |  0.59393412\n",
            "Step  82300: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  82400: Ran 100 train steps in 18.70 secs\n",
            "Step  82400: train CrossEntropyLoss |  0.46534029\n",
            "Step  82400: eval  CrossEntropyLoss |  0.51390308\n",
            "Step  82400: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  82500: Ran 100 train steps in 18.65 secs\n",
            "Step  82500: train CrossEntropyLoss |  0.48097545\n",
            "Step  82500: eval  CrossEntropyLoss |  0.20622405\n",
            "Step  82500: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  82600: Ran 100 train steps in 18.74 secs\n",
            "Step  82600: train CrossEntropyLoss |  0.46962783\n",
            "Step  82600: eval  CrossEntropyLoss |  0.66586196\n",
            "Step  82600: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  82700: Ran 100 train steps in 19.03 secs\n",
            "Step  82700: train CrossEntropyLoss |  0.46229026\n",
            "Step  82700: eval  CrossEntropyLoss |  0.38841861\n",
            "Step  82700: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  82800: Ran 100 train steps in 18.80 secs\n",
            "Step  82800: train CrossEntropyLoss |  0.49645314\n",
            "Step  82800: eval  CrossEntropyLoss |  0.31628615\n",
            "Step  82800: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  82900: Ran 100 train steps in 18.80 secs\n",
            "Step  82900: train CrossEntropyLoss |  0.46852008\n",
            "Step  82900: eval  CrossEntropyLoss |  0.76140738\n",
            "Step  82900: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  83000: Ran 100 train steps in 18.87 secs\n",
            "Step  83000: train CrossEntropyLoss |  0.47985703\n",
            "Step  83000: eval  CrossEntropyLoss |  0.45566654\n",
            "Step  83000: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  83100: Ran 100 train steps in 19.09 secs\n",
            "Step  83100: train CrossEntropyLoss |  0.46995062\n",
            "Step  83100: eval  CrossEntropyLoss |  0.42153004\n",
            "Step  83100: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  83200: Ran 100 train steps in 19.07 secs\n",
            "Step  83200: train CrossEntropyLoss |  0.48764372\n",
            "Step  83200: eval  CrossEntropyLoss |  0.44441715\n",
            "Step  83200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  83300: Ran 100 train steps in 19.12 secs\n",
            "Step  83300: train CrossEntropyLoss |  0.44555849\n",
            "Step  83300: eval  CrossEntropyLoss |  0.34082243\n",
            "Step  83300: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  83400: Ran 100 train steps in 19.25 secs\n",
            "Step  83400: train CrossEntropyLoss |  0.46797344\n",
            "Step  83400: eval  CrossEntropyLoss |  0.55302304\n",
            "Step  83400: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  83500: Ran 100 train steps in 19.02 secs\n",
            "Step  83500: train CrossEntropyLoss |  0.46090961\n",
            "Step  83500: eval  CrossEntropyLoss |  0.37775195\n",
            "Step  83500: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  83600: Ran 100 train steps in 18.78 secs\n",
            "Step  83600: train CrossEntropyLoss |  0.47925112\n",
            "Step  83600: eval  CrossEntropyLoss |  0.47052696\n",
            "Step  83600: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  83700: Ran 100 train steps in 18.74 secs\n",
            "Step  83700: train CrossEntropyLoss |  0.43775642\n",
            "Step  83700: eval  CrossEntropyLoss |  0.39766315\n",
            "Step  83700: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  83800: Ran 100 train steps in 18.75 secs\n",
            "Step  83800: train CrossEntropyLoss |  0.44936904\n",
            "Step  83800: eval  CrossEntropyLoss |  0.51369673\n",
            "Step  83800: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  83900: Ran 100 train steps in 18.74 secs\n",
            "Step  83900: train CrossEntropyLoss |  0.47970343\n",
            "Step  83900: eval  CrossEntropyLoss |  0.45255581\n",
            "Step  83900: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  84000: Ran 100 train steps in 18.85 secs\n",
            "Step  84000: train CrossEntropyLoss |  0.45641634\n",
            "Step  84000: eval  CrossEntropyLoss |  0.36021551\n",
            "Step  84000: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  84100: Ran 100 train steps in 18.73 secs\n",
            "Step  84100: train CrossEntropyLoss |  0.43486711\n",
            "Step  84100: eval  CrossEntropyLoss |  0.42914414\n",
            "Step  84100: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  84200: Ran 100 train steps in 18.63 secs\n",
            "Step  84200: train CrossEntropyLoss |  0.48123321\n",
            "Step  84200: eval  CrossEntropyLoss |  0.35699132\n",
            "Step  84200: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  84300: Ran 100 train steps in 18.93 secs\n",
            "Step  84300: train CrossEntropyLoss |  0.46798500\n",
            "Step  84300: eval  CrossEntropyLoss |  0.35103139\n",
            "Step  84300: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  84400: Ran 100 train steps in 18.84 secs\n",
            "Step  84400: train CrossEntropyLoss |  0.46161452\n",
            "Step  84400: eval  CrossEntropyLoss |  0.73900688\n",
            "Step  84400: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  84500: Ran 100 train steps in 18.67 secs\n",
            "Step  84500: train CrossEntropyLoss |  0.45250177\n",
            "Step  84500: eval  CrossEntropyLoss |  0.44318211\n",
            "Step  84500: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  84600: Ran 100 train steps in 18.76 secs\n",
            "Step  84600: train CrossEntropyLoss |  0.49576256\n",
            "Step  84600: eval  CrossEntropyLoss |  0.66064930\n",
            "Step  84600: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  84700: Ran 100 train steps in 18.64 secs\n",
            "Step  84700: train CrossEntropyLoss |  0.47148174\n",
            "Step  84700: eval  CrossEntropyLoss |  0.40886873\n",
            "Step  84700: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  84800: Ran 100 train steps in 18.88 secs\n",
            "Step  84800: train CrossEntropyLoss |  0.48064995\n",
            "Step  84800: eval  CrossEntropyLoss |  0.41586673\n",
            "Step  84800: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  84900: Ran 100 train steps in 18.67 secs\n",
            "Step  84900: train CrossEntropyLoss |  0.47049129\n",
            "Step  84900: eval  CrossEntropyLoss |  0.43054035\n",
            "Step  84900: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  85000: Ran 100 train steps in 18.76 secs\n",
            "Step  85000: train CrossEntropyLoss |  0.49702755\n",
            "Step  85000: eval  CrossEntropyLoss |  0.66186452\n",
            "Step  85000: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  85100: Ran 100 train steps in 18.72 secs\n",
            "Step  85100: train CrossEntropyLoss |  0.47261715\n",
            "Step  85100: eval  CrossEntropyLoss |  0.43523344\n",
            "Step  85100: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  85200: Ran 100 train steps in 18.84 secs\n",
            "Step  85200: train CrossEntropyLoss |  0.47840735\n",
            "Step  85200: eval  CrossEntropyLoss |  0.31214675\n",
            "Step  85200: eval          Accuracy |  1.00000000\n",
            "\n",
            "Step  85300: Ran 100 train steps in 18.81 secs\n",
            "Step  85300: train CrossEntropyLoss |  0.46352053\n",
            "Step  85300: eval  CrossEntropyLoss |  0.26977104\n",
            "Step  85300: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  85400: Ran 100 train steps in 18.71 secs\n",
            "Step  85400: train CrossEntropyLoss |  0.47521186\n",
            "Step  85400: eval  CrossEntropyLoss |  0.53635693\n",
            "Step  85400: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  85500: Ran 100 train steps in 18.64 secs\n",
            "Step  85500: train CrossEntropyLoss |  0.48089322\n",
            "Step  85500: eval  CrossEntropyLoss |  0.54764873\n",
            "Step  85500: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  85600: Ran 100 train steps in 18.72 secs\n",
            "Step  85600: train CrossEntropyLoss |  0.45818901\n",
            "Step  85600: eval  CrossEntropyLoss |  0.43925071\n",
            "Step  85600: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  85700: Ran 100 train steps in 19.69 secs\n",
            "Step  85700: train CrossEntropyLoss |  0.47658888\n",
            "Step  85700: eval  CrossEntropyLoss |  0.47550249\n",
            "Step  85700: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  85800: Ran 100 train steps in 18.66 secs\n",
            "Step  85800: train CrossEntropyLoss |  0.48417419\n",
            "Step  85800: eval  CrossEntropyLoss |  0.36526927\n",
            "Step  85800: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  85900: Ran 100 train steps in 18.74 secs\n",
            "Step  85900: train CrossEntropyLoss |  0.45807177\n",
            "Step  85900: eval  CrossEntropyLoss |  0.57556796\n",
            "Step  85900: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  86000: Ran 100 train steps in 18.83 secs\n",
            "Step  86000: train CrossEntropyLoss |  0.45976087\n",
            "Step  86000: eval  CrossEntropyLoss |  0.49504590\n",
            "Step  86000: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  86100: Ran 100 train steps in 18.85 secs\n",
            "Step  86100: train CrossEntropyLoss |  0.48341042\n",
            "Step  86100: eval  CrossEntropyLoss |  0.34526461\n",
            "Step  86100: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  86200: Ran 100 train steps in 18.72 secs\n",
            "Step  86200: train CrossEntropyLoss |  0.47656825\n",
            "Step  86200: eval  CrossEntropyLoss |  0.43853426\n",
            "Step  86200: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  86300: Ran 100 train steps in 18.67 secs\n",
            "Step  86300: train CrossEntropyLoss |  0.48515731\n",
            "Step  86300: eval  CrossEntropyLoss |  0.51157862\n",
            "Step  86300: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  86400: Ran 100 train steps in 18.72 secs\n",
            "Step  86400: train CrossEntropyLoss |  0.49827549\n",
            "Step  86400: eval  CrossEntropyLoss |  0.41182697\n",
            "Step  86400: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  86500: Ran 100 train steps in 18.79 secs\n",
            "Step  86500: train CrossEntropyLoss |  0.47142491\n",
            "Step  86500: eval  CrossEntropyLoss |  0.69944209\n",
            "Step  86500: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  86600: Ran 100 train steps in 18.78 secs\n",
            "Step  86600: train CrossEntropyLoss |  0.46890861\n",
            "Step  86600: eval  CrossEntropyLoss |  0.39181224\n",
            "Step  86600: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  86700: Ran 100 train steps in 18.78 secs\n",
            "Step  86700: train CrossEntropyLoss |  0.46115521\n",
            "Step  86700: eval  CrossEntropyLoss |  0.75660944\n",
            "Step  86700: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step  86800: Ran 100 train steps in 18.76 secs\n",
            "Step  86800: train CrossEntropyLoss |  0.48164222\n",
            "Step  86800: eval  CrossEntropyLoss |  0.35034886\n",
            "Step  86800: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  86900: Ran 100 train steps in 18.84 secs\n",
            "Step  86900: train CrossEntropyLoss |  0.47334152\n",
            "Step  86900: eval  CrossEntropyLoss |  0.45813310\n",
            "Step  86900: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  87000: Ran 100 train steps in 18.79 secs\n",
            "Step  87000: train CrossEntropyLoss |  0.46035320\n",
            "Step  87000: eval  CrossEntropyLoss |  0.30218029\n",
            "Step  87000: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  87100: Ran 100 train steps in 18.67 secs\n",
            "Step  87100: train CrossEntropyLoss |  0.48246637\n",
            "Step  87100: eval  CrossEntropyLoss |  0.44696939\n",
            "Step  87100: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  87200: Ran 100 train steps in 18.82 secs\n",
            "Step  87200: train CrossEntropyLoss |  0.47620222\n",
            "Step  87200: eval  CrossEntropyLoss |  0.66180849\n",
            "Step  87200: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  87300: Ran 100 train steps in 18.77 secs\n",
            "Step  87300: train CrossEntropyLoss |  0.49265322\n",
            "Step  87300: eval  CrossEntropyLoss |  0.42382628\n",
            "Step  87300: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  87400: Ran 100 train steps in 18.81 secs\n",
            "Step  87400: train CrossEntropyLoss |  0.46276987\n",
            "Step  87400: eval  CrossEntropyLoss |  0.63645661\n",
            "Step  87400: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  87500: Ran 100 train steps in 18.70 secs\n",
            "Step  87500: train CrossEntropyLoss |  0.47237068\n",
            "Step  87500: eval  CrossEntropyLoss |  0.46049282\n",
            "Step  87500: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  87600: Ran 100 train steps in 18.90 secs\n",
            "Step  87600: train CrossEntropyLoss |  0.46610117\n",
            "Step  87600: eval  CrossEntropyLoss |  0.37095013\n",
            "Step  87600: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  87700: Ran 100 train steps in 18.69 secs\n",
            "Step  87700: train CrossEntropyLoss |  0.48295376\n",
            "Step  87700: eval  CrossEntropyLoss |  0.29175219\n",
            "Step  87700: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  87800: Ran 100 train steps in 18.88 secs\n",
            "Step  87800: train CrossEntropyLoss |  0.47500381\n",
            "Step  87800: eval  CrossEntropyLoss |  0.33000582\n",
            "Step  87800: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  87900: Ran 100 train steps in 18.72 secs\n",
            "Step  87900: train CrossEntropyLoss |  0.48425502\n",
            "Step  87900: eval  CrossEntropyLoss |  0.53729075\n",
            "Step  87900: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  88000: Ran 100 train steps in 18.74 secs\n",
            "Step  88000: train CrossEntropyLoss |  0.47833395\n",
            "Step  88000: eval  CrossEntropyLoss |  0.44865292\n",
            "Step  88000: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  88100: Ran 100 train steps in 18.74 secs\n",
            "Step  88100: train CrossEntropyLoss |  0.47263122\n",
            "Step  88100: eval  CrossEntropyLoss |  0.39018440\n",
            "Step  88100: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  88200: Ran 100 train steps in 18.74 secs\n",
            "Step  88200: train CrossEntropyLoss |  0.46178249\n",
            "Step  88200: eval  CrossEntropyLoss |  0.23479633\n",
            "Step  88200: eval          Accuracy |  1.00000000\n",
            "\n",
            "Step  88300: Ran 100 train steps in 18.80 secs\n",
            "Step  88300: train CrossEntropyLoss |  0.46096817\n",
            "Step  88300: eval  CrossEntropyLoss |  0.39211595\n",
            "Step  88300: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  88400: Ran 100 train steps in 18.68 secs\n",
            "Step  88400: train CrossEntropyLoss |  0.46586594\n",
            "Step  88400: eval  CrossEntropyLoss |  0.46003518\n",
            "Step  88400: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  88500: Ran 100 train steps in 18.80 secs\n",
            "Step  88500: train CrossEntropyLoss |  0.45165464\n",
            "Step  88500: eval  CrossEntropyLoss |  0.56967765\n",
            "Step  88500: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step  88600: Ran 100 train steps in 18.71 secs\n",
            "Step  88600: train CrossEntropyLoss |  0.46718815\n",
            "Step  88600: eval  CrossEntropyLoss |  0.50256538\n",
            "Step  88600: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  88700: Ran 100 train steps in 18.86 secs\n",
            "Step  88700: train CrossEntropyLoss |  0.47845724\n",
            "Step  88700: eval  CrossEntropyLoss |  0.32988861\n",
            "Step  88700: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  88800: Ran 100 train steps in 18.69 secs\n",
            "Step  88800: train CrossEntropyLoss |  0.47157797\n",
            "Step  88800: eval  CrossEntropyLoss |  0.47891748\n",
            "Step  88800: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  88900: Ran 100 train steps in 18.72 secs\n",
            "Step  88900: train CrossEntropyLoss |  0.45180652\n",
            "Step  88900: eval  CrossEntropyLoss |  0.39985210\n",
            "Step  88900: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  89000: Ran 100 train steps in 18.74 secs\n",
            "Step  89000: train CrossEntropyLoss |  0.46500331\n",
            "Step  89000: eval  CrossEntropyLoss |  0.35662603\n",
            "Step  89000: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  89100: Ran 100 train steps in 18.87 secs\n",
            "Step  89100: train CrossEntropyLoss |  0.44457674\n",
            "Step  89100: eval  CrossEntropyLoss |  0.29511231\n",
            "Step  89100: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  89200: Ran 100 train steps in 18.76 secs\n",
            "Step  89200: train CrossEntropyLoss |  0.46152884\n",
            "Step  89200: eval  CrossEntropyLoss |  0.79163235\n",
            "Step  89200: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step  89300: Ran 100 train steps in 18.88 secs\n",
            "Step  89300: train CrossEntropyLoss |  0.47145271\n",
            "Step  89300: eval  CrossEntropyLoss |  0.38179097\n",
            "Step  89300: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  89400: Ran 100 train steps in 18.68 secs\n",
            "Step  89400: train CrossEntropyLoss |  0.48290002\n",
            "Step  89400: eval  CrossEntropyLoss |  0.26524115\n",
            "Step  89400: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step  89500: Ran 100 train steps in 18.79 secs\n",
            "Step  89500: train CrossEntropyLoss |  0.45772737\n",
            "Step  89500: eval  CrossEntropyLoss |  0.52468574\n",
            "Step  89500: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  89600: Ran 100 train steps in 18.69 secs\n",
            "Step  89600: train CrossEntropyLoss |  0.47799757\n",
            "Step  89600: eval  CrossEntropyLoss |  0.38503096\n",
            "Step  89600: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  89700: Ran 100 train steps in 18.78 secs\n",
            "Step  89700: train CrossEntropyLoss |  0.45146126\n",
            "Step  89700: eval  CrossEntropyLoss |  0.51459289\n",
            "Step  89700: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  89800: Ran 100 train steps in 18.72 secs\n",
            "Step  89800: train CrossEntropyLoss |  0.48404753\n",
            "Step  89800: eval  CrossEntropyLoss |  0.54352808\n",
            "Step  89800: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step  89900: Ran 100 train steps in 18.77 secs\n",
            "Step  89900: train CrossEntropyLoss |  0.47334522\n",
            "Step  89900: eval  CrossEntropyLoss |  0.33268145\n",
            "Step  89900: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  90000: Ran 100 train steps in 18.74 secs\n",
            "Step  90000: train CrossEntropyLoss |  0.46073043\n",
            "Step  90000: eval  CrossEntropyLoss |  0.45758960\n",
            "Step  90000: eval          Accuracy |  0.75000000\n",
            "\n",
            "Step  90100: Ran 100 train steps in 18.76 secs\n",
            "Step  90100: train CrossEntropyLoss |  0.47619933\n",
            "Step  90100: eval  CrossEntropyLoss |  0.43226266\n",
            "Step  90100: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step  90200: Ran 100 train steps in 18.67 secs\n",
            "Step  90200: train CrossEntropyLoss |  0.47561079\n",
            "Step  90200: eval  CrossEntropyLoss |  0.98424405\n",
            "Step  90200: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  90300: Ran 100 train steps in 18.73 secs\n",
            "Step  90300: train CrossEntropyLoss |  0.47456014\n",
            "Step  90300: eval  CrossEntropyLoss |  0.36380988\n",
            "Step  90300: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  90400: Ran 100 train steps in 18.83 secs\n",
            "Step  90400: train CrossEntropyLoss |  0.45084018\n",
            "Step  90400: eval  CrossEntropyLoss |  0.38531056\n",
            "Step  90400: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step  90500: Ran 100 train steps in 18.75 secs\n",
            "Step  90500: train CrossEntropyLoss |  0.46106964\n",
            "Step  90500: eval  CrossEntropyLoss |  0.68363172\n",
            "Step  90500: eval          Accuracy |  0.62500000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOEL9K23JNda"
      },
      "source": [
        "# Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nRZS2KGJNda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2b073de-0a9c-47a3-86fe-ee647137dc5c"
      },
      "source": [
        "# Create a generator object\n",
        "tmp_train_generator = train_generator(16)\n",
        "\n",
        "# get one batch\n",
        "tmp_batch = next(tmp_train_generator)\n",
        "\n",
        "# Position 0 has the model inputs (tweets as tensors)\n",
        "# position 1 has the targets (the actual labels)\n",
        "tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n",
        "\n",
        "print(f\"The batch is a tuple of length {len(tmp_batch)} because position 0 contains the tweets, and position 1 contains the targets.\") \n",
        "print(f\"The shape of the tweet tensors is {tmp_inputs.shape} (num of examples, length of tweet tensors)\")\n",
        "print(f\"The shape of the labels is {tmp_targets.shape}, which is the batch size.\")\n",
        "print(f\"The shape of the example_weights is {tmp_example_weights.shape}, which is the same as inputs/targets size.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The batch is a tuple of length 3 because position 0 contains the tweets, and position 1 contains the targets.\n",
            "The shape of the tweet tensors is (16, 21) (num of examples, length of tweet tensors)\n",
            "The shape of the labels is (16,), which is the batch size.\n",
            "The shape of the example_weights is (16,), which is the same as inputs/targets size.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0OTWRttJNda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ecf5673-74bb-4fa1-f7e2-e7305884813d"
      },
      "source": [
        "# feed the tweet tensors into the model to get a prediction\n",
        "tmp_pred = training_loop.eval_model(tmp_inputs)\n",
        "print(f\"The prediction shape is {tmp_pred.shape}, num of tensor_tweets as rows\")\n",
        "print(\"Column 0 is the probability of a negative sentiment (class 0)\")\n",
        "print(\"Column 1 is the probability of a positive sentiment (class 1)\")\n",
        "print()\n",
        "print(\"View the prediction array\")\n",
        "tmp_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The prediction shape is (16, 2), num of tensor_tweets as rows\n",
            "Column 0 is the probability of a negative sentiment (class 0)\n",
            "Column 1 is the probability of a positive sentiment (class 1)\n",
            "\n",
            "View the prediction array\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[-2.2622967 , -0.10993886],\n",
              "             [-4.0666504 , -0.0172832 ],\n",
              "             [-1.890605  , -0.16367304],\n",
              "             [-2.0663567 , -0.13541478],\n",
              "             [-0.77469456, -0.6177513 ],\n",
              "             [-0.9887702 , -0.46526915],\n",
              "             [-4.560339  , -0.01051354],\n",
              "             [-0.8533397 , -0.55510825],\n",
              "             [-0.45352408, -1.0089135 ],\n",
              "             [-0.5496769 , -0.86070526],\n",
              "             [-0.74579984, -0.64312863],\n",
              "             [-0.11520672, -2.218078  ],\n",
              "             [-0.1414603 , -2.0256321 ],\n",
              "             [-0.5003785 , -0.9321689 ],\n",
              "             [-2.1311235 , -0.12636155],\n",
              "             [-0.04255414, -3.1781788 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcUaFF-sJNda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "460cb3d1-8cce-4f07-e226-6f12d5b48005"
      },
      "source": [
        "# turn probabilites into category predictions\n",
        "tmp_is_positive = tmp_pred[:,1] > tmp_pred[:,0]\n",
        "for i, p in enumerate(tmp_is_positive):\n",
        "    print(f\"Neg log prob {tmp_pred[i,0]:.4f}\\tPos log prob {tmp_pred[i,1]:.4f}\\t is positive? {p}\\t actual {tmp_targets[i]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neg log prob -2.2623\tPos log prob -0.1099\t is positive? True\t actual 1\n",
            "Neg log prob -4.0667\tPos log prob -0.0173\t is positive? True\t actual 1\n",
            "Neg log prob -1.8906\tPos log prob -0.1637\t is positive? True\t actual 1\n",
            "Neg log prob -2.0664\tPos log prob -0.1354\t is positive? True\t actual 1\n",
            "Neg log prob -0.7747\tPos log prob -0.6178\t is positive? True\t actual 1\n",
            "Neg log prob -0.9888\tPos log prob -0.4653\t is positive? True\t actual 1\n",
            "Neg log prob -4.5603\tPos log prob -0.0105\t is positive? True\t actual 1\n",
            "Neg log prob -0.8533\tPos log prob -0.5551\t is positive? True\t actual 1\n",
            "Neg log prob -0.4535\tPos log prob -1.0089\t is positive? False\t actual 0\n",
            "Neg log prob -0.5497\tPos log prob -0.8607\t is positive? False\t actual 0\n",
            "Neg log prob -0.7458\tPos log prob -0.6431\t is positive? True\t actual 0\n",
            "Neg log prob -0.1152\tPos log prob -2.2181\t is positive? False\t actual 0\n",
            "Neg log prob -0.1415\tPos log prob -2.0256\t is positive? False\t actual 0\n",
            "Neg log prob -0.5004\tPos log prob -0.9322\t is positive? False\t actual 0\n",
            "Neg log prob -2.1311\tPos log prob -0.1264\t is positive? True\t actual 0\n",
            "Neg log prob -0.0426\tPos log prob -3.1782\t is positive? False\t actual 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA5-gvudJNda"
      },
      "source": [
        "# Build Accuracy Function and Test on One Batch of Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBjEhHJCJNda"
      },
      "source": [
        "def compute_accuracy(preds, y, y_weights):\n",
        "    \"\"\"\n",
        "    Input: \n",
        "        preds: a tensor of shape (dim_batch, output_dim) \n",
        "        y: a tensor of shape (dim_batch, output_dim) with the true labels\n",
        "        y_weights: a n.ndarray with the a weight for each example\n",
        "    Output: \n",
        "        accuracy: a float between 0-1 \n",
        "        weighted_num_correct (np.float32): Sum of the weighted correct predictions\n",
        "        sum_weights (np.float32): Sum of the weights\n",
        "    \"\"\"\n",
        "\n",
        "    # Create an array of booleans, \n",
        "    # True if the probability of positive sentiment is greater than\n",
        "    # the probability of negative sentiment\n",
        "    # else False\n",
        "    is_pos =  preds[:,1] > preds[:,0]\n",
        "\n",
        "    # convert the array of booleans into an array of np.int32\n",
        "    is_pos_int = np.array(is_pos, dtype = np.int32)\n",
        "    \n",
        "    # compare the array of predictions (as int32) with the target (labels) of type int32\n",
        "    correct = is_pos_int == y\n",
        "\n",
        "    # Count the sum of the weights.\n",
        "    sum_weights = np.sum(y_weights)\n",
        "    \n",
        "    # convert the array of correct predictions (boolean) into an arrayof np.float32\n",
        "    correct_float = np.array(correct, dtype = np.float32)\n",
        "    \n",
        "    # Multiply each prediction with its corresponding weight.\n",
        "    weighted_correct_float = correct_float * y_weights\n",
        "\n",
        "    # Sum up the weighted correct predictions (of type np.float32), to go in the\n",
        "    # denominator.\n",
        "    weighted_num_correct = np.sum(weighted_correct_float)\n",
        " \n",
        "    # Divide the number of weighted correct predictions by the sum of the\n",
        "    # weights.\n",
        "    accuracy = sum_weights / weighted_num_correct \n",
        "\n",
        "\n",
        "    return accuracy, weighted_num_correct, sum_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxS-0abXJNda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df361025-fc5e-48aa-ff6b-f28cefee3e36"
      },
      "source": [
        "# test your function\n",
        "tmp_val_generator = val_generator(64)\n",
        "\n",
        "# get one batch\n",
        "tmp_batch = next(tmp_val_generator)\n",
        "\n",
        "# Position 0 has the model inputs (tweets as tensors)\n",
        "# position 1 has the targets (the actual labels)\n",
        "tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n",
        "\n",
        "# feed the tweet tensors into the model to get a prediction\n",
        "tmp_pred = training_loop.eval_model(tmp_inputs)\n",
        "\n",
        "tmp_acc, tmp_num_correct, tmp_num_predictions = compute_accuracy(preds=tmp_pred, y=tmp_targets, y_weights=tmp_example_weights)\n",
        "\n",
        "print(f\"Model's prediction accuracy on a single training batch is: {100 * tmp_acc}%\")\n",
        "print(f\"Weighted number of correct predictions {tmp_num_correct}; weighted number of total observations predicted {tmp_num_predictions}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model's prediction accuracy on a single training batch is: 120.75471496582031%\n",
            "Weighted number of correct predictions 53.0; weighted number of total observations predicted 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvYnafsMJNda"
      },
      "source": [
        "# Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNMwBEtTJNda"
      },
      "source": [
        "def test_model(generator, model):\n",
        "    '''\n",
        "    Input: \n",
        "        generator: an iterator instance that provides batches of inputs and targets\n",
        "        model: a model instance \n",
        "    Output: \n",
        "        accuracy: float corresponding to the accuracy\n",
        "    '''\n",
        "    \n",
        "    accuracy = 0.\n",
        "    total_num_correct = 0\n",
        "    total_num_pred = 0\n",
        "\n",
        "    for batch in generator: \n",
        "        \n",
        "        # Retrieve the inputs from the batch\n",
        "        inputs = batch[0]\n",
        "        \n",
        "        # Retrieve the targets (actual labels) from the batch\n",
        "        targets = batch[1]\n",
        "        \n",
        "        # Retrieve the example weight.\n",
        "        example_weight = batch[2]\n",
        "\n",
        "        # Make predictions using the inputs\n",
        "        pred = model(inputs)\n",
        "        \n",
        "        # Calculate accuracy for the batch by comparing its predictions and targets\n",
        "        batch_accuracy, batch_num_correct, batch_num_pred = compute_accuracy(preds=pred, y=targets, y_weights=example_weight)\n",
        "        \n",
        "        # Update the total number of correct predictions\n",
        "        # by adding the number of correct predictions from this batch\n",
        "        total_num_correct += batch_num_correct\n",
        "        \n",
        "        # Update the total number of predictions \n",
        "        # by adding the number of predictions made for the batch\n",
        "        total_num_pred += batch_num_pred\n",
        "\n",
        "    # Calculate accuracy over all examples\n",
        "    accuracy = total_num_correct / total_num_pred\n",
        "\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otkJDPGKJNda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42658945-7550-4cbc-cfa3-54aaaf3be613"
      },
      "source": [
        "# testing the accuracy of your model: this takes around 20 seconds\n",
        "model = training_loop.eval_model\n",
        "\n",
        "accuracy = test_model(test_generator(16), model)\n",
        "\n",
        "print(f'The accuracy of your model on the validation set is {accuracy:.4f}', )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of your model on the validation set is 0.7852\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muh07KQiJNda"
      },
      "source": [
        "# Getting Live Twitter Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTOolr3WJNda",
        "outputId": "c6b73744-93a5-4054-8bc5-4c349b7e811a"
      },
      "source": [
        "usa = 23424977  \n",
        "\n",
        "# grab top 50 trending topics for the USA\n",
        "trending_topics = api.trends_place(usa)                          \n",
        "\n",
        "topic_list = []\n",
        "\n",
        "# loop though each trending topic and create a list\n",
        "for k in range(len(trending_topics[0]['trends'])):                \n",
        "\n",
        "    topic = trending_topics[0]['trends'][k]['name']               \n",
        "    \n",
        "    topic_list.append(topic)\n",
        "    \n",
        "    print(topic)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Taysom Hill\n",
            "Geraldo\n",
            "Jameis\n",
            "Edward Norton\n",
            "Sean Payton\n",
            "#TransDayOfRemembrance\n",
            "#fridaymorning\n",
            "Mr. President\n",
            "Rick Scott\n",
            "#FreePcFriday\n",
            "Megan\n",
            "Ed Norton\n",
            "Autism Speaks\n",
            "#FridayThoughts\n",
            "#FridayVibes\n",
            "Trump Virus\n",
            "Meek\n",
            "Tucker\n",
            "Trumped\n",
            "Romney\n",
            "Good Friday\n",
            "\"Trump\"\n",
            "Al Capone\n",
            "Myles Garrett\n",
            "Gary Owen\n",
            "Mnuchin\n",
            "Bob Bauer\n",
            "Shots Fired\n",
            "Transgender Day of Remembrance\n",
            "Trump Vaccine\n",
            "Mr. Norton\n",
            "Dershowitz\n",
            "Tory\n",
            "Jamies\n",
            "Bridgeport\n",
            "Tina Snow\n",
            "Morgan Wallen\n",
            "Arenado\n",
            "SAINt JHN\n",
            "Jazmine Sullivan\n",
            "Finally Friday\n",
            "Rounders\n",
            "Today is Friday\n",
            "Sasse\n",
            "Dolph\n",
            "Chapter 12\n",
            "Benoit Mandelbrot\n",
            "Michigan and Minnesota\n",
            "Susan\n",
            "Pookie Loc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_dY-1WEJNda",
        "outputId": "2c74aa96-851e-4d7a-e42a-964c7400bc9f"
      },
      "source": [
        "program_start = time.time()            # time the program\n",
        "\n",
        "max_tweets = 50        # number of tweets to grab for each topic\n",
        "n = 50                 # top n number of retweets to keep\n",
        "\n",
        "#assert (max_tweets * len(df) * 50) <= 100000 , 'Request Limit Assert Error: Too many tweets! \\n \\n Twitter has a 24 hour limit of 100,000 requests \\n \\n Try changing \"max_tweets\"'    \n",
        "    \n",
        "columns = ['username', 'user_location', 'retweetcount', 'favorites', 'text',   # set up columns for dataframes\n",
        "           'hashtags', 'emojis', 'trending_topic', 'search_type']          \n",
        "\n",
        "recent_tweets_data = pd.DataFrame(columns = columns)                           # create empty dataframe    \n",
        "    \n",
        "\n",
        "for k in range(len(trending_topics[0]['trends'])):                # loop though each trending topic for each town\n",
        "    \n",
        "    run_start = time.time()                                       # start time of the run for each topic   \n",
        "\n",
        "    topic = trending_topics[0]['trends'][k]['name']               # the trending topic or hashtag (1 out of 50)  \n",
        "                                                        \n",
        "                                                                            # grab tweets with Cursor\n",
        "    tweets = tweepy.Cursor(api.search, q = topic,                           # search for each trending topic                                 \n",
        "                     lang=\"en\", result_type = 'recent' ,                    # most recent tweets in english\n",
        "                      tweet_mode = 'extended').items(max_tweets)            # longer tweets and grab max_tweets number of tweets\n",
        "    \n",
        "    tweet_list = [tweet for tweet in tweets]                                # create list of tweets\n",
        "                \n",
        "    recent_tweets_topic = pd.DataFrame(columns = columns)         # create dataframe to put in current top tweets for this town and trending topic\n",
        "        \n",
        "    for tweet in tweet_list:                                      # loop through each tweet that was grabbed\n",
        "        \n",
        "        username = tweet.user.screen_name              # store username\n",
        "        user_location = tweet.user.location            # store location of user\n",
        "        retweetcount = tweet.retweet_count             # store retweet count\n",
        "        favorites = tweet.favorite_count               # store favorite count\n",
        "        hashtags = tweet.entities['hashtags']          # store hashtags\n",
        "        search_type = 'recent'\n",
        "    \n",
        "        try:                              \n",
        "            text = tweet.retweeted_status.full_text    # store text if it's a retweet\n",
        "        \n",
        "        except AttributeError: \n",
        "            text = tweet.full_text                     # store text if it's a regular tweet\n",
        "            \n",
        "        emoji = list(emojis.get(text))                      # get the emojis\n",
        "        \n",
        "        curr_tweet = [username, user_location, retweetcount, favorites, text,            # store current tweet's data in a list soon to be a row\n",
        "                                hashtags, emoji, topic, search_type]                             \n",
        "        \n",
        "        recent_tweets_topic.loc[len(recent_tweets_topic)] = curr_tweet                   # add current tweet data to dataframe for town and topic\n",
        "                            \n",
        "    recent_tweets_topic.sort_values(by=['retweetcount', 'favorites'], inplace = True, ascending = False)        # sort the retweet values highest first\n",
        "                            \n",
        "    top_n = recent_tweets_topic[0:n]                                                     # keep only the top n\n",
        "                            \n",
        "    recent_tweets_data = pd.concat([recent_tweets_data, top_n], ignore_index = True, sort = False)              # concatenate top n to final dataframe\n",
        "    \n",
        "    run_end = time.time()                                                              # stop the time for the run\n",
        "    run_duration = round((run_end - run_start)/60, 2)                                  # calculate duration\n",
        "    print(f'Time for {topic} : {run_duration} mins')                                   # print duration       \n",
        "    \n",
        "\n",
        "program_end = time.time()                                                               # stop the program time\n",
        "program_duration = round(((program_end - program_start) / 60), 2)                       # calculate duration\n",
        "print(f'\\n \\n ************************************************** \\n \\n TOTAL TIME TO GRAB TWEETS : \\t {program_duration} mins')    # print duration as total time\n",
        "print('\\n **************************************************')    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time for Taysom Hill : 0.03 mins\n",
            "Time for Geraldo : 0.03 mins\n",
            "Time for Jameis : 0.03 mins\n",
            "Time for Edward Norton : 0.03 mins\n",
            "Time for Sean Payton : 0.03 mins\n",
            "Time for #TransDayOfRemembrance : 0.03 mins\n",
            "Time for #fridaymorning : 0.03 mins\n",
            "Time for Mr. President : 0.03 mins\n",
            "Time for Rick Scott : 0.03 mins\n",
            "Time for #FreePcFriday : 0.03 mins\n",
            "Time for Megan : 0.02 mins\n",
            "Time for Ed Norton : 0.03 mins\n",
            "Time for Autism Speaks : 0.03 mins\n",
            "Time for #FridayThoughts : 0.03 mins\n",
            "Time for #FridayVibes : 0.07 mins\n",
            "Time for Trump Virus : 0.03 mins\n",
            "Time for Meek : 0.03 mins\n",
            "Time for Tucker : 0.0 mins\n",
            "Time for Trumped : 0.03 mins\n",
            "Time for Romney : 0.03 mins\n",
            "Time for Good Friday : 0.03 mins\n",
            "Time for \"Trump\" : 0.03 mins\n",
            "Time for Al Capone : 0.03 mins\n",
            "Time for Myles Garrett : 0.03 mins\n",
            "Time for Gary Owen : 0.03 mins\n",
            "Time for Mnuchin : 0.03 mins\n",
            "Time for Bob Bauer : 0.03 mins\n",
            "Time for Shots Fired : 0.03 mins\n",
            "Time for Transgender Day of Remembrance : 0.03 mins\n",
            "Time for Trump Vaccine : 0.03 mins\n",
            "Time for Mr. Norton : 0.03 mins\n",
            "Time for Dershowitz : 0.03 mins\n",
            "Time for Tory : 0.03 mins\n",
            "Time for Jamies : 0.03 mins\n",
            "Time for Bridgeport : 0.03 mins\n",
            "Time for Tina Snow : 0.03 mins\n",
            "Time for Morgan Wallen : 0.03 mins\n",
            "Time for Arenado : 0.03 mins\n",
            "Time for SAINt JHN : 0.03 mins\n",
            "Time for Jazmine Sullivan : 0.03 mins\n",
            "Time for Finally Friday : 0.03 mins\n",
            "Time for Rounders : 0.03 mins\n",
            "Time for Today is Friday : 0.02 mins\n",
            "Time for Sasse : 0.03 mins\n",
            "Time for Dolph : 0.03 mins\n",
            "Time for Chapter 12 : 13.85 mins\n",
            "Time for Benoit Mandelbrot : 0.03 mins\n",
            "Time for Michigan and Minnesota : 0.03 mins\n",
            "Time for Susan : 0.03 mins\n",
            "Time for Pookie Loc : 0.03 mins\n",
            "\n",
            " \n",
            " ************************************************** \n",
            " \n",
            " TOTAL TIME TO GRAB TWEETS : \t 15.24 mins\n",
            "\n",
            " **************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eqHuiZXJNda",
        "outputId": "4af442aa-bd39-4264-9cbc-96b1fa0d7d3c"
      },
      "source": [
        "recent_tweets_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>user_location</th>\n",
              "      <th>retweetcount</th>\n",
              "      <th>favorites</th>\n",
              "      <th>text</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>emojis</th>\n",
              "      <th>trending_topic</th>\n",
              "      <th>search_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>JoeyMorris122</td>\n",
              "      <td></td>\n",
              "      <td>3256</td>\n",
              "      <td>0</td>\n",
              "      <td>When Taysom Hill finally becomes QB1 https://t...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KDot_Flow</td>\n",
              "      <td></td>\n",
              "      <td>924</td>\n",
              "      <td>0</td>\n",
              "      <td>Saints’ QB Taysom Hill will start Sunday vs. t...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BRGIANTSKNICKS4</td>\n",
              "      <td></td>\n",
              "      <td>924</td>\n",
              "      <td>0</td>\n",
              "      <td>Saints’ QB Taysom Hill will start Sunday vs. t...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LC_w0ng</td>\n",
              "      <td></td>\n",
              "      <td>924</td>\n",
              "      <td>0</td>\n",
              "      <td>Saints’ QB Taysom Hill will start Sunday vs. t...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CraigWLearn</td>\n",
              "      <td>Buffalo, NY</td>\n",
              "      <td>924</td>\n",
              "      <td>0</td>\n",
              "      <td>Saints’ QB Taysom Hill will start Sunday vs. t...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          username user_location retweetcount favorites  \\\n",
              "0    JoeyMorris122                       3256         0   \n",
              "1        KDot_Flow                        924         0   \n",
              "2  BRGIANTSKNICKS4                        924         0   \n",
              "3          LC_w0ng                        924         0   \n",
              "4      CraigWLearn   Buffalo, NY          924         0   \n",
              "\n",
              "                                                text hashtags emojis  \\\n",
              "0  When Taysom Hill finally becomes QB1 https://t...       []     []   \n",
              "1  Saints’ QB Taysom Hill will start Sunday vs. t...       []     []   \n",
              "2  Saints’ QB Taysom Hill will start Sunday vs. t...       []     []   \n",
              "3  Saints’ QB Taysom Hill will start Sunday vs. t...       []     []   \n",
              "4  Saints’ QB Taysom Hill will start Sunday vs. t...       []     []   \n",
              "\n",
              "  trending_topic search_type  \n",
              "0    Taysom Hill      recent  \n",
              "1    Taysom Hill      recent  \n",
              "2    Taysom Hill      recent  \n",
              "3    Taysom Hill      recent  \n",
              "4    Taysom Hill      recent  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 930
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfajWqsqJNda",
        "outputId": "a4c067c9-24ad-4cf9-f431-79045480f6ba"
      },
      "source": [
        "len(recent_tweets_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2450"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 931
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dCravYmJNda",
        "outputId": "15052fa2-f626-4ed8-a58e-f7d60bc6c24b"
      },
      "source": [
        "program_start = time.time()            # time the program\n",
        "\n",
        "max_tweets = 100        # number of tweets to grab for each topic\n",
        "n = 100                 # top n number of retweets to keep\n",
        "\n",
        "#assert (max_tweets * len(df) * 50) <= 100000 , 'Request Limit Assert Error: Too many tweets! \\n \\n Twitter has a 24 hour limit of 100,000 requests \\n \\n Try changing \"max_tweets\"'\n",
        "\n",
        "\n",
        "columns = ['username', 'user_location', 'retweetcount', 'favorites', 'text',   # set up columns for dataframes\n",
        "           'hashtags', 'emojis', 'trending_topic', 'search_type']          \n",
        "\n",
        "pop_tweets_data = pd.DataFrame(columns = columns)                 # create empty dataframe\n",
        "    \n",
        "\n",
        "for q in range(len(trending_topics[0]['trends'])):                # loop though each trending topic for each town\n",
        "\n",
        "    topic = trending_topics[0]['trends'][q]['name']               # the trending topic or hashtag (1 out of 50)  \n",
        "                                                                           \n",
        "                                                                            # grab tweets with Cursor\n",
        "    tweets = tweepy.Cursor(api.search, q = topic,                           # search for each trending topic                                 \n",
        "                     lang=\"en\", result_type = 'popular' ,                   # most recent tweets in english\n",
        "                      tweet_mode = 'extended').items(max_tweets)            # longer tweets and grab max_tweets number of tweets\n",
        "    \n",
        "    tweet_list = [tweet for tweet in tweets]                                # create list of tweets\n",
        "                \n",
        "    pop_tweets_topic = pd.DataFrame(columns = columns)         # create dataframe to put in current top tweets for this town and trending topic\n",
        "        \n",
        "    for tweet in tweet_list:                                   # loop through each tweet that was grabbed\n",
        "        \n",
        "        username = tweet.user.screen_name              # store username\n",
        "        user_location = tweet.user.location            # store location of user\n",
        "        retweetcount = tweet.retweet_count             # store retweet count\n",
        "        favorites = tweet.favorite_count               # store favorite count\n",
        "        hashtags = tweet.entities['hashtags']          # store hashtags\n",
        "        search_type = 'popular'\n",
        "        \n",
        "        try:                              \n",
        "            text = tweet.retweeted_status.full_text    # store text if it's a retweet\n",
        "        \n",
        "        except AttributeError: \n",
        "            text = tweet.full_text                     # store text if it's a regular tweet\n",
        "            \n",
        "        emoji = list(emojis.get(text))                      # get the emojis    \n",
        "        \n",
        "        curr_tweet = [username, user_location, retweetcount, favorites, text,      # store current tweet's data in a list soon to be a row\n",
        "                                hashtags, emoji, topic, search_type]                             \n",
        "        \n",
        "        pop_tweets_topic.loc[len(pop_tweets_topic)] = curr_tweet                   # add current tweet data to dataframe for town and topic\n",
        "                            \n",
        "    pop_tweets_topic.sort_values(by=['retweetcount', 'favorites'], inplace = True, ascending = False)        # sort the retweet values highest first\n",
        "                            \n",
        "    top_n = pop_tweets_topic[0:n]                                                                  # keep only the top n\n",
        "                            \n",
        "    pop_tweets_data = pd.concat([pop_tweets_data, top_n], ignore_index = True, sort = False)       # concatenate top n to final dataframe\n",
        "            \n",
        "    \n",
        "program_end = time.time()                                                               # stop the program time\n",
        "program_duration = round(((program_end - program_start) / 60), 2)                       # calculate duration\n",
        "print(f'\\n \\n ************************************************** \\n \\n TOTAL TIME TO GRAB TWEETS : \\t {program_duration} mins')    # print duration as total time\n",
        "print('\\n **************************************************')    \n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " \n",
            " ************************************************** \n",
            " \n",
            " TOTAL TIME TO GRAB TWEETS : \t 0.73 mins\n",
            "\n",
            " **************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REsLG11PJNda",
        "outputId": "d71682bf-6de4-43da-ffa1-6f294992f638"
      },
      "source": [
        "pop_tweets_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>user_location</th>\n",
              "      <th>retweetcount</th>\n",
              "      <th>favorites</th>\n",
              "      <th>text</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>emojis</th>\n",
              "      <th>trending_topic</th>\n",
              "      <th>search_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdamSchefter</td>\n",
              "      <td>New York</td>\n",
              "      <td>1009</td>\n",
              "      <td>8346</td>\n",
              "      <td>Saints’ QB Taysom Hill will start Sunday vs. t...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>popular</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SportsCenter</td>\n",
              "      <td></td>\n",
              "      <td>951</td>\n",
              "      <td>12221</td>\n",
              "      <td>Taysom Hill will start Sunday for the Saints a...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>popular</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AdamSchefter</td>\n",
              "      <td>New York</td>\n",
              "      <td>669</td>\n",
              "      <td>4902</td>\n",
              "      <td>Jameis Winston will not be part of any offensi...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>popular</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RapSheet</td>\n",
              "      <td>New York</td>\n",
              "      <td>524</td>\n",
              "      <td>4456</td>\n",
              "      <td>Intrigue in New Orleans: The #Saints have give...</td>\n",
              "      <td>[{'text': 'Saints', 'indices': [29, 36]}]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>popular</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>FieldYates</td>\n",
              "      <td>Boston, MA</td>\n",
              "      <td>158</td>\n",
              "      <td>1925</td>\n",
              "      <td>A note: Taysom Hill is eligible to be utilized...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>popular</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       username user_location retweetcount favorites  \\\n",
              "0  AdamSchefter      New York         1009      8346   \n",
              "1  SportsCenter                        951     12221   \n",
              "2  AdamSchefter      New York          669      4902   \n",
              "3      RapSheet      New York          524      4456   \n",
              "4    FieldYates    Boston, MA          158      1925   \n",
              "\n",
              "                                                text  \\\n",
              "0  Saints’ QB Taysom Hill will start Sunday vs. t...   \n",
              "1  Taysom Hill will start Sunday for the Saints a...   \n",
              "2  Jameis Winston will not be part of any offensi...   \n",
              "3  Intrigue in New Orleans: The #Saints have give...   \n",
              "4  A note: Taysom Hill is eligible to be utilized...   \n",
              "\n",
              "                                    hashtags emojis trending_topic search_type  \n",
              "0                                         []     []    Taysom Hill     popular  \n",
              "1                                         []     []    Taysom Hill     popular  \n",
              "2                                         []     []    Taysom Hill     popular  \n",
              "3  [{'text': 'Saints', 'indices': [29, 36]}]     []    Taysom Hill     popular  \n",
              "4                                         []     []    Taysom Hill     popular  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 933
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLf4YPnSJNdb",
        "outputId": "76ab7bfa-eff2-498f-bc2f-f15f55f5d4bb"
      },
      "source": [
        "len(pop_tweets_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1083"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 934
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2mWuw6KJNdb",
        "outputId": "096aafe9-0020-4ec7-e9ae-16668567cab0"
      },
      "source": [
        "pop_tweets_data['trending_topic'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Trump\"                           89\n",
              "Tory                              75\n",
              "Trump Vaccine                     52\n",
              "Mr. President                     42\n",
              "Trump Virus                       40\n",
              "Susan                             39\n",
              "Mnuchin                           39\n",
              "Romney                            36\n",
              "Tucker                            35\n",
              "Megan                             33\n",
              "#TransDayOfRemembrance            31\n",
              "Today is Friday                   30\n",
              "Sasse                             30\n",
              "Jameis                            30\n",
              "Transgender Day of Remembrance    30\n",
              "Shots Fired                       29\n",
              "Good Friday                       29\n",
              "Geraldo                           29\n",
              "Myles Garrett                     29\n",
              "Taysom Hill                       29\n",
              "Meek                              26\n",
              "Rounders                          26\n",
              "Rick Scott                        25\n",
              "#FridayThoughts                   24\n",
              "Sean Payton                       20\n",
              "Bob Bauer                         19\n",
              "Finally Friday                    15\n",
              "Trumped                           14\n",
              "Michigan and Minnesota            14\n",
              "#FridayVibes                      14\n",
              "Dershowitz                        12\n",
              "Chapter 12                        11\n",
              "#fridaymorning                    11\n",
              "Arenado                           10\n",
              "Edward Norton                     10\n",
              "SAINt JHN                         10\n",
              "Autism Speaks                      6\n",
              "Pookie Loc                         6\n",
              "Dolph                              5\n",
              "Benoit Mandelbrot                  5\n",
              "Jazmine Sullivan                   5\n",
              "Bridgeport                         5\n",
              "Ed Norton                          4\n",
              "Tina Snow                          4\n",
              "Al Capone                          2\n",
              "Morgan Wallen                      2\n",
              "Mr. Norton                         1\n",
              "Jamies                             1\n",
              "Name: trending_topic, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 935
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_gHWCuUJNdb",
        "outputId": "2175dc9b-3784-4e06-8eb4-482bcad6aeb5"
      },
      "source": [
        "tweets_data = pd.concat([recent_tweets_data, pop_tweets_data], ignore_index=True)\n",
        "\n",
        "tweets_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>user_location</th>\n",
              "      <th>retweetcount</th>\n",
              "      <th>favorites</th>\n",
              "      <th>text</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>emojis</th>\n",
              "      <th>trending_topic</th>\n",
              "      <th>search_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>JoeyMorris122</td>\n",
              "      <td></td>\n",
              "      <td>3256</td>\n",
              "      <td>0</td>\n",
              "      <td>When Taysom Hill finally becomes QB1 https://t...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KDot_Flow</td>\n",
              "      <td></td>\n",
              "      <td>924</td>\n",
              "      <td>0</td>\n",
              "      <td>Saints’ QB Taysom Hill will start Sunday vs. t...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BRGIANTSKNICKS4</td>\n",
              "      <td></td>\n",
              "      <td>924</td>\n",
              "      <td>0</td>\n",
              "      <td>Saints’ QB Taysom Hill will start Sunday vs. t...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LC_w0ng</td>\n",
              "      <td></td>\n",
              "      <td>924</td>\n",
              "      <td>0</td>\n",
              "      <td>Saints’ QB Taysom Hill will start Sunday vs. t...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CraigWLearn</td>\n",
              "      <td>Buffalo, NY</td>\n",
              "      <td>924</td>\n",
              "      <td>0</td>\n",
              "      <td>Saints’ QB Taysom Hill will start Sunday vs. t...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          username user_location retweetcount favorites  \\\n",
              "0    JoeyMorris122                       3256         0   \n",
              "1        KDot_Flow                        924         0   \n",
              "2  BRGIANTSKNICKS4                        924         0   \n",
              "3          LC_w0ng                        924         0   \n",
              "4      CraigWLearn   Buffalo, NY          924         0   \n",
              "\n",
              "                                                text hashtags emojis  \\\n",
              "0  When Taysom Hill finally becomes QB1 https://t...       []     []   \n",
              "1  Saints’ QB Taysom Hill will start Sunday vs. t...       []     []   \n",
              "2  Saints’ QB Taysom Hill will start Sunday vs. t...       []     []   \n",
              "3  Saints’ QB Taysom Hill will start Sunday vs. t...       []     []   \n",
              "4  Saints’ QB Taysom Hill will start Sunday vs. t...       []     []   \n",
              "\n",
              "  trending_topic search_type  \n",
              "0    Taysom Hill      recent  \n",
              "1    Taysom Hill      recent  \n",
              "2    Taysom Hill      recent  \n",
              "3    Taysom Hill      recent  \n",
              "4    Taysom Hill      recent  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 936
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kyu9_M4DJNdb",
        "outputId": "da204583-8381-49a0-941e-2e7857625add"
      },
      "source": [
        "tweets_data.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>user_location</th>\n",
              "      <th>retweetcount</th>\n",
              "      <th>favorites</th>\n",
              "      <th>text</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>emojis</th>\n",
              "      <th>trending_topic</th>\n",
              "      <th>search_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3528</th>\n",
              "      <td>yungmal__15</td>\n",
              "      <td></td>\n",
              "      <td>48</td>\n",
              "      <td>194</td>\n",
              "      <td>I’m Smoking on pookie loc tonighttt 😭😭😂😂 https...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[😭, 😂]</td>\n",
              "      <td>Pookie Loc</td>\n",
              "      <td>popular</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3529</th>\n",
              "      <td>CHXPO</td>\n",
              "      <td>Bleveland 💸 LA</td>\n",
              "      <td>30</td>\n",
              "      <td>114</td>\n",
              "      <td>Smoking on Pookie Loc Tonight !</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Pookie Loc</td>\n",
              "      <td>popular</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3530</th>\n",
              "      <td>OGJOHNNY5</td>\n",
              "      <td>stunna island</td>\n",
              "      <td>12</td>\n",
              "      <td>13</td>\n",
              "      <td>“SMOKIN ON POOKIE LOC TONIGHT\"</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Pookie Loc</td>\n",
              "      <td>popular</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3531</th>\n",
              "      <td>WhoIsClayJames</td>\n",
              "      <td></td>\n",
              "      <td>7</td>\n",
              "      <td>13</td>\n",
              "      <td>Man Gucci said we smoking that Pookie Loc pack...</td>\n",
              "      <td>[{'text': 'VERZUZ', 'indices': [108, 115]}]</td>\n",
              "      <td>[😓]</td>\n",
              "      <td>Pookie Loc</td>\n",
              "      <td>popular</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3532</th>\n",
              "      <td>jkboxing</td>\n",
              "      <td>London, United Kingdom</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>When Gucci Mane said “We smoking on pookie loc...</td>\n",
              "      <td>[{'text': 'VERZUZ', 'indices': [85, 92]}]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Pookie Loc</td>\n",
              "      <td>popular</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            username            user_location retweetcount favorites  \\\n",
              "3528     yungmal__15                                    48       194   \n",
              "3529           CHXPO           Bleveland 💸 LA           30       114   \n",
              "3530       OGJOHNNY5            stunna island           12        13   \n",
              "3531  WhoIsClayJames                                     7        13   \n",
              "3532        jkboxing  London, United Kingdom             5         6   \n",
              "\n",
              "                                                   text  \\\n",
              "3528  I’m Smoking on pookie loc tonighttt 😭😭😂😂 https...   \n",
              "3529                    Smoking on Pookie Loc Tonight !   \n",
              "3530                     “SMOKIN ON POOKIE LOC TONIGHT\"   \n",
              "3531  Man Gucci said we smoking that Pookie Loc pack...   \n",
              "3532  When Gucci Mane said “We smoking on pookie loc...   \n",
              "\n",
              "                                         hashtags  emojis trending_topic  \\\n",
              "3528                                           []  [😭, 😂]     Pookie Loc   \n",
              "3529                                           []      []     Pookie Loc   \n",
              "3530                                           []      []     Pookie Loc   \n",
              "3531  [{'text': 'VERZUZ', 'indices': [108, 115]}]     [😓]     Pookie Loc   \n",
              "3532    [{'text': 'VERZUZ', 'indices': [85, 92]}]      []     Pookie Loc   \n",
              "\n",
              "     search_type  \n",
              "3528     popular  \n",
              "3529     popular  \n",
              "3530     popular  \n",
              "3531     popular  \n",
              "3532     popular  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 937
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSGWb6RWJNdb",
        "outputId": "5ebd00b4-0dcd-45cb-eaf5-faefa8847c67"
      },
      "source": [
        "len(tweets_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3533"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 938
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkvLFe71JNdb"
      },
      "source": [
        "# Use Models on Live Twitter Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDeRRxhQJNdb"
      },
      "source": [
        "# this is used to predict on your own sentnece\n",
        "def predict(sentence):\n",
        "    inputs = np.array(tweet_to_tensor(sentence, vocab_dict=Vocab))\n",
        "    \n",
        "    # Batch size 1, add dimension for batch, to work with the model\n",
        "    inputs = inputs[None, :]  \n",
        "    \n",
        "    # predict with the model\n",
        "    preds_probs = model(inputs)\n",
        "    \n",
        "    # Turn probabilities into categories\n",
        "    preds = int(preds_probs[0, 1] > preds_probs[0, 0])\n",
        "    \n",
        "    sentiment = \"negative\"\n",
        "    if preds == 1:\n",
        "        sentiment = 'positive'\n",
        "\n",
        "    return sentiment, round(float(preds_probs[0, 0]),4), round(float(preds_probs[0, 1]),4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-VVim9kJNdb"
      },
      "source": [
        "\n",
        "\n",
        "for i in range(len(tweets_data)):\n",
        "\n",
        "    tweets_data.loc[i, 'sentiment'], tweets_data.loc[i, 'neg_prob'], tweets_data.loc[i, 'pos_prob'] = predict(tweets_data['text'].iloc[i])\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTflbsbfJNdb",
        "outputId": "08124f0a-655b-468a-9cc1-1894b3dd390f"
      },
      "source": [
        "tweets_data.head(50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>user_location</th>\n",
              "      <th>retweetcount</th>\n",
              "      <th>favorites</th>\n",
              "      <th>text</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>emojis</th>\n",
              "      <th>trending_topic</th>\n",
              "      <th>search_type</th>\n",
              "      <th>preds</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>neg_prob</th>\n",
              "      <th>pos_prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>JoeyMorris122</td>\n",
              "      <td></td>\n",
              "      <td>3256</td>\n",
              "      <td>0</td>\n",
              "      <td>When Taysom Hill finally becomes QB1 https://t...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.4197</td>\n",
              "      <td>-1.0708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KDot_Flow</td>\n",
              "      <td></td>\n",
              "      <td>924</td>\n",
              "      <td>0</td>\n",
              "      <td>Saints’ QB Taysom Hill will start Sunday vs. t...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-0.8397</td>\n",
              "      <td>-0.5653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BRGIANTSKNICKS4</td>\n",
              "      <td></td>\n",
              "      <td>924</td>\n",
              "      <td>0</td>\n",
              "      <td>Saints’ QB Taysom Hill will start Sunday vs. t...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-0.8397</td>\n",
              "      <td>-0.5653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LC_w0ng</td>\n",
              "      <td></td>\n",
              "      <td>924</td>\n",
              "      <td>0</td>\n",
              "      <td>Saints’ QB Taysom Hill will start Sunday vs. t...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-0.8397</td>\n",
              "      <td>-0.5653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CraigWLearn</td>\n",
              "      <td>Buffalo, NY</td>\n",
              "      <td>924</td>\n",
              "      <td>0</td>\n",
              "      <td>Saints’ QB Taysom Hill will start Sunday vs. t...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-0.8397</td>\n",
              "      <td>-0.5653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NewNewFantasyFB</td>\n",
              "      <td>Kansas City, MO</td>\n",
              "      <td>924</td>\n",
              "      <td>0</td>\n",
              "      <td>Saints’ QB Taysom Hill will start Sunday vs. t...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-0.8397</td>\n",
              "      <td>-0.5653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>JawshBurton</td>\n",
              "      <td>Downtown Canada, Canada</td>\n",
              "      <td>770</td>\n",
              "      <td>0</td>\n",
              "      <td>Taysom Hill will start Sunday for the Saints a...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.6436</td>\n",
              "      <td>-0.7453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>iPulledDaTrigga</td>\n",
              "      <td>Ft. Lauderdale, FL  ☀️</td>\n",
              "      <td>770</td>\n",
              "      <td>0</td>\n",
              "      <td>Taysom Hill will start Sunday for the Saints a...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.6436</td>\n",
              "      <td>-0.7453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Jmaddy_2zero</td>\n",
              "      <td>Flint,MI</td>\n",
              "      <td>770</td>\n",
              "      <td>0</td>\n",
              "      <td>Taysom Hill will start Sunday for the Saints a...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.6436</td>\n",
              "      <td>-0.7453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>mccooley15</td>\n",
              "      <td></td>\n",
              "      <td>770</td>\n",
              "      <td>0</td>\n",
              "      <td>Taysom Hill will start Sunday for the Saints a...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.6436</td>\n",
              "      <td>-0.7453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>A_Shah86</td>\n",
              "      <td>Easton, PA</td>\n",
              "      <td>593</td>\n",
              "      <td>0</td>\n",
              "      <td>Jameis Winston will not be part of any offensi...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-1.0101</td>\n",
              "      <td>-0.4528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>BRGIANTSKNICKS4</td>\n",
              "      <td></td>\n",
              "      <td>593</td>\n",
              "      <td>0</td>\n",
              "      <td>Jameis Winston will not be part of any offensi...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-1.0101</td>\n",
              "      <td>-0.4528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>BRGIANTSKNICKS4</td>\n",
              "      <td></td>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "      <td>The #Saints gave a 2-year, $21M contract this ...</td>\n",
              "      <td>[{'text': 'Saints', 'indices': [18, 25]}]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-0.8325</td>\n",
              "      <td>-0.5708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>BRGIANTSKNICKS4</td>\n",
              "      <td></td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "      <td>Taysom Hill has only 18 career regular season ...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.4836</td>\n",
              "      <td>-0.9585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>BRGIANTSKNICKS4</td>\n",
              "      <td></td>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "      <td>This is a good explanation for why the #Saints...</td>\n",
              "      <td>[{'text': 'Saints', 'indices': [53, 60]}]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-0.8596</td>\n",
              "      <td>-0.5505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Joopis5</td>\n",
              "      <td></td>\n",
              "      <td>33</td>\n",
              "      <td>0</td>\n",
              "      <td>Fantasy managers running to the waiver wire fo...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.4385</td>\n",
              "      <td>-1.0357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>FNR_TIGG</td>\n",
              "      <td>The Ozark Highlands</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>Taysom Hill has four more pass attempts in his...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.1172</td>\n",
              "      <td>-2.2021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>LockedOnCougars</td>\n",
              "      <td>Provo, UT</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>Taysom Hill’s first start is football twitter’...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-1.4857</td>\n",
              "      <td>-0.2566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>jamaloh_james</td>\n",
              "      <td>most likely my bed</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>Taysom Hill’s first start is football twitter’...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-1.4857</td>\n",
              "      <td>-0.2566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>SterlSilva</td>\n",
              "      <td>Richmond, VA</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>Taysom Hill starting on Sunday?!?! https://t.c...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.5733</td>\n",
              "      <td>-0.8293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>MariouxVee</td>\n",
              "      <td></td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>Sean Payton: Make them all believe Jameis Wins...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-0.7969</td>\n",
              "      <td>-0.5992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Rumboyznet</td>\n",
              "      <td>Colorado Springs, CO</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Is it fair to have Taysom Hill in as your TE t...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-1.6807</td>\n",
              "      <td>-0.2061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Philip_Velez_Jr</td>\n",
              "      <td>Pembroke Pines, FL</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Saints QB Taysom Hill's stats from the 2019 pr...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.6132</td>\n",
              "      <td>-0.7801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>wdsu</td>\n",
              "      <td>New Orleans, LA</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>I’ve said all week I thought Taysom Hill would...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-1.1506</td>\n",
              "      <td>-0.3804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>AccidentalFrea1</td>\n",
              "      <td>Your Moms room / So Cal</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Shocker: Taysom Hill is not only starting over...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.5864</td>\n",
              "      <td>-0.8127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>RushedMyLuck</td>\n",
              "      <td>(South) Scottsdale, AZ</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Saints fans after hearing that Taysom Hill is ...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-0.7249</td>\n",
              "      <td>-0.6624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>CodeNameFinesse</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>It’s bullshit Taysom Hill can be put as TE whe...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.4290</td>\n",
              "      <td>-1.0531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>4for4_Josh</td>\n",
              "      <td>Madison, WI</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>How many Taysom Hill snaps until we’re screami...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-0.7261</td>\n",
              "      <td>-0.6612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>_levviii_</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>i mean, at least i can finally watch Taysom Hi...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.1587</td>\n",
              "      <td>-1.9188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>MSizzle83</td>\n",
              "      <td>Malden-Boston</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>@FieldYates taysom hill or Matt stafford?</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-0.7122</td>\n",
              "      <td>-0.6744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>jonobrien614</td>\n",
              "      <td>Columbus, OH</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Taysom Hill starting over Winston is racist. I...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.3843</td>\n",
              "      <td>-1.1423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>vinceb_77</td>\n",
              "      <td>Scottsdale, AZ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Wanted to watch Jameis play not taysom hill</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0511</td>\n",
              "      <td>-3.0003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>NotDuckHodges</td>\n",
              "      <td>Mr. Worldwide</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Go ahead and play Taysom Hill. I dare you.</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.1967</td>\n",
              "      <td>-1.7228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>88Snizzy88</td>\n",
              "      <td>New Jersey</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>On record, Alex “The Snake” Taroff has signed ...</td>\n",
              "      <td>[{'text': 'sad', 'indices': [171, 175]}]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-0.9667</td>\n",
              "      <td>-0.4786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Boredy_Mcbored</td>\n",
              "      <td>In my head</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>I don't even like Jameis Winston as a QB but T...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.5436</td>\n",
              "      <td>-0.8690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>ChaseSMusic</td>\n",
              "      <td>Los Angeles, CA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>@bigtime_tim @bball798 @MatthewBerryTMR @ESPNF...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.5522</td>\n",
              "      <td>-0.8573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>peteroverzet</td>\n",
              "      <td>Boston, MA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>what's funny is most of these guys will try to...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.6346</td>\n",
              "      <td>-0.7554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>BrendenErtle</td>\n",
              "      <td>ertlebrenden9@gmail.com</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Taysom Hill QB 1</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.2792</td>\n",
              "      <td>-1.4123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>BlownoffRichard</td>\n",
              "      <td>203 - 914 - 240</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Starting Taysom Hill at tight end https://t.co...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.1223</td>\n",
              "      <td>-2.1621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>LittleHooty</td>\n",
              "      <td>Pennsylvania, USA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Taysom Hill sucks and I can’t stand the Saints...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.3877</td>\n",
              "      <td>-1.1351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>jayj______</td>\n",
              "      <td>Atlanta, GA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>I cannot believe Taysom Hill is about to be a ...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.1790</td>\n",
              "      <td>-1.8088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>Collin_Keane</td>\n",
              "      <td>ECSU '18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Putting Taysom Hill in my FLEX https://t.co/Wy...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.2531</td>\n",
              "      <td>-1.4980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>fantasyfballin5</td>\n",
              "      <td>Pennsylvania, USA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>make Taysom Hill a QB/TE you cowards @YahooFan...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-1.2332</td>\n",
              "      <td>-0.3444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>SB__Dana</td>\n",
              "      <td>Baltimore, MD</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Max Kellerman just compared Taysom Hill to Ode...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.6052</td>\n",
              "      <td>-0.7895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>wtfBRIZ</td>\n",
              "      <td>093 Flipside Chicago.</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Taysom Hill is your starter 😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[😂]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-0.9291</td>\n",
              "      <td>-0.5024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>MSizzle83</td>\n",
              "      <td>Malden-Boston</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>@daverichard taysom hill or Matt stafford #fft</td>\n",
              "      <td>[{'text': 'fft', 'indices': [42, 46]}]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-0.7186</td>\n",
              "      <td>-0.6683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>kkjeff</td>\n",
              "      <td>Houma, La</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Saints expected to start Taysom Hill vs. Falcons</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-1.5086</td>\n",
              "      <td>-0.2500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>RichardJanvrin</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Jameis Winston - Sit\\nTaysom Hill at QB - Sit\\...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.4708</td>\n",
              "      <td>-0.9794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>jrmaher01</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>@heykayadams what do you think about taysom hi...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-1.4352</td>\n",
              "      <td>-0.2719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>bodakorange22</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>I genuinely hope Taysom hill starts and the pl...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.4106</td>\n",
              "      <td>-1.0883</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           username            user_location retweetcount favorites  \\\n",
              "0     JoeyMorris122                                  3256         0   \n",
              "1         KDot_Flow                                   924         0   \n",
              "2   BRGIANTSKNICKS4                                   924         0   \n",
              "3           LC_w0ng                                   924         0   \n",
              "4       CraigWLearn              Buffalo, NY          924         0   \n",
              "5   NewNewFantasyFB          Kansas City, MO          924         0   \n",
              "6       JawshBurton  Downtown Canada, Canada          770         0   \n",
              "7   iPulledDaTrigga   Ft. Lauderdale, FL  ☀️          770         0   \n",
              "8      Jmaddy_2zero                 Flint,MI          770         0   \n",
              "9        mccooley15                                   770         0   \n",
              "10         A_Shah86               Easton, PA          593         0   \n",
              "11  BRGIANTSKNICKS4                                   593         0   \n",
              "12  BRGIANTSKNICKS4                                    95         0   \n",
              "13  BRGIANTSKNICKS4                                    42         0   \n",
              "14  BRGIANTSKNICKS4                                    34         0   \n",
              "15          Joopis5                                    33         0   \n",
              "16         FNR_TIGG     The Ozark Highlands            16         0   \n",
              "17  LockedOnCougars                Provo, UT           15         0   \n",
              "18    jamaloh_james       most likely my bed           15         0   \n",
              "19       SterlSilva             Richmond, VA           10         0   \n",
              "20       MariouxVee                                     8         0   \n",
              "21       Rumboyznet     Colorado Springs, CO            2         0   \n",
              "22  Philip_Velez_Jr       Pembroke Pines, FL            1         0   \n",
              "23             wdsu          New Orleans, LA            1         0   \n",
              "24  AccidentalFrea1  Your Moms room / So Cal            1         0   \n",
              "25     RushedMyLuck   (South) Scottsdale, AZ            1         0   \n",
              "26  CodeNameFinesse                                     1         0   \n",
              "27       4for4_Josh              Madison, WI            0         1   \n",
              "28        _levviii_                                     0         0   \n",
              "29        MSizzle83            Malden-Boston            0         0   \n",
              "30     jonobrien614             Columbus, OH            0         0   \n",
              "31        vinceb_77           Scottsdale, AZ            0         0   \n",
              "32    NotDuckHodges            Mr. Worldwide            0         0   \n",
              "33       88Snizzy88              New Jersey             0         0   \n",
              "34   Boredy_Mcbored               In my head            0         0   \n",
              "35      ChaseSMusic          Los Angeles, CA            0         0   \n",
              "36     peteroverzet               Boston, MA            0         0   \n",
              "37     BrendenErtle  ertlebrenden9@gmail.com            0         0   \n",
              "38  BlownoffRichard          203 - 914 - 240            0         0   \n",
              "39      LittleHooty        Pennsylvania, USA            0         0   \n",
              "40       jayj______              Atlanta, GA            0         0   \n",
              "41     Collin_Keane                 ECSU '18            0         0   \n",
              "42  fantasyfballin5        Pennsylvania, USA            0         0   \n",
              "43         SB__Dana            Baltimore, MD            0         0   \n",
              "44          wtfBRIZ   093 Flipside Chicago.             0         0   \n",
              "45        MSizzle83            Malden-Boston            0         0   \n",
              "46           kkjeff                Houma, La            0         0   \n",
              "47   RichardJanvrin                                     0         0   \n",
              "48        jrmaher01                                     0         0   \n",
              "49    bodakorange22                                     0         0   \n",
              "\n",
              "                                                 text  \\\n",
              "0   When Taysom Hill finally becomes QB1 https://t...   \n",
              "1   Saints’ QB Taysom Hill will start Sunday vs. t...   \n",
              "2   Saints’ QB Taysom Hill will start Sunday vs. t...   \n",
              "3   Saints’ QB Taysom Hill will start Sunday vs. t...   \n",
              "4   Saints’ QB Taysom Hill will start Sunday vs. t...   \n",
              "5   Saints’ QB Taysom Hill will start Sunday vs. t...   \n",
              "6   Taysom Hill will start Sunday for the Saints a...   \n",
              "7   Taysom Hill will start Sunday for the Saints a...   \n",
              "8   Taysom Hill will start Sunday for the Saints a...   \n",
              "9   Taysom Hill will start Sunday for the Saints a...   \n",
              "10  Jameis Winston will not be part of any offensi...   \n",
              "11  Jameis Winston will not be part of any offensi...   \n",
              "12  The #Saints gave a 2-year, $21M contract this ...   \n",
              "13  Taysom Hill has only 18 career regular season ...   \n",
              "14  This is a good explanation for why the #Saints...   \n",
              "15  Fantasy managers running to the waiver wire fo...   \n",
              "16  Taysom Hill has four more pass attempts in his...   \n",
              "17  Taysom Hill’s first start is football twitter’...   \n",
              "18  Taysom Hill’s first start is football twitter’...   \n",
              "19  Taysom Hill starting on Sunday?!?! https://t.c...   \n",
              "20  Sean Payton: Make them all believe Jameis Wins...   \n",
              "21  Is it fair to have Taysom Hill in as your TE t...   \n",
              "22  Saints QB Taysom Hill's stats from the 2019 pr...   \n",
              "23  I’ve said all week I thought Taysom Hill would...   \n",
              "24  Shocker: Taysom Hill is not only starting over...   \n",
              "25  Saints fans after hearing that Taysom Hill is ...   \n",
              "26  It’s bullshit Taysom Hill can be put as TE whe...   \n",
              "27  How many Taysom Hill snaps until we’re screami...   \n",
              "28  i mean, at least i can finally watch Taysom Hi...   \n",
              "29          @FieldYates taysom hill or Matt stafford?   \n",
              "30  Taysom Hill starting over Winston is racist. I...   \n",
              "31        Wanted to watch Jameis play not taysom hill   \n",
              "32         Go ahead and play Taysom Hill. I dare you.   \n",
              "33  On record, Alex “The Snake” Taroff has signed ...   \n",
              "34  I don't even like Jameis Winston as a QB but T...   \n",
              "35  @bigtime_tim @bball798 @MatthewBerryTMR @ESPNF...   \n",
              "36  what's funny is most of these guys will try to...   \n",
              "37                                   Taysom Hill QB 1   \n",
              "38  Starting Taysom Hill at tight end https://t.co...   \n",
              "39  Taysom Hill sucks and I can’t stand the Saints...   \n",
              "40  I cannot believe Taysom Hill is about to be a ...   \n",
              "41  Putting Taysom Hill in my FLEX https://t.co/Wy...   \n",
              "42  make Taysom Hill a QB/TE you cowards @YahooFan...   \n",
              "43  Max Kellerman just compared Taysom Hill to Ode...   \n",
              "44  Taysom Hill is your starter 😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂...   \n",
              "45     @daverichard taysom hill or Matt stafford #fft   \n",
              "46   Saints expected to start Taysom Hill vs. Falcons   \n",
              "47  Jameis Winston - Sit\\nTaysom Hill at QB - Sit\\...   \n",
              "48  @heykayadams what do you think about taysom hi...   \n",
              "49  I genuinely hope Taysom hill starts and the pl...   \n",
              "\n",
              "                                     hashtags emojis trending_topic  \\\n",
              "0                                          []     []    Taysom Hill   \n",
              "1                                          []     []    Taysom Hill   \n",
              "2                                          []     []    Taysom Hill   \n",
              "3                                          []     []    Taysom Hill   \n",
              "4                                          []     []    Taysom Hill   \n",
              "5                                          []     []    Taysom Hill   \n",
              "6                                          []     []    Taysom Hill   \n",
              "7                                          []     []    Taysom Hill   \n",
              "8                                          []     []    Taysom Hill   \n",
              "9                                          []     []    Taysom Hill   \n",
              "10                                         []     []    Taysom Hill   \n",
              "11                                         []     []    Taysom Hill   \n",
              "12  [{'text': 'Saints', 'indices': [18, 25]}]     []    Taysom Hill   \n",
              "13                                         []     []    Taysom Hill   \n",
              "14  [{'text': 'Saints', 'indices': [53, 60]}]     []    Taysom Hill   \n",
              "15                                         []     []    Taysom Hill   \n",
              "16                                         []     []    Taysom Hill   \n",
              "17                                         []     []    Taysom Hill   \n",
              "18                                         []     []    Taysom Hill   \n",
              "19                                         []     []    Taysom Hill   \n",
              "20                                         []     []    Taysom Hill   \n",
              "21                                         []     []    Taysom Hill   \n",
              "22                                         []     []    Taysom Hill   \n",
              "23                                         []     []    Taysom Hill   \n",
              "24                                         []     []    Taysom Hill   \n",
              "25                                         []     []    Taysom Hill   \n",
              "26                                         []     []    Taysom Hill   \n",
              "27                                         []     []    Taysom Hill   \n",
              "28                                         []     []    Taysom Hill   \n",
              "29                                         []     []    Taysom Hill   \n",
              "30                                         []     []    Taysom Hill   \n",
              "31                                         []     []    Taysom Hill   \n",
              "32                                         []     []    Taysom Hill   \n",
              "33   [{'text': 'sad', 'indices': [171, 175]}]     []    Taysom Hill   \n",
              "34                                         []     []    Taysom Hill   \n",
              "35                                         []     []    Taysom Hill   \n",
              "36                                         []     []    Taysom Hill   \n",
              "37                                         []     []    Taysom Hill   \n",
              "38                                         []     []    Taysom Hill   \n",
              "39                                         []     []    Taysom Hill   \n",
              "40                                         []     []    Taysom Hill   \n",
              "41                                         []     []    Taysom Hill   \n",
              "42                                         []     []    Taysom Hill   \n",
              "43                                         []     []    Taysom Hill   \n",
              "44                                         []    [😂]    Taysom Hill   \n",
              "45     [{'text': 'fft', 'indices': [42, 46]}]     []    Taysom Hill   \n",
              "46                                         []     []    Taysom Hill   \n",
              "47                                         []     []    Taysom Hill   \n",
              "48                                         []     []    Taysom Hill   \n",
              "49                                         []     []    Taysom Hill   \n",
              "\n",
              "   search_type  preds sentiment  neg_prob  pos_prob  \n",
              "0       recent    0.0  negative   -0.4197   -1.0708  \n",
              "1       recent    1.0  positive   -0.8397   -0.5653  \n",
              "2       recent    1.0  positive   -0.8397   -0.5653  \n",
              "3       recent    1.0  positive   -0.8397   -0.5653  \n",
              "4       recent    1.0  positive   -0.8397   -0.5653  \n",
              "5       recent    1.0  positive   -0.8397   -0.5653  \n",
              "6       recent    0.0  negative   -0.6436   -0.7453  \n",
              "7       recent    0.0  negative   -0.6436   -0.7453  \n",
              "8       recent    0.0  negative   -0.6436   -0.7453  \n",
              "9       recent    0.0  negative   -0.6436   -0.7453  \n",
              "10      recent    1.0  positive   -1.0101   -0.4528  \n",
              "11      recent    1.0  positive   -1.0101   -0.4528  \n",
              "12      recent    1.0  positive   -0.8325   -0.5708  \n",
              "13      recent    0.0  negative   -0.4836   -0.9585  \n",
              "14      recent    1.0  positive   -0.8596   -0.5505  \n",
              "15      recent    0.0  negative   -0.4385   -1.0357  \n",
              "16      recent    0.0  negative   -0.1172   -2.2021  \n",
              "17      recent    1.0  positive   -1.4857   -0.2566  \n",
              "18      recent    1.0  positive   -1.4857   -0.2566  \n",
              "19      recent    0.0  negative   -0.5733   -0.8293  \n",
              "20      recent    1.0  positive   -0.7969   -0.5992  \n",
              "21      recent    1.0  positive   -1.6807   -0.2061  \n",
              "22      recent    0.0  negative   -0.6132   -0.7801  \n",
              "23      recent    1.0  positive   -1.1506   -0.3804  \n",
              "24      recent    0.0  negative   -0.5864   -0.8127  \n",
              "25      recent    1.0  positive   -0.7249   -0.6624  \n",
              "26      recent    0.0  negative   -0.4290   -1.0531  \n",
              "27      recent    1.0  positive   -0.7261   -0.6612  \n",
              "28      recent    0.0  negative   -0.1587   -1.9188  \n",
              "29      recent    1.0  positive   -0.7122   -0.6744  \n",
              "30      recent    0.0  negative   -0.3843   -1.1423  \n",
              "31      recent    0.0  negative   -0.0511   -3.0003  \n",
              "32      recent    0.0  negative   -0.1967   -1.7228  \n",
              "33      recent    1.0  positive   -0.9667   -0.4786  \n",
              "34      recent    0.0  negative   -0.5436   -0.8690  \n",
              "35      recent    0.0  negative   -0.5522   -0.8573  \n",
              "36      recent    0.0  negative   -0.6346   -0.7554  \n",
              "37      recent    0.0  negative   -0.2792   -1.4123  \n",
              "38      recent    0.0  negative   -0.1223   -2.1621  \n",
              "39      recent    0.0  negative   -0.3877   -1.1351  \n",
              "40      recent    0.0  negative   -0.1790   -1.8088  \n",
              "41      recent    0.0  negative   -0.2531   -1.4980  \n",
              "42      recent    1.0  positive   -1.2332   -0.3444  \n",
              "43      recent    0.0  negative   -0.6052   -0.7895  \n",
              "44      recent    1.0  positive   -0.9291   -0.5024  \n",
              "45      recent    1.0  positive   -0.7186   -0.6683  \n",
              "46      recent    1.0  positive   -1.5086   -0.2500  \n",
              "47      recent    0.0  negative   -0.4708   -0.9794  \n",
              "48      recent    1.0  positive   -1.4352   -0.2719  \n",
              "49      recent    0.0  negative   -0.4106   -1.0883  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 941
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb64l_zmJNdb",
        "outputId": "fde55729-a951-480c-971c-737f7f668c86"
      },
      "source": [
        "max_pos = max(tweets_data['pos_prob'])\n",
        "\n",
        "pos_tweets  = tweets_data.loc[tweets_data['pos_prob'] == max_pos][['text']]\n",
        "\n",
        "for p in pos_tweets['text']:\n",
        "    print(p) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thank you @DarrellIssa, so nice! \n",
            "https://t.co/FlUfzV7pSQ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkNHWxdvJNdb",
        "outputId": "612f7ce4-58b0-406d-a7f7-4c09f2b3e15e"
      },
      "source": [
        "max_neg = max(tweets_data['neg_prob'])\n",
        "\n",
        "neg_tweets  = tweets_data.loc[tweets_data['neg_prob'] == max_neg][['text']]\n",
        "\n",
        "for n in neg_tweets['text']:\n",
        "    print(n) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "They don’t want Jameis to eat https://t.co/aYGRxfX8Ra\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMz_f5b2JNdb",
        "outputId": "07f698cb-585e-4747-8b15-6fe48ec61c22"
      },
      "source": [
        "difficult = \"Great!! I got a flat tire!!\"\n",
        "\n",
        "predict(difficult)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 'negative', -0.525, -0.8954)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 946
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOp_21maJNdc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIMEf1JDJNdc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alzWsjmxJNdc",
        "outputId": "3aa626dd-4388-4dc3-cf7f-2c2dcd346a57"
      },
      "source": [
        "\n",
        "pred_positive_tweets = tweets_data.sort_values(by=['pos_prob'], inplace = False, ascending = False)       \n",
        "                            \n",
        "top_positive = pred_positive_tweets[0:25]     \n",
        "\n",
        "top_positive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>user_location</th>\n",
              "      <th>retweetcount</th>\n",
              "      <th>favorites</th>\n",
              "      <th>text</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>emojis</th>\n",
              "      <th>trending_topic</th>\n",
              "      <th>search_type</th>\n",
              "      <th>preds</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>neg_prob</th>\n",
              "      <th>pos_prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2944</th>\n",
              "      <td>realDonaldTrump</td>\n",
              "      <td>Washington, DC</td>\n",
              "      <td>29647</td>\n",
              "      <td>149063</td>\n",
              "      <td>Thank you @DarrellIssa, so nice! \\nhttps://t.c...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>\"Trump\"</td>\n",
              "      <td>popular</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-9.9925</td>\n",
              "      <td>-0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1478</th>\n",
              "      <td>JakeVendegna</td>\n",
              "      <td>Colorado</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>@EdwardNorton Thank you Mr. Norton. \\n\\nThank ...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Mr. Norton</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-5.7670</td>\n",
              "      <td>-0.0031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2921</th>\n",
              "      <td>DefectedRecords</td>\n",
              "      <td>London, UK</td>\n",
              "      <td>346</td>\n",
              "      <td>1318</td>\n",
              "      <td>Friday the 13th? 🤔\\n\\nNo thanks, good vibes on...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[🤔, 🎉]</td>\n",
              "      <td>Good Friday</td>\n",
              "      <td>popular</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-5.2337</td>\n",
              "      <td>-0.0053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1493</th>\n",
              "      <td>UptownGrowLab</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Thank you Mr Norton.  Nice thread. https://t.c...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Mr. Norton</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-5.0344</td>\n",
              "      <td>-0.0065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1477</th>\n",
              "      <td>Charlot67889548</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Well said, Mr Norton.... https://t.co/PoydhKGDkT</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Mr. Norton</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-4.9456</td>\n",
              "      <td>-0.0071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1485</th>\n",
              "      <td>ShnydaOne</td>\n",
              "      <td>East Coast</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Well said Mr. Norton... https://t.co/gijP4CCH86</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Mr. Norton</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-4.9456</td>\n",
              "      <td>-0.0071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2651</th>\n",
              "      <td>JeffreyGuterman</td>\n",
              "      <td>Global</td>\n",
              "      <td>38</td>\n",
              "      <td>643</td>\n",
              "      <td>@JoeBiden Thank you, Mr. President-elect. http...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Mr. President</td>\n",
              "      <td>popular</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-4.9374</td>\n",
              "      <td>-0.0072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1459</th>\n",
              "      <td>MicheleALeach</td>\n",
              "      <td>New Jersey</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>@EdwardNorton We can't flinch.\\n\\nThank you Mr...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Mr. Norton</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-4.8867</td>\n",
              "      <td>-0.0076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1788</th>\n",
              "      <td>kelsiebusbyyy</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>THANK YOU MORGAN WALLEN FOR THE NEW SONGS ILY ...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Morgan Wallen</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-4.7729</td>\n",
              "      <td>-0.0085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>979</th>\n",
              "      <td>Jemylooh</td>\n",
              "      <td>California, USA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Good morning and happy Friday !</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Good Friday</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-4.7518</td>\n",
              "      <td>-0.0087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>980</th>\n",
              "      <td>jacinto_gaby</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Good morning Friday ☀️</td>\n",
              "      <td>[]</td>\n",
              "      <td>[☀️]</td>\n",
              "      <td>Good Friday</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-4.7425</td>\n",
              "      <td>-0.0088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>KimKirchherr</td>\n",
              "      <td>Chicago</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Some great inspiration here! #gratitude #thank...</td>\n",
              "      <td>[{'text': 'gratitude', 'indices': [29, 39]}, {...</td>\n",
              "      <td>[]</td>\n",
              "      <td>#fridaymorning</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-4.6614</td>\n",
              "      <td>-0.0095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1876</th>\n",
              "      <td>young_arich</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>I can’t wait to listen to this Saint Jhn album</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>SAINt JHN</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-4.6572</td>\n",
              "      <td>-0.0095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2039</th>\n",
              "      <td>geekbroll</td>\n",
              "      <td>Portland, Oregon</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>@jpalmiotti Rounders is a great movie.</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Rounders</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-4.6314</td>\n",
              "      <td>-0.0098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2910</th>\n",
              "      <td>MarcusRashford</td>\n",
              "      <td></td>\n",
              "      <td>961</td>\n",
              "      <td>41287</td>\n",
              "      <td>Happy Friday everyone! Good day?</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Good Friday</td>\n",
              "      <td>popular</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-4.5320</td>\n",
              "      <td>-0.0108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3343</th>\n",
              "      <td>ComplexMusic</td>\n",
              "      <td></td>\n",
              "      <td>31</td>\n",
              "      <td>142</td>\n",
              "      <td>Listen to @SAINtJHN's new album 'While the Wor...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>SAINt JHN</td>\n",
              "      <td>popular</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-4.5052</td>\n",
              "      <td>-0.0111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>919</th>\n",
              "      <td>cindywue</td>\n",
              "      <td></td>\n",
              "      <td>1175</td>\n",
              "      <td>0</td>\n",
              "      <td>Thank you, Mitt Romney, for speaking up.</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Romney</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-4.5023</td>\n",
              "      <td>-0.0111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1978</th>\n",
              "      <td>FrankLevtow</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>It's finally Friday!!! Thank you for the RTs, ...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Finally Friday</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-4.4601</td>\n",
              "      <td>-0.0116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>967</th>\n",
              "      <td>wolfstall</td>\n",
              "      <td></td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>@Elvira51908110 @Cathhewat123 @otrogoga60 @Bel...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[☀️, ☕]</td>\n",
              "      <td>Good Friday</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-4.4328</td>\n",
              "      <td>-0.0120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1976</th>\n",
              "      <td>Vodka_Villain</td>\n",
              "      <td>STL</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>I'm just glad it's finally Friday 🙌🏾</td>\n",
              "      <td>[]</td>\n",
              "      <td>[🙌]</td>\n",
              "      <td>Finally Friday</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-4.4051</td>\n",
              "      <td>-0.0123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1464</th>\n",
              "      <td>TeresaMac2009</td>\n",
              "      <td>Portland, OR</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>This is brilliant... please unroll and read. T...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Mr. Norton</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-4.3895</td>\n",
              "      <td>-0.0125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1875</th>\n",
              "      <td>GoldenPussy28</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>SAINt JHN album pretty good.</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>SAINt JHN</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-4.3125</td>\n",
              "      <td>-0.0135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>984</th>\n",
              "      <td>MorgaKate</td>\n",
              "      <td>Montreal Girl🇨🇦</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>@DaveDleary @G12Rocco Good morning Dave! Happy...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[🌞]</td>\n",
              "      <td>Good Friday</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-4.2558</td>\n",
              "      <td>-0.0143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2914</th>\n",
              "      <td>TheNotoriousMMA</td>\n",
              "      <td>Dublin, Ireland</td>\n",
              "      <td>610</td>\n",
              "      <td>12892</td>\n",
              "      <td>Good night everyone! And happy Friday the 13th...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Good Friday</td>\n",
              "      <td>popular</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-4.1550</td>\n",
              "      <td>-0.0158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1489</th>\n",
              "      <td>LowandeMarlene</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>I think Mr. Norton nails it...read it https://...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Mr. Norton</td>\n",
              "      <td>recent</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-4.1029</td>\n",
              "      <td>-0.0167</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             username     user_location retweetcount favorites  \\\n",
              "2944  realDonaldTrump    Washington, DC        29647    149063   \n",
              "1478     JakeVendegna          Colorado            0         0   \n",
              "2921  DefectedRecords        London, UK          346      1318   \n",
              "1493    UptownGrowLab                              0         0   \n",
              "1477  Charlot67889548                              0         0   \n",
              "1485        ShnydaOne        East Coast            0         0   \n",
              "2651  JeffreyGuterman            Global           38       643   \n",
              "1459    MicheleALeach        New Jersey            3         0   \n",
              "1788    kelsiebusbyyy                              0         0   \n",
              "979          Jemylooh   California, USA            0         0   \n",
              "980      jacinto_gaby                              0         0   \n",
              "342      KimKirchherr           Chicago            0         0   \n",
              "1876      young_arich                              0         0   \n",
              "2039        geekbroll  Portland, Oregon            0         0   \n",
              "2910   MarcusRashford                            961     41287   \n",
              "3343     ComplexMusic                             31       142   \n",
              "919          cindywue                           1175         0   \n",
              "1978      FrankLevtow                              0         0   \n",
              "967         wolfstall                              2         0   \n",
              "1976    Vodka_Villain              STL             0         0   \n",
              "1464    TeresaMac2009      Portland, OR            0         3   \n",
              "1875    GoldenPussy28                              0         0   \n",
              "984         MorgaKate   Montreal Girl🇨🇦            0         0   \n",
              "2914  TheNotoriousMMA   Dublin, Ireland          610     12892   \n",
              "1489   LowandeMarlene                              0         0   \n",
              "\n",
              "                                                   text  \\\n",
              "2944  Thank you @DarrellIssa, so nice! \\nhttps://t.c...   \n",
              "1478  @EdwardNorton Thank you Mr. Norton. \\n\\nThank ...   \n",
              "2921  Friday the 13th? 🤔\\n\\nNo thanks, good vibes on...   \n",
              "1493  Thank you Mr Norton.  Nice thread. https://t.c...   \n",
              "1477   Well said, Mr Norton.... https://t.co/PoydhKGDkT   \n",
              "1485    Well said Mr. Norton... https://t.co/gijP4CCH86   \n",
              "2651  @JoeBiden Thank you, Mr. President-elect. http...   \n",
              "1459  @EdwardNorton We can't flinch.\\n\\nThank you Mr...   \n",
              "1788  THANK YOU MORGAN WALLEN FOR THE NEW SONGS ILY ...   \n",
              "979                     Good morning and happy Friday !   \n",
              "980                              Good morning Friday ☀️   \n",
              "342   Some great inspiration here! #gratitude #thank...   \n",
              "1876     I can’t wait to listen to this Saint Jhn album   \n",
              "2039             @jpalmiotti Rounders is a great movie.   \n",
              "2910                   Happy Friday everyone! Good day?   \n",
              "3343  Listen to @SAINtJHN's new album 'While the Wor...   \n",
              "919            Thank you, Mitt Romney, for speaking up.   \n",
              "1978  It's finally Friday!!! Thank you for the RTs, ...   \n",
              "967   @Elvira51908110 @Cathhewat123 @otrogoga60 @Bel...   \n",
              "1976               I'm just glad it's finally Friday 🙌🏾   \n",
              "1464  This is brilliant... please unroll and read. T...   \n",
              "1875                       SAINt JHN album pretty good.   \n",
              "984   @DaveDleary @G12Rocco Good morning Dave! Happy...   \n",
              "2914  Good night everyone! And happy Friday the 13th...   \n",
              "1489  I think Mr. Norton nails it...read it https://...   \n",
              "\n",
              "                                               hashtags   emojis  \\\n",
              "2944                                                 []       []   \n",
              "1478                                                 []       []   \n",
              "2921                                                 []   [🤔, 🎉]   \n",
              "1493                                                 []       []   \n",
              "1477                                                 []       []   \n",
              "1485                                                 []       []   \n",
              "2651                                                 []       []   \n",
              "1459                                                 []       []   \n",
              "1788                                                 []       []   \n",
              "979                                                  []       []   \n",
              "980                                                  []     [☀️]   \n",
              "342   [{'text': 'gratitude', 'indices': [29, 39]}, {...       []   \n",
              "1876                                                 []       []   \n",
              "2039                                                 []       []   \n",
              "2910                                                 []       []   \n",
              "3343                                                 []       []   \n",
              "919                                                  []       []   \n",
              "1978                                                 []       []   \n",
              "967                                                  []  [☀️, ☕]   \n",
              "1976                                                 []      [🙌]   \n",
              "1464                                                 []       []   \n",
              "1875                                                 []       []   \n",
              "984                                                  []      [🌞]   \n",
              "2914                                                 []       []   \n",
              "1489                                                 []       []   \n",
              "\n",
              "      trending_topic search_type  preds sentiment  neg_prob  pos_prob  \n",
              "2944         \"Trump\"     popular    1.0  positive   -9.9925   -0.0000  \n",
              "1478      Mr. Norton      recent    1.0  positive   -5.7670   -0.0031  \n",
              "2921     Good Friday     popular    1.0  positive   -5.2337   -0.0053  \n",
              "1493      Mr. Norton      recent    1.0  positive   -5.0344   -0.0065  \n",
              "1477      Mr. Norton      recent    1.0  positive   -4.9456   -0.0071  \n",
              "1485      Mr. Norton      recent    1.0  positive   -4.9456   -0.0071  \n",
              "2651   Mr. President     popular    1.0  positive   -4.9374   -0.0072  \n",
              "1459      Mr. Norton      recent    1.0  positive   -4.8867   -0.0076  \n",
              "1788   Morgan Wallen      recent    1.0  positive   -4.7729   -0.0085  \n",
              "979      Good Friday      recent    1.0  positive   -4.7518   -0.0087  \n",
              "980      Good Friday      recent    1.0  positive   -4.7425   -0.0088  \n",
              "342   #fridaymorning      recent    1.0  positive   -4.6614   -0.0095  \n",
              "1876       SAINt JHN      recent    1.0  positive   -4.6572   -0.0095  \n",
              "2039        Rounders      recent    1.0  positive   -4.6314   -0.0098  \n",
              "2910     Good Friday     popular    1.0  positive   -4.5320   -0.0108  \n",
              "3343       SAINt JHN     popular    1.0  positive   -4.5052   -0.0111  \n",
              "919           Romney      recent    1.0  positive   -4.5023   -0.0111  \n",
              "1978  Finally Friday      recent    1.0  positive   -4.4601   -0.0116  \n",
              "967      Good Friday      recent    1.0  positive   -4.4328   -0.0120  \n",
              "1976  Finally Friday      recent    1.0  positive   -4.4051   -0.0123  \n",
              "1464      Mr. Norton      recent    1.0  positive   -4.3895   -0.0125  \n",
              "1875       SAINt JHN      recent    1.0  positive   -4.3125   -0.0135  \n",
              "984      Good Friday      recent    1.0  positive   -4.2558   -0.0143  \n",
              "2914     Good Friday     popular    1.0  positive   -4.1550   -0.0158  \n",
              "1489      Mr. Norton      recent    1.0  positive   -4.1029   -0.0167  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 947
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNG_E97kJNdc",
        "outputId": "05e3ad2a-ec6c-41b0-ac60-130c2590f064"
      },
      "source": [
        "\n",
        "pred_negative_tweets = tweets_data.sort_values(by=['neg_prob'], inplace = False, ascending = False)        \n",
        "                            \n",
        "top_negative = pred_negative_tweets[0:25]     \n",
        "\n",
        "top_negative"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>user_location</th>\n",
              "      <th>retweetcount</th>\n",
              "      <th>favorites</th>\n",
              "      <th>text</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>emojis</th>\n",
              "      <th>trending_topic</th>\n",
              "      <th>search_type</th>\n",
              "      <th>preds</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>neg_prob</th>\n",
              "      <th>pos_prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>thedebster24</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>They don’t want Jameis to eat https://t.co/aYG...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Jameis</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0012</td>\n",
              "      <td>-6.7441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>TheFineFeminine</td>\n",
              "      <td>Dallas, TX</td>\n",
              "      <td>724</td>\n",
              "      <td>0</td>\n",
              "      <td>Megan ate this down  https://t.co/Yc54dp2EkL</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Megan</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-5.2513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2047</th>\n",
              "      <td>grupp_mc</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>@EdwardNorton I didn't realize Rounders Politi...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Rounders</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0110</td>\n",
              "      <td>-4.5189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2079</th>\n",
              "      <td>TAEY0NGCULT</td>\n",
              "      <td>she/her 🇲🇾</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>OH TODAY IS FRIDAY https://t.co/Br4WiOaFuj</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Today is Friday</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0146</td>\n",
              "      <td>-4.2329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1634</th>\n",
              "      <td>Coach_bjones44</td>\n",
              "      <td>wherever the positive vibe at</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>I wanted Jamies to play too https://t.co/7ipIT...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Jamies</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0147</td>\n",
              "      <td>-4.2268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>IdpBob</td>\n",
              "      <td>Kentucky, USA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Someone photoshop Jameis eating the L. https:/...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Jameis</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0179</td>\n",
              "      <td>-4.0293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3403</th>\n",
              "      <td>backlon</td>\n",
              "      <td>San Francisco, CA</td>\n",
              "      <td>95</td>\n",
              "      <td>1585</td>\n",
              "      <td>Today is Friday (really) and that means you ha...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Today is Friday</td>\n",
              "      <td>popular</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0190</td>\n",
              "      <td>-3.9733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2097</th>\n",
              "      <td>richbastard_</td>\n",
              "      <td>university of houston</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Do I wanna go back to work today? I mean it is...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Today is Friday</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0200</td>\n",
              "      <td>-3.9207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>683</th>\n",
              "      <td>KaiAlle19609176</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Jealousy is a sick wasted emotion \\n#FridayTho...</td>\n",
              "      <td>[{'text': 'FridayThoughts', 'indices': [54, 69]}]</td>\n",
              "      <td>[]</td>\n",
              "      <td>#FridayThoughts</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0227</td>\n",
              "      <td>-3.7962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2078</th>\n",
              "      <td>adriexna</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>the fact that today is friday when monday was ...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Today is Friday</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0228</td>\n",
              "      <td>-3.7930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1633</th>\n",
              "      <td>chris_rop</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>How bad is Jamies that their starting a tight ...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Jamies</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0298</td>\n",
              "      <td>-3.5284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022</th>\n",
              "      <td>whereandy</td>\n",
              "      <td>Deutschland</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>@buzz All the Rounders gifs today. https://t.c...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Rounders</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0345</td>\n",
              "      <td>-3.3832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1944</th>\n",
              "      <td>Mickey_Spades</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>#NowPlaying \"Pick Up Your Feelings\" by Jazmine...</td>\n",
              "      <td>[{'text': 'NowPlaying', 'indices': [0, 11]}]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Jazmine Sullivan</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0411</td>\n",
              "      <td>-3.2132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>548</th>\n",
              "      <td>lasantamuerta</td>\n",
              "      <td>United States</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>god i need megan to fuck me up https://t.co/1X...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Megan</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0415</td>\n",
              "      <td>-3.2025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1011</th>\n",
              "      <td>dalalondon</td>\n",
              "      <td></td>\n",
              "      <td>414</td>\n",
              "      <td>0</td>\n",
              "      <td>Re-upping this for those of you first realizin...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>\"Trump\"</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0427</td>\n",
              "      <td>-3.1754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1763</th>\n",
              "      <td>connerroby22</td>\n",
              "      <td>Massachusetts, USA</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>HEAVY.\\n\\n@MorganWallen \\nhttps://t.co/zvmx0gjyot</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Morgan Wallen</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0477</td>\n",
              "      <td>-3.0656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1762</th>\n",
              "      <td>kyhulagirl</td>\n",
              "      <td>Big Blue Nation, Kentucky</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>HEAVY.\\n\\n@MorganWallen \\nhttps://t.co/zvmx0gjyot</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Morgan Wallen</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0477</td>\n",
              "      <td>-3.0656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3013</th>\n",
              "      <td>AshleyRParker</td>\n",
              "      <td></td>\n",
              "      <td>765</td>\n",
              "      <td>3108</td>\n",
              "      <td>“They all know he lost, and they are lying abo...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>\"Trump\"</td>\n",
              "      <td>popular</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0478</td>\n",
              "      <td>-3.0646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1189</th>\n",
              "      <td>TheQueenIsDope</td>\n",
              "      <td>Pennsylvania, USA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Y’all better leave Gary Owen alone! https://t....</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Gary Owen</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0478</td>\n",
              "      <td>-3.0646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2095</th>\n",
              "      <td>MommaNEQ</td>\n",
              "      <td>Baltimore, MD</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Today is NOT giving Friday 🥴</td>\n",
              "      <td>[]</td>\n",
              "      <td>[🥴]</td>\n",
              "      <td>Today is Friday</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0493</td>\n",
              "      <td>-3.0334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>vinceb_77</td>\n",
              "      <td>Scottsdale, AZ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Wanted to watch Jameis play not taysom hill</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Jameis</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0511</td>\n",
              "      <td>-3.0003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>vinceb_77</td>\n",
              "      <td>Scottsdale, AZ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Wanted to watch Jameis play not taysom hill</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Taysom Hill</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0511</td>\n",
              "      <td>-3.0003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2082</th>\n",
              "      <td>dericecourcy</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Today is Friday\\n\\nI know this stuff is depres...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Today is Friday</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0521</td>\n",
              "      <td>-2.9802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>216</th>\n",
              "      <td>TheReal_TWill</td>\n",
              "      <td></td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>Sean Payton lost his damn mind</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Sean Payton</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0598</td>\n",
              "      <td>-2.8466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>215</th>\n",
              "      <td>milescourtland_</td>\n",
              "      <td></td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>Sean Payton lost his damn mind</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Sean Payton</td>\n",
              "      <td>recent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.0598</td>\n",
              "      <td>-2.8466</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             username                   user_location retweetcount favorites  \\\n",
              "134      thedebster24                                            0         0   \n",
              "514   TheFineFeminine                      Dallas, TX          724         0   \n",
              "2047         grupp_mc                                            0         0   \n",
              "2079      TAEY0NGCULT                      she/her 🇲🇾            0         0   \n",
              "1634   Coach_bjones44  wherever the positive vibe at             0         0   \n",
              "148            IdpBob                   Kentucky, USA            0         0   \n",
              "3403          backlon               San Francisco, CA           95      1585   \n",
              "2097     richbastard_          university of houston             0         0   \n",
              "683   KaiAlle19609176                                            1         0   \n",
              "2078         adriexna                                            0         1   \n",
              "1633        chris_rop                                            0         0   \n",
              "2022        whereandy                     Deutschland            0         1   \n",
              "1944    Mickey_Spades                                            0         0   \n",
              "548     lasantamuerta                   United States            0         0   \n",
              "1011       dalalondon                                          414         0   \n",
              "1763     connerroby22              Massachusetts, USA           24         0   \n",
              "1762       kyhulagirl       Big Blue Nation, Kentucky           24         0   \n",
              "3013    AshleyRParker                                          765      3108   \n",
              "1189   TheQueenIsDope               Pennsylvania, USA            0         0   \n",
              "2095         MommaNEQ                   Baltimore, MD            0         0   \n",
              "130         vinceb_77                  Scottsdale, AZ            0         0   \n",
              "31          vinceb_77                  Scottsdale, AZ            0         0   \n",
              "2082     dericecourcy                                            0         0   \n",
              "216     TheReal_TWill                                            6         0   \n",
              "215   milescourtland_                                            6         0   \n",
              "\n",
              "                                                   text  \\\n",
              "134   They don’t want Jameis to eat https://t.co/aYG...   \n",
              "514        Megan ate this down  https://t.co/Yc54dp2EkL   \n",
              "2047  @EdwardNorton I didn't realize Rounders Politi...   \n",
              "2079         OH TODAY IS FRIDAY https://t.co/Br4WiOaFuj   \n",
              "1634  I wanted Jamies to play too https://t.co/7ipIT...   \n",
              "148   Someone photoshop Jameis eating the L. https:/...   \n",
              "3403  Today is Friday (really) and that means you ha...   \n",
              "2097  Do I wanna go back to work today? I mean it is...   \n",
              "683   Jealousy is a sick wasted emotion \\n#FridayTho...   \n",
              "2078  the fact that today is friday when monday was ...   \n",
              "1633  How bad is Jamies that their starting a tight ...   \n",
              "2022  @buzz All the Rounders gifs today. https://t.c...   \n",
              "1944  #NowPlaying \"Pick Up Your Feelings\" by Jazmine...   \n",
              "548   god i need megan to fuck me up https://t.co/1X...   \n",
              "1011  Re-upping this for those of you first realizin...   \n",
              "1763  HEAVY.\\n\\n@MorganWallen \\nhttps://t.co/zvmx0gjyot   \n",
              "1762  HEAVY.\\n\\n@MorganWallen \\nhttps://t.co/zvmx0gjyot   \n",
              "3013  “They all know he lost, and they are lying abo...   \n",
              "1189  Y’all better leave Gary Owen alone! https://t....   \n",
              "2095                       Today is NOT giving Friday 🥴   \n",
              "130         Wanted to watch Jameis play not taysom hill   \n",
              "31          Wanted to watch Jameis play not taysom hill   \n",
              "2082  Today is Friday\\n\\nI know this stuff is depres...   \n",
              "216                      Sean Payton lost his damn mind   \n",
              "215                      Sean Payton lost his damn mind   \n",
              "\n",
              "                                               hashtags emojis  \\\n",
              "134                                                  []     []   \n",
              "514                                                  []     []   \n",
              "2047                                                 []     []   \n",
              "2079                                                 []     []   \n",
              "1634                                                 []     []   \n",
              "148                                                  []     []   \n",
              "3403                                                 []     []   \n",
              "2097                                                 []     []   \n",
              "683   [{'text': 'FridayThoughts', 'indices': [54, 69]}]     []   \n",
              "2078                                                 []     []   \n",
              "1633                                                 []     []   \n",
              "2022                                                 []     []   \n",
              "1944       [{'text': 'NowPlaying', 'indices': [0, 11]}]     []   \n",
              "548                                                  []     []   \n",
              "1011                                                 []     []   \n",
              "1763                                                 []     []   \n",
              "1762                                                 []     []   \n",
              "3013                                                 []     []   \n",
              "1189                                                 []     []   \n",
              "2095                                                 []    [🥴]   \n",
              "130                                                  []     []   \n",
              "31                                                   []     []   \n",
              "2082                                                 []     []   \n",
              "216                                                  []     []   \n",
              "215                                                  []     []   \n",
              "\n",
              "        trending_topic search_type  preds sentiment  neg_prob  pos_prob  \n",
              "134             Jameis      recent    0.0  negative   -0.0012   -6.7441  \n",
              "514              Megan      recent    0.0  negative   -0.0053   -5.2513  \n",
              "2047          Rounders      recent    0.0  negative   -0.0110   -4.5189  \n",
              "2079   Today is Friday      recent    0.0  negative   -0.0146   -4.2329  \n",
              "1634            Jamies      recent    0.0  negative   -0.0147   -4.2268  \n",
              "148             Jameis      recent    0.0  negative   -0.0179   -4.0293  \n",
              "3403   Today is Friday     popular    0.0  negative   -0.0190   -3.9733  \n",
              "2097   Today is Friday      recent    0.0  negative   -0.0200   -3.9207  \n",
              "683    #FridayThoughts      recent    0.0  negative   -0.0227   -3.7962  \n",
              "2078   Today is Friday      recent    0.0  negative   -0.0228   -3.7930  \n",
              "1633            Jamies      recent    0.0  negative   -0.0298   -3.5284  \n",
              "2022          Rounders      recent    0.0  negative   -0.0345   -3.3832  \n",
              "1944  Jazmine Sullivan      recent    0.0  negative   -0.0411   -3.2132  \n",
              "548              Megan      recent    0.0  negative   -0.0415   -3.2025  \n",
              "1011           \"Trump\"      recent    0.0  negative   -0.0427   -3.1754  \n",
              "1763     Morgan Wallen      recent    0.0  negative   -0.0477   -3.0656  \n",
              "1762     Morgan Wallen      recent    0.0  negative   -0.0477   -3.0656  \n",
              "3013           \"Trump\"     popular    0.0  negative   -0.0478   -3.0646  \n",
              "1189         Gary Owen      recent    0.0  negative   -0.0478   -3.0646  \n",
              "2095   Today is Friday      recent    0.0  negative   -0.0493   -3.0334  \n",
              "130             Jameis      recent    0.0  negative   -0.0511   -3.0003  \n",
              "31         Taysom Hill      recent    0.0  negative   -0.0511   -3.0003  \n",
              "2082   Today is Friday      recent    0.0  negative   -0.0521   -2.9802  \n",
              "216        Sean Payton      recent    0.0  negative   -0.0598   -2.8466  \n",
              "215        Sean Payton      recent    0.0  negative   -0.0598   -2.8466  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 948
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKGioGCTJNdc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXMGaKiUJNdc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHiE4E98JNdc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yODHMXjAJNdc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbi80oQaJNdc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgvUye6yJNdc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EEcVvCyJNdc"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baK8gmhRJNdc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4wtCV-EJNdc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03Iirr_AJNdc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mcNzjOCJNdc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}