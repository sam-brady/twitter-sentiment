# -*- coding: utf-8 -*-
"""twitter_sentiment.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CG5mexiCgmf1ADod7VJLbYldg6tm89fF
"""

####################################################################################################





################################################[IMPORTS]############################################

import pandas as pd
import numpy as np
import datetime
import time
import os 
import json


import re
import string    
import emojis
import tweepy


import nltk
from nltk.corpus import stopwords          
from nltk.stem import PorterStemmer        
from nltk.tokenize import TweetTokenizer  


import trax
from trax import layers as tl


import os
from google.cloud import storage
from google.cloud.storage.blob import Blob
from google.cloud import secretmanager



###################################[LOAD VOCAB JSON FILE]############################################

Vocab = json.load(open( r"/home/mr_sam_j_brady/Vocab.json" ))

#####################################################################################################


def access_secret_version(project_id, secret_id, version_id):
    """
    Access the payload for the given secret version if one exists. The version
    can be a version number as a string (e.g. "5") or an alias (e.g. "latest").
    """

    # Create the Secret Manager client.
    client = secretmanager.SecretManagerServiceClient()

    # Build the resource name of the secret version.
    name = f"projects/{project_id}/secrets/{secret_id}/versions/{version_id}"

    # Access the secret version.
    response = client.access_secret_version(request={"name": name})

    payload = response.payload.data.decode("UTF-8")

    return payload


#####################################################################################################


def process_tweet(tweet):  

    # remove old style retweet text "RT"
    new_tweet = re.sub(r'^RT[\s]+', '', tweet)
    
    # decode emojis to text descriptions
    new_tweet = emojis.decode(new_tweet)

    # remove hyperlinks
    new_tweet = re.sub('((www\.[^\s]+)|(https?://[^\s]+)|(http?://[^\s]+))', '', new_tweet)
    new_tweet = re.sub(r'http\S+', '', new_tweet)

    # remove hashtags
    new_tweet = re.sub(r'#', '', new_tweet)
    
    # remove underscores
    new_tweet = re.sub(r'_', '', new_tweet)

    # remove all numbers
    new_tweet = re.sub(r'[0-9]', '', new_tweet)

    # remove usernames
    new_tweet = re.sub('@[^\s]+', '', new_tweet)
    
    # remove punctuation even in the middle of a string "in.the.middle"
    new_tweet = re.sub(r'[^\w\s]',' ', new_tweet)

    # instantiate tokenizer class
    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)

    # tokenize tweets
    tweet_tokens = tokenizer.tokenize(new_tweet)

    tweets_clean = []

    for word in tweet_tokens: # Go through every word in your tokens list
        if (word not in string.punctuation):  # remove punctuation
            tweets_clean.append(word)

    # Instantiate stemming class
    stemmer = PorterStemmer() 

    # Create an empty list to store the stems
    tweets_stem = [] 

    for word in tweets_clean:
        stem_word = stemmer.stem(word)  # stemming word
        tweets_stem.append(stem_word)  # append to the list
    
    return tweets_stem


#####################################################################################################


def grab_trending_topics(country_id):   
    
    trending_topics = api.trends_place(country_id)                      # grab top 50 trending topics for the USA
    
    topic_list = []
    
    for k in range(len(trending_topics[0]['trends'])):                # loop though each trending topic
    
        topic = trending_topics[0]['trends'][k]['name']               # the trending topic or hashtag (1 out of 50)  
        
        topic_list.append(topic)
        
    return topic_list   


#####################################################################################################


def grab_popular_tweets(topic_list, max_tweets):             
    
    columns = [ 'pulled_at', 'created_at', 'username', 'user_location', 'region', 'search_type', 
               'trending_topic', 'retweetcount', 'favorites', 'text', 'hashtags', 'emojis']         # set up columns for dataframes   
    
    tweets_data_grab = pd.DataFrame(columns = columns)                                  # create empty dataframe    
        
    for topic in topic_list:                # loop though each trending topic
                                                            
                                                                                # grab tweets with Cursor
        tweets = tweepy.Cursor(api.search, q = topic,                           # search for each trending topic                                 
                         lang="en", result_type = 'popular',                    # tweets in english , type is "recent"/"popular"
                          tweet_mode = 'extended').items(max_tweets)            # longer tweets,  grab max_tweets number of tweets
        
        tweet_list = [tweet for tweet in tweets]                                # create list of tweets
                    
        tweets_topic = pd.DataFrame(columns = columns)         # create dataframe to put in current top tweets for this town and trending topic
            
        for tweet in tweet_list:                                      # loop through each tweet that was grabbed
            
            username = tweet.user.screen_name                                    # store username
            user_location = tweet.user.location                                  # store location of user
            retweetcount = tweet.retweet_count                                   # store retweet count
            favorites = tweet.favorite_count                                     # store favorite count
            hashtags = [h['text'].lower() for h in tweet.entities['hashtags']]   # store hashtags
            search_type = 'popular'                                              # store search type
            region = "USA"                                                       # trending tweets in USA
            created_at = tweet.created_at                                        # time tweet created
            pulled_at = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")    # time tweet was pulled
        
            try:                              
                text = tweet.retweeted_status.full_text    # store text if it's a retweet
            
            except AttributeError: 
                text = tweet.full_text                     # store text if it's a regular tweet
                
            emoji = list(emojis.get(text))                 # get the emojis
            
            curr_tweet = [pulled_at, created_at, username, user_location, region,     # store current tweet's data in a list soon to be a row
                          search_type, topic, retweetcount, favorites, text, hashtags, emoji]                             
        
            tweets_topic.loc[len(tweets_topic)] = curr_tweet                         # add current tweet data to dataframe for town and topic         
                                
        tweets_topic.sort_values(by=['retweetcount', 'favorites'], inplace = True, ascending = False)     # sort the retweet values highest first
                                
        tweets_data_grab = pd.concat([tweets_data_grab, tweets_topic], ignore_index = True, sort = False)       # concatenate top n to final dataframe
        
    return tweets_data_grab


#####################################################################################################


def tweet_to_tensor(tweet, vocab_dict, unk_token='__UNK__', verbose=False):
    '''
    Input: 
        tweet - A string containing a tweet
        vocab_dict - The words dictionary
        unk_token - The special string for unknown tokens
        verbose - Print info durign runtime
    Output:
        tensor_l - A python list with
        
    '''  

    # Process the tweet into a list of words
    # where only important words are kept (stop words removed)
    word_l = process_tweet(tweet)
    
    if verbose:
        print("List of words from the processed tweet:")
        print(word_l)
        
    # Initialize the list that will contain the unique integer IDs of each word
    tensor_l = []
    
    # Get the unique integer ID of the __UNK__ token
    unk_ID = vocab_dict[unk_token]
    
    if verbose:
        print(f"The unique integer ID for the unk_token is {unk_ID}")
        
    # for each word in the list:
    for word in word_l:
        
        # Get the unique integer ID.
        # If the word doesn't exist in the vocab dictionary,
        # use the unique ID for __UNK__ instead.
        word_ID = vocab_dict.get(word, unk_ID)

        
        # Append the unique integer ID to the tensor list.
        tensor_l.append(word_ID) 
    
    return tensor_l


#####################################################################################################


def classifier(vocab_size=len(Vocab), embedding_dim=256, output_dim=2, mode='predict'):
        

    # create embedding layer
    embed_layer = tl.Embedding(
        vocab_size=vocab_size, # Size of the vocabulary
        d_feature=embedding_dim)  # Embedding dimension
    
    # Create a mean layer, to create an "average" word embedding
    mean_layer = tl.Mean(axis=1)
    
    # Create a dense layer, one unit for each output
    dense_output_layer = tl.Dense(n_units = output_dim)

    
    # Create the log softmax layer (no parameters needed)
    log_softmax_layer = tl.LogSoftmax()
    
    # Use tl.Serial to combine all layers
    # and create the classifier
    # of type trax.layers.combinators.Serial
    model = tl.Serial(
      embed_layer, # embedding layer
      mean_layer, # mean layer
      dense_output_layer, # dense output layer 
      log_softmax_layer # log softmax layer
    )

    # return the model of type
    return model


#####################################################################################################


def predict(sentence):
    inputs = np.array(tweet_to_tensor(sentence, vocab_dict=Vocab))
    
    # Batch size 1, add dimension for batch, to work with the model
    inputs = inputs[None, :]  
     
    try:
    
        # predict with the model
        preds_probs = model(inputs)
    
        # Turn probabilities into categories
        preds = int(preds_probs[0, 1] > preds_probs[0, 0])
    
        sentiment = "negative"
        if preds == 1:
            sentiment = 'positive'
            
    except:
        
        return  'N/A', -0.0, -0.0

    return sentiment, round(float(preds_probs[0, 0]),4), round(float(preds_probs[0, 1]),4)


#####################################################################################################


def add_sentiment(tweets_data):

    for i in range(len(tweets_data)):            

        tweets_data.loc[i, 'sentiment'], tweets_data.loc[i, 'neg_prob'], tweets_data.loc[i, 'pos_prob'] = predict(tweets_data['text'].iloc[i])        
                
    return tweets_data


##########################################[MAIN FUNCTION]###############################################

project_id = 'twitter-test-298418'
version_id = 'latest'

access_token = access_secret_version(project_id, 'access_token', version_id)
access_token_secret = access_secret_version(project_id, 'access_token_secret', version_id)
api_key = access_secret_version(project_id, 'api_key', version_id)
api_secret = access_secret_version(project_id, 'api_secret', version_id)


# authorize api handshake
auth = tweepy.OAuthHandler(api_key, api_secret)
auth.set_access_token(access_token, access_token_secret)
api = tweepy.API(auth, wait_on_rate_limit=True)

#---------------------------------------------------------------------------------------------------------#

usa = 23424977        

model = classifier()

# Initialize using pre-trained weights.
model.init_from_file(r'/home/mr_sam_j_brady/model.pkl.gz', weights_only=True)

trending_topics = grab_trending_topics(country_id = usa)

pop_tweets_data = grab_popular_tweets(topic_list = trending_topics, max_tweets = 100)

tweets_data = add_sentiment(pop_tweets_data)

tweets_data.to_csv(r"/home/mr_sam_j_brady/tweets_data.csv",index=False)

#---------------------------------------------------------------------------------------------------------#


# Instantiates a client
storage_client = storage.Client()

# The name for the new bucket
bucket_name = "twitter-test-bucket"

# Creates/Gets the new bucket
bucket = storage_client.get_bucket(bucket_name)

data = bucket.blob("tweets_data.csv")

data.upload_from_filename(r"/home/mr_sam_j_brady/tweets_data.csv")    

#####################################################################################################